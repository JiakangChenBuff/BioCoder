{"golden":{"0":"def terminal_supports_color():\n    \"\"\"\n    Returns True if the running system's terminal supports color, and False\n    otherwise. Source:\n    https:\/\/github.com\/django\/django\/blob\/master\/django\/core\/management\/color.py\n    \"\"\"\n    plat = sys.platform\n    supported_platform = plat != 'Pocket PC' and (plat != 'win32' or \n        'ANSICON' in os.environ)\n    is_a_tty = hasattr(sys.stdout, 'isatty') and sys.stdout.isatty()\n    if not supported_platform or not is_a_tty:\n        return False\n    return True\n","1":"def openFile(filename, mode='r', create_dir=False):\n    \"\"\"open file in *filename* with mode *mode*.\n\n    If *create* is set, the directory containing filename\n    will be created if it does not exist.\n\n    gzip - compressed files are recognized by the\n    suffix ``.gz`` and opened transparently.\n\n    Note that there are differences in the file\n    like objects returned, for example in the\n    ability to seek.\n\n    returns a file or file-like object.\n    \"\"\"\n    _, ext = os.path.splitext(filename)\n    if create_dir:\n        dirname = os.path.dirname(filename)\n        if dirname and not os.path.exists(dirname):\n            os.makedirs(dirname)\n    if ext.lower() in ('.gz', '.z'):\n        if sys.version_info.major >= 3:\n            if mode == 'r':\n                return gzip.open(filename, 'rt', encoding='ascii')\n            elif mode == 'w':\n                return gzip.open(filename, 'wt', compresslevel=\n                    global_options.compresslevel, encoding='ascii')\n            else:\n                raise NotImplementedError(\"mode '{}' not implemented\".\n                    format(mode))\n        else:\n            return gzip.open(filename, mode, compresslevel=global_options.\n                compresslevel)\n    else:\n        return open(filename, mode)\n","2":"def select_templates_by_validation_score(targetid, validation_score_cutoff=\n    None, validation_score_percentile=None):\n    \"\"\"\n    Parameters\n    ----------\n    targetid: str\n    validation_score_cutoff: float\n    validation_score_percentile: float\n\n    Returns\n    -------\n    selected_templateids: list of str\n    \"\"\"\n    validation_score_filenames = ['validation_scores_sorted-molprobity-{}'.\n        format(stagename) for stagename in modeling_stages]\n    for validation_score_filename in validation_score_filenames[::-1]:\n        validation_score_filepath = os.path.join(default_project_dirnames.\n            models, targetid, validation_score_filename)\n        if os.path.exists(validation_score_filepath):\n            break\n    with open(validation_score_filepath) as validation_score_file:\n        validation_score_lines_split = [line.split() for line in\n            validation_score_file.read().splitlines()]\n    templateids = np.array([i[0] for i in validation_score_lines_split])\n    validation_scores = np.array([float(i[1]) for i in\n        validation_score_lines_split])\n    if validation_score_cutoff:\n        selected_templateids = [str(x) for x in templateids[\n            validation_scores < validation_score_cutoff]]\n    elif validation_score_percentile:\n        percentile_index = len(templateids) - 1 - int((len(templateids) - 1\n            ) * (float(validation_score_percentile) \/ 100.0))\n        selected_templateids = [str(x) for x in templateids[:percentile_index]]\n    else:\n        selected_templateids = templateids\n    return selected_templateids\n","3":"def tsl2int(tsl):\n    \"\"\"Convert an Ensembl Transcript Support Level (TSL) code to an integer.\n\n    The code has the format \"tsl([1-5]|NA)\".\n\n    See: https:\/\/www.ensembl.org\/Help\/Glossary?id=492\n    \"\"\"\n    if tsl in (np.nan, '', 'tslNA'):\n        return 0\n    assert tsl[:3] == 'tsl'\n    value = tsl[3:]\n    if len(value) > 2:\n        value = value[:2].rstrip()\n    if value == 'NA':\n        return 0\n    return int(value)\n","4":"@click.command()\n@click.argument('sam', required=True)\ndef bamtag(sam):\n    \"\"\" Convert a BAM\/SAM with fastqtransformed read names to have UMI and\n    cellular barcode tags\n    \"\"\"\n    from pysam import AlignmentFile\n    start_time = time.time()\n    sam_file = open_bamfile(sam)\n    out_file = AlignmentFile('-', 'wh', template=sam_file)\n    track = sam_file.fetch(until_eof=True)\n    if is_python3():\n        queryalignment = next(track)\n    else:\n        queryalignment = track.next()\n    annotations = detect_alignment_annotations(queryalignment)\n    track = itertools.chain([queryalignment], track)\n    re_string = construct_transformed_regex(annotations)\n    parser_re = re.compile(re_string)\n    for count, aln in enumerate(track, start=1):\n        if count and not count % 1000000:\n            logger.info('Processed %d alignments.' % count)\n        match = parser_re.match(aln.qname)\n        tags = aln.tags\n        if 'cellular' in annotations:\n            aln.tags += [('XC', match.group('CB'))]\n        if 'molecular' in annotations:\n            aln.tags += [('RX', match.group('MB'))]\n        if 'sample' in annotations:\n            aln.tags += [('XS', match.group('SB'))]\n        out_file.write(aln)\n    total_time = time.time() - start_time\n    logger.info('BAM tag conversion done - {:.3}s, {:,} alns\/min'.format(\n        total_time, int(60.0 * count \/ total_time)))\n    logger.info('Processed %d alignments.' % count)\n","5":"def write_index(fn, index):\n    \"\"\"Writes the index to file.\n\n    Args:\n        fn (str): the name of the file that will contain the index\n        index (pandas.DataFrame): the index\n\n    \"\"\"\n    with open(fn, 'wb') as o_file:\n        o_file.write(_CHECK_STRING)\n        o_file.write(zlib.compress(bytes(index.to_csv(None, index=False,\n            encoding='utf-8'), encoding='utf-8')))\n","6":"def split_sequence_file_on_sample_ids_to_files(seqs, outdir):\n    \"\"\"Split FASTA file on sample IDs.\n\n    Parameters\n    ----------\n    seqs: file handler\n        file handler to demultiplexed FASTA file\n    outdir: string\n        dirpath to output split FASTA files\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    logger.info(\n        'split_sequence_file_on_sample_ids_to_files for file %s into dir %s' %\n        (seqs, outdir))\n    outputs = {}\n    for bits in sequence_generator(seqs):\n        sample = sample_id_from_read_id(bits[0])\n        if sample not in outputs:\n            outputs[sample] = open(join(outdir, sample + '.fasta'), 'w')\n        outputs[sample].write('>%s\\n%s\\n' % (bits[0], bits[1]))\n    for sample in outputs:\n        outputs[sample].close()\n    logger.info('split to %d files' % len(outputs))\n","7":"def get_segment_vafs(variants, segments):\n    \"\"\"Group SNP allele frequencies by segment.\n\n    Assume variants and segments were already subset to one chromosome.\n\n    Yields\n    ------\n    tuple\n        (segment, value)\n    \"\"\"\n    if segments:\n        chunks = variants.by_ranges(segments)\n    else:\n        chunks = [(None, variants)]\n    for seg, seg_snvs in chunks:\n        freqs = seg_snvs['alt_freq'].values\n        idx_above_mid = freqs > 0.5\n        for idx_vaf in (idx_above_mid, ~idx_above_mid):\n            if sum(idx_vaf) > 1:\n                yield seg, np.median(freqs[idx_vaf])\n","8":"@jit_or_passthrough\ndef jax_core_adaptive(u_kn, N_k, f_k, gamma):\n    \"\"\"JAX version of adaptive inner loop.\n    N_k must be float (should be cast at a higher level)\n\n    \"\"\"\n    g = mbar_gradient(u_kn, N_k, f_k)\n    H = mbar_hessian(u_kn, N_k, f_k)\n    Hinvg = lstsq(H, g, rcond=-1)[0]\n    Hinvg -= Hinvg[0]\n    f_nr = f_k - gamma * Hinvg\n    f_sci = self_consistent_update(u_kn, N_k, f_k)\n    f_sci = f_sci - f_sci[0]\n    g_sci = mbar_gradient(u_kn, N_k, f_sci)\n    gnorm_sci = dot(g_sci, g_sci)\n    g_nr = mbar_gradient(u_kn, N_k, f_nr)\n    gnorm_nr = dot(g_nr, g_nr)\n    return f_sci, g_sci, gnorm_sci, f_nr, g_nr, gnorm_nr\n","9":"def check_file(raw, default, expected, sequence=False):\n    \"\"\"\n    Check if value point to a file\n\n    :param str raw: the value return by the user\n    :param str default: the default value for the option\n    :param expected: not used here to have the same signature for all check_xxx functions\n    :return: value\n    :raise MacsypyError: if the value cannot be cast in right type\n    \"\"\"\n\n    def path(value):\n        if value.lower() == 'none':\n            return None\n        if os.path.exists(value):\n            if os.path.isfile(value):\n                return value\n            else:\n                raise ValueError(f\"'{value}' is not a file.\")\n        else:\n            raise ValueError(f\"'{value}' no such file or directory.\")\n    return _validator(path, raw, default, sequence=sequence)\n","10":"def integrated_autocorrelation_timeMultiple(A_kn, fast=False):\n    \"\"\"Estimate the integrated autocorrelation time from multiple timeseries.\n\n    See Also\n    --------\n    statistical_inefficiency_multiple\n\n    \"\"\"\n    g = statistical_inefficiency_multiple(A_kn, fast, False)\n    tau = (g - 1.0) \/ 2.0\n    return tau\n","11":"def _skat_generate_r_script(dir_name, r_files, args):\n    \"\"\"Uses jinja2 to generate an R script to do the SKAT analysis.\n\n    Args:\n        dir_name (str): the output directory name to write the scripts in\n        r_files (dict): contains the different input files required by the R\n                        script\n        args (argparse.Namespace): the parsed arguments\n\n    Returns:\n        list: the list of all script names\n\n    \"\"\"\n    jinja_env = jinja2.Environment(loader=jinja2.PackageLoader('genipe',\n        'script_templates'))\n    template = jinja_env.get_template('run_skat.R')\n    scripts = []\n    for snp_set_file in r_files['snp_sets']:\n        rendered_script = template.render(version=__version__, snp_set_file\n            =snp_set_file, covariate_file=r_files['covariates'],\n            outcome_file=r_files['outcome'], weights=r_files['weights'],\n            outcome_type='C' if args.outcome_type == 'continuous' else 'D',\n            skat_o=args.skat_o)\n        script_filename = 'run_skat_{}R'.format(os.path.basename(\n            snp_set_file)[:-len('.genotype.csv')])\n        script_filename = os.path.join(dir_name, script_filename)\n        with open(script_filename, 'w') as f:\n            f.write(rendered_script)\n        scripts.append(script_filename)\n    return scripts\n","12":"def check_bool(raw, default, expected, sequence=False):\n    \"\"\"\n    Check if value can be cast in str\n\n    :param str raw: the value return by the user\n    :param str default: the default value for the option\n    :param expected: not used here to have the same signature for all check_xxx functions\n    :return: value\n    :raise MacsypyError: if the value cannot be cast in right type\n    \"\"\"\n\n    def bool_cast(raw):\n        raw = str(raw).lower()\n        if raw in ('false', 'no', '0'):\n            casted = False\n        elif raw in ('true', 'yes', '1'):\n            casted = True\n        else:\n            raise ValueError(\"Authorized values ['True'\/False\/0\/1]\")\n        return casted\n    return _validator(bool_cast, raw, default, sequence=sequence)\n","13":"def infer_sexes(cnn_fnames, is_haploid_x):\n    \"\"\"Map sample IDs to inferred chromosomal sex, where possible.\n\n    For samples where the source file is empty or does not include either sex\n    chromosome, that sample ID will not be in the returned dictionary.\n    \"\"\"\n    sexes = {}\n    for fname in cnn_fnames:\n        cnarr = read_cna(fname)\n        if cnarr:\n            is_xx = cnarr.guess_xx(is_haploid_x)\n            if is_xx is not None:\n                sexes[cnarr.sample_id] = is_xx\n    return sexes\n","14":"def _search_in_desc(pattern: str, remote: RemoteModelIndex, packages: List[\n    str], match_case: bool=False):\n    \"\"\"\n\n    :param pattern: the substring to search packages descriptions\n    :param remote: the uri of the macsy-models index\n    :param packages: list of packages to search in\n    :param match_case: True if the search is case sensitive, False otherwise\n    :return:\n    \"\"\"\n    results = []\n    for pack_name in packages:\n        all_versions = remote.list_package_vers(pack_name)\n        if all_versions:\n            metadata = remote.get_metadata(pack_name)\n            desc = metadata['short_desc']\n            if not match_case:\n                pack = pack_name.lower()\n                desc = desc.lower()\n                pattern = pattern.lower()\n            else:\n                pack = pack_name\n            if pattern in pack or pattern in desc:\n                last_vers = all_versions[0]\n                results.append((pack_name, last_vers, metadata['short_desc']))\n    return results\n","15":"def compress_impute2_files(required_chrom, filename_template, db_name, options\n    ):\n    \"\"\"Merges impute2 files.\n\n    Args:\n        required_chrom (tuple): the list of chromosome to compress\n        filename_template (str): the template for the final IMPUTE2 file\n        db_name (str): the name of the DB saving tasks' information\n        options (argparse.Namespace): the pipeline options\n\n    A template contains the string ``{chrom}``, which will be replaced by the\n    chromosome number (e.g. ``genipe\/chr{chrom}\/chr{chrom}.final`` will be\n    replaced by ``genipe\/chr1\/chr1.final``).\n\n    \"\"\"\n    commands_info = []\n    base_command = ['bgzip', '-f']\n    for chrom in required_chrom:\n        filename = filename_template.format(chrom=chrom)\n        remaining_command = [filename]\n        commands_info.append({'task_id': 'bgzip_chr{}'.format(chrom),\n            'name': 'Compress chr{}'.format(chrom), 'command': base_command +\n            remaining_command, 'task_db': db_name, 'o_files': [filename +\n            '.gz']})\n    logging.info('Compressing impute2 files')\n    launcher.launch_tasks(commands_info, options.thread, hpc=options.\n        use_drmaa, hpc_options=options.task_options, out_dir=options.\n        out_dir, preamble=options.preamble)\n    logging.info('Done compressing impute2 files')\n","16":"def against_contigs(log, blast_db, query_file, hits_file, **kwargs):\n    \"\"\"Blast the query sequence against the contigs.\n\n    The blast output will have the scores for later processing.\n    \"\"\"\n    cmd = []\n    if kwargs['protein']:\n        cmd.append('tblastn')\n        cmd.append('-db_gencode {}'.format(kwargs['blast_db_gencode']))\n    else:\n        cmd.append('blastn')\n    cmd.append('-db {}'.format(blast_db))\n    cmd.append('-query {}'.format(query_file))\n    cmd.append('-out {}'.format(hits_file))\n    cmd.append('-outfmt 15')\n    command = ' '.join(cmd)\n    log.subcommand(command, kwargs['temp_dir'], timeout=kwargs['timeout'])\n","17":"def count_distinct_psms(csv_file_path=None, psm_defining_colnames=None):\n    \"\"\"\n    Returns a counter based on PSM-defining column names (i.e spectrum & peptide,\n    but also score field because sometimes the same PSMs are reported\n    with different scores...).\n    \"\"\"\n    psm_counter = Counter()\n    with open(csv_file_path, 'r') as in_file:\n        csv_input = csv.DictReader(in_file)\n        output_fieldnames = list(csv_input.fieldnames)\n        for line_dict in csv_input:\n            psm = tuple([line_dict[x] for x in psm_defining_colnames if x in\n                line_dict.keys()])\n            psm_counter[psm] += 1\n    return psm_counter\n","18":"@click.command()\n@click.argument('SAM', required=True)\n@click.argument('barcodes', type=click.File('r'), required=True)\ndef subset_bamfile(sam, barcodes):\n    \"\"\"\n    Subset a SAM\/BAM file, keeping only alignments from given\n    cellular barcodes\n    \"\"\"\n    from pysam import AlignmentFile\n    start_time = time.time()\n    sam_file = open_bamfile(sam)\n    out_file = AlignmentFile('-', 'wh', template=sam_file)\n    track = sam_file.fetch(until_eof=True)\n    queryalignment = track.next()\n    annotations = detect_alignment_annotations(queryalignment)\n    track = itertools.chain([queryalignment], track)\n    re_string = construct_transformed_regex(annotations)\n    parser_re = re.compile(re_string)\n    barcodes = set(barcode.strip() for barcode in barcodes)\n    for count, aln in enumerate(track, start=1):\n        if count and not count % 1000000:\n            logger.info('Processed %d alignments.' % count)\n        match = parser_re.match(aln.qname)\n        tags = aln.tags\n        if 'cellular' in annotations:\n            cb = match.group('CB')\n            if cb in barcodes:\n                out_file.write(aln)\n","19":"def cull(population: list, percentage: float=0.5) ->list:\n    \"\"\"\n    The purpose of this function will be to cull the bacteria created in the model\n    :param percentage: percentage of the population to eliminate\n    :param population: the list of members to cull\n    :return: The list of remaining members\n    \"\"\"\n    cull_amount = round(len(population) * percentage)\n    print('Culling {} members from population'.format(cull_amount))\n    for _ in range(cull_amount):\n        selection = random.choice(population)\n        population.remove(selection)\n        selection.remove()\n    return population\n","20":"def calc_FDR(PSM_count, false_positives):\n    \"\"\"\n    calculate false discovery rate according to FDR Method 2\n    (K\u00e4ll et al. 2007) as explained by Jones et al. 2009\n    \"\"\"\n    true_positives = PSM_count - 2 * false_positives\n    if true_positives <= 0:\n        return 1.0\n    FDR = false_positives \/ (false_positives + true_positives)\n    return FDR\n","21":"def skat_read_snp_set(i_filename):\n    \"\"\"Reads the SKAT SNP set file.\n\n    Args:\n        i_filename (str): the name of the input file\n\n    Returns:\n        pandas.DataFrame: the SNP set for the SKAT analysis\n\n    This file has to be supplied by the user. The recognized columns are:\n    ``variant``, ``snp_set`` and ``weight``. The ``weight`` column is optional\n    and can be used to specify a custom weighting scheme for SKAT. If nothing\n    is specified, the default Beta weights are used.\n\n    The file has to be tab delimited.\n\n    \"\"\"\n    skat_info = pd.read_csv(i_filename, sep='\\t', header=0)\n    if 'variant' not in skat_info.columns:\n        raise GenipeError(\n            \"The SKAT SNP set file needs to have a 'variant' column containing the ID of every variant of interest.\"\n            )\n    if 'snp_set' not in skat_info.columns:\n        raise GenipeError(\n            \"The SKAT SNP set file needs to have a 'snp_set' column containing the SNP set ID for every variant. The user is free to choose the SNP ID\"\n            )\n    return skat_info\n","22":"def create_all_shards(args, log, shard_list):\n    \"\"\"Assign processes to make the blast DBs.\n\n    One process for each blast DB shard.\n    \"\"\"\n    log.info('Making blast DBs')\n    with multiprocessing.Pool(processes=args['cpus']) as pool:\n        results = []\n        for idx, shard_params in enumerate(shard_list, 1):\n            results.append(pool.apply_async(create_one_blast_shard, (args,\n                shard_params, idx)))\n        all_results = [result.get() for result in results]\n    log.info('Finished making all {} blast DBs'.format(len(all_results)))\n","23":"def do_download(args: argparse.Namespace) ->str:\n    \"\"\"\n    Download tarball from remote models repository.\n\n    :param args: the arguments passed on the command line\n    :type args: :class:`argparse.Namespace` object\n    :rtype: None\n    \"\"\"\n    try:\n        remote = RemoteModelIndex(org=args.org)\n        req = requirements.Requirement(args.package)\n        pack_name = req.name\n        specifier = req.specifier\n        all_versions = remote.list_package_vers(pack_name)\n        if all_versions:\n            compatible_version = list(specifier.filter(all_versions))\n            if compatible_version:\n                vers = compatible_version[0]\n                _log.info(f'Downloading {pack_name} {vers}')\n                arch_path = remote.download(pack_name, vers, dest=args.dest)\n                _log.info(\n                    f'Successfully downloaded packaging {pack_name} in {arch_path}'\n                    )\n                return arch_path\n            else:\n                _log.error(\n                    f\"No version that satisfy requirements '{specifier}' for '{pack_name}'.\"\n                    )\n                _log.warning(f\"Available versions: {','.join(all_versions)}\")\n    except MacsyDataLimitError as err:\n        _log.critical(str(err))\n","24":"def _search_in_pack_name(pattern: str, remote: RemoteModelIndex, packages:\n    List[str], match_case: bool=False) ->List[Tuple[str, str, Dict]]:\n    \"\"\"\n\n    :param pattern: the substring to search packages names\n    :param remote: the uri of the macsy-models index\n    :param packages: list of packages to search in\n    :param match_case: True if the search is case sensitive, False otherwise\n    :return:\n    \"\"\"\n    results = []\n    for pack_name in packages:\n        if not match_case:\n            pack = pack_name.lower()\n            pattern = pattern.lower()\n        else:\n            pack = pack_name\n        if pattern in pack:\n            all_versions = remote.list_package_vers(pack_name)\n            if all_versions:\n                metadata = remote.get_metadata(pack_name)\n                last_vers = all_versions[0]\n                results.append((pack_name, last_vers, metadata['short_desc']))\n    return results\n","25":"def do_cite(args: argparse.Namespace) ->None:\n    \"\"\"\n    How to cite an installed model.\n\n    :param args: the arguments passed on the command line\n    :type args: :class:`argparse.Namespace` object\n    :rtype: None\n    \"\"\"\n    pack_name = args.package\n    inst_pack_loc = _find_installed_package(pack_name, models_dir=args.\n        models_dir)\n    if inst_pack_loc:\n        pack = Package(inst_pack_loc.path)\n        pack_citations = pack.metadata['cite']\n        pack_citations = [cite.replace('\\n', '\\n  ') for cite in pack_citations\n            ]\n        pack_citations = '\\n- '.join(pack_citations)\n        pack_citations = '_ ' + pack_citations.rstrip()\n        macsy_cite = macsypy.__citation__\n        macsy_cite = macsy_cite.replace('\\n', '\\n  ')\n        macsy_cite = '- ' + macsy_cite\n        print(\n            f'To cite {pack_name}:\\n\\n{pack_citations}\\n\\nTo cite MacSyFinder:\\n\\n{macsy_cite}\\n'\n            )\n    else:\n        _log.error(f\"Models '{pack_name}' not found locally.\")\n        sys.tracebacklimit = 0\n        raise ValueError()\n","26":"@contextlib.contextmanager\ndef temp_write_text(text, mode='w+b'):\n    \"\"\"Save text to a temporary file.\n\n    NB: This won't work on Windows b\/c the file stays open.\n    \"\"\"\n    with tempfile.NamedTemporaryFile(mode=mode) as tmp:\n        tmp.write(text)\n        tmp.flush()\n        yield tmp.name\n","27":"def start_log(level=logging.DEBUG, filename=None):\n    \"\"\"start the logger for the run\n\n    Parameters\n    ----------\n    level : int, optional\n        logging.DEBUG, logging.INFO etc. for the log level (between 0-50).\n    filename : str, optional\n      name of the filename to save the log to or\n      None (default) to use deblur.log.TIMESTAMP\n    \"\"\"\n    if filename is None:\n        tstr = time.ctime()\n        tstr = tstr.replace(' ', '.')\n        tstr = tstr.replace(':', '.')\n        filename = 'deblur.log.%s' % tstr\n    logging.basicConfig(filename=filename, level=level, format=\n        '%(levelname)s(%(thread)d)%(asctime)s:%(message)s')\n    logger = logging.getLogger(__name__)\n    logger.info('*************************')\n    logger.info('deblurring started')\n","28":"def search_compound(model, id):\n    \"\"\"Search a set of compounds, then print detailed properties.\n\n    Args:\n        id: a list of compound ids\n    \"\"\"\n    selected_compounds = set()\n    for compound in model.compounds:\n        if len(id) > 0:\n            if any(c == compound.id for c in id):\n                selected_compounds.add(compound)\n                continue\n    for compound in selected_compounds:\n        props = set(compound.properties) - {'id'}\n        if compound.filemark is not None:\n            print('Defined in {}'.format(compound.filemark))\n        print('id: {}'.format(compound.id))\n        for prop in sorted(props):\n            print('{}: {}'.format(prop, compound.properties[prop]))\n        print('\\n')\n","29":"def field_to_float(field):\n    \"\"\"\n    Converts the value of a field to float.\n    If the field contains multiple separated floats,\n    the mean is returned instead. If the field is empty,\n    numpy.NaN is returned.\n    \"\"\"\n    try:\n        result = float(field)\n    except ValueError:\n        if field == '':\n            result = np.NaN\n        elif ';' in field:\n            values = [float(v) for v in field.split(';')]\n            result = sum(values) \/ len(values)\n        else:\n            raise Exception('Field value {0} cannot be converted to float.'\n                .format(field))\n    return float(result)\n","30":"def field_to_bayes_float(field):\n    \"\"\"\n    Converts the value of a field to float.\n    If the field contains multiple separated floats,\n    the naive Bayes combined value is returned instead.\n    If the field is empty, numpy.NaN is returned.\n    \"\"\"\n    try:\n        result = float(field)\n    except ValueError:\n        if field == '':\n            result = np.NaN\n        elif ';' in field:\n            values = [float(v) for v in field.split(';')]\n            result = naive_bayes(values)\n        else:\n            raise Exception('Field value {0} cannot be converted to float.'\n                .format(field))\n    return float(result)\n","31":"def compute_marker_missing_rate(prefix, db_name, options):\n    \"\"\"Compute (using Plink) marker missing rate.\n\n    Args:\n        prefix (str): the prefix of the input file\n        db_name (str): the name of the DB saving tasks' information\n        options (argparse.Namespace): the pipeline options\n\n    Returns:\n        pandas.DataFrame: the missing rate for each site (results from Plink)\n\n    \"\"\"\n    o_prefix = os.path.join(options.out_dir, 'missing')\n    if not os.path.isdir(o_prefix):\n        os.mkdir(o_prefix)\n    o_prefix = os.path.join(o_prefix, 'missing')\n    commands_info = []\n    command = ['plink' if options.plink_bin is None else options.plink_bin,\n        '--noweb', '--bfile', prefix, '--missing', '--out', o_prefix]\n    commands_info.append({'task_id': 'plink_missing_rate', 'name':\n        'plink missing rate', 'command': command, 'task_db': db_name,\n        'o_files': [(o_prefix + ext) for ext in ('.lmiss', '.imiss')]})\n    logging.info('Computing missing rate')\n    launcher.launch_tasks(commands_info, options.thread, hpc=options.\n        use_drmaa, hpc_options=options.task_options, out_dir=options.\n        out_dir, preamble=options.preamble)\n    logging.info('Done computing missing rate')\n    logging.info('Reading the missing rate')\n    return pd.read_csv(o_prefix + '.lmiss', delim_whitespace=True)\n","32":"def do_search(args: argparse.Namespace) ->None:\n    \"\"\"\n    Search macsy-models for Model in a remote index.\n    by default search in package name,\n    if option -S is set search also in description\n    by default the search is case insensitive except if\n    option --match-case is set.\n\n    :param args: the arguments passed on the command line\n    :type args: :class:`argparse.Namespace` object\n    :rtype: None\n    \"\"\"\n    try:\n        remote = RemoteModelIndex(org=args.org)\n        packages = remote.list_packages()\n        if args.careful:\n            results = _search_in_desc(args.pattern, remote, packages,\n                match_case=args.match_case)\n        else:\n            results = _search_in_pack_name(args.pattern, remote, packages,\n                match_case=args.match_case)\n        for pack, last_vers, desc in results:\n            pack_vers = f'{pack} ({last_vers})'\n            print(f'{pack_vers:26.25} - {desc}')\n    except MacsyDataLimitError as err:\n        _log.critical(str(err))\n","33":"def calc_intercept_and_slope(a, b):\n    \"\"\"\n    Given two points in 2D-space (tuples a and b), calculates\n    the slope (m) and intercept (b) of the line connecting\n    both points. Required for FDR Score calculcation.\n    Returns intercept and slope in a tuple.\n    \"\"\"\n    if b[0] - a[0] == 0:\n        slope = 0\n    else:\n        slope = (b[1] - a[1]) \/ (b[0] - a[0])\n    intercept = -slope * a[0] + a[1]\n    return intercept, slope\n","34":"def ensure_pme_parameters_are_explicit(system):\n    \"\"\"Ensure that the PME parameters in an OpenMM system are explicit.\n    If they are not explicit, set them explicitly.\n\n    Parameters\n    ----------\n    system : simtk.openmm.System\n        System for which NonbondedForce PME parameters are to be set explicitly.\n\n    \"\"\"\n    forces = {system.getForce(force_index).__class__.__name__: system.\n        getForce(force_index) for force_index in range(system.getNumForces())}\n    force = forces['NonbondedForce']\n    alpha, nx, ny, nz = force.getPMEParameters()\n    if alpha == 0.0 \/ unit.nanometers:\n        alpha, nx, ny, nz = calc_pme_parameters(system)\n        force.setPMEParameters(alpha, nx, ny, nz)\n    return\n","35":"def unify_sequence(seq):\n    \"\"\"\n    Some sequences cannot be distinguished by MS (i.e. sequences\n    where L is replaced by I and vice versa).\n    Such target\/decoy pairs are not suitable for training.\n    This function applies rules to a sequence that allow identification\n    of such target\/decoy pairs,\n    i.e.\n        unify_sequence('EILE') == unify_sequence('ELLE')\n        -> True\n    \"\"\"\n    seq = seq.replace('L', 'I')\n    if len(seq) >= 2:\n        seq = ''.join(sorted(seq[:2])) + seq[2:]\n    return seq\n","36":"def init_logger(level='INFO', out=True):\n    \"\"\"\n\n    :param level: The logger threshold could be a positive int or string\n                  among: 'CRITICAL', 'ERROR', 'WARNING', 'INFO', 'DEBUG'\n    :param out: if the log message must be displayed\n    :return: logger\n    :rtype: :class:`logging.Logger` instance\n    \"\"\"\n    logger = colorlog.getLogger('macsydata')\n    handlers = []\n    if out:\n        stdout_handler = colorlog.StreamHandler(sys.stderr)\n        if level <= logging.DEBUG:\n            msg_formatter = (\n                '%(log_color)s%(levelname)-8s : %(module)s: L %(lineno)d :%(reset)s %(message)s'\n                )\n        else:\n            msg_formatter = '%(log_color)s%(message)s'\n        stdout_formatter = colorlog.ColoredFormatter(msg_formatter, datefmt\n            =None, reset=True, log_colors={'DEBUG': 'cyan', 'INFO': 'green',\n            'WARNING': 'yellow', 'ERROR': 'red', 'CRITICAL': 'bold_red'},\n            secondary_log_colors={}, style='%')\n        stdout_handler.setFormatter(stdout_formatter)\n        logger.addHandler(stdout_handler)\n        handlers.append(stdout_handler)\n    else:\n        null_handler = logging.NullHandler()\n        logger.addHandler(null_handler)\n        handlers.append(null_handler)\n    if isinstance(level, str):\n        level = getattr(logging, level)\n    logger.setLevel(level)\n    return logger\n","37":"@click.command()\n@click.argument('fastq', required=True)\n@click.option('--umi_histogram', required=False, help=\n    'Output a count of each UMI for each cellular barcode to this file.')\ndef cb_histogram(fastq, umi_histogram):\n    \"\"\" Counts the number of reads for each cellular barcode\n\n    Expects formatted fastq files.\n    \"\"\"\n    annotations = detect_fastq_annotations(fastq)\n    re_string = construct_transformed_regex(annotations)\n    parser_re = re.compile(re_string)\n    cb_counter = collections.Counter()\n    umi_counter = collections.Counter()\n    for read in read_fastq(fastq):\n        match = parser_re.search(read).groupdict()\n        cb = match['CB']\n        cb_counter[cb] += 1\n        if umi_histogram:\n            umi = match['MB']\n            umi_counter[cb, umi] += 1\n    for bc, count in cb_counter.most_common():\n        sys.stdout.write('{}\\t{}\\n'.format(bc, count))\n    if umi_histogram:\n        with open(umi_histogram, 'w') as umi_handle:\n            for cbumi, count in umi_counter.most_common():\n                umi_handle.write('{}\\t{}\\t{}\\n'.format(cbumi[0], cbumi[1],\n                    count))\n","38":"def export_vcf(segments, ploidy, is_reference_male, is_sample_female,\n    sample_id=None, cnarr=None):\n    \"\"\"Convert segments to Variant Call Format.\n\n    For now, only 1 sample per VCF. (Overlapping CNVs seem tricky.)\n\n    Spec: https:\/\/samtools.github.io\/hts-specs\/VCFv4.2.pdf\n    \"\"\"\n    vcf_columns = ['#CHROM', 'POS', 'ID', 'REF', 'ALT', 'QUAL', 'FILTER',\n        'INFO', 'FORMAT', sample_id or segments.sample_id]\n    if cnarr:\n        segments = assign_ci_start_end(segments, cnarr)\n    vcf_rows = segments2vcf(segments, ploidy, is_reference_male,\n        is_sample_female)\n    table = pd.DataFrame.from_records(vcf_rows, columns=vcf_columns)\n    vcf_body = table.to_csv(sep='\\t', header=True, index=False,\n        float_format='%.3g')\n    return VCF_HEADER, vcf_body\n","39":"def summarize_info(all_logr, all_depths):\n    \"\"\"Average & spread of log2ratios and depths for a group of samples.\n\n    Can apply to all samples, or a given cluster of samples.\n    \"\"\"\n    logging.info('Calculating average bin coverages')\n    cvg_centers = np.apply_along_axis(descriptives.biweight_location, 0,\n        all_logr)\n    depth_centers = np.apply_along_axis(descriptives.biweight_location, 0,\n        all_depths)\n    logging.info('Calculating bin spreads')\n    spreads = np.array([descriptives.biweight_midvariance(a, initial=i) for\n        a, i in zip(all_logr.T, cvg_centers)])\n    result = {'log2': cvg_centers, 'depth': depth_centers, 'spread': spreads}\n    return result\n","40":"def model_compounds(compound_entry_list):\n    \"\"\"\n    Function to sort the downloaded\tkegg object into a format\n    that is\tcompatible with the psamm api for storage in\n    a compounds.yaml file.\n    \"\"\"\n    non_gen_compounds = []\n    for compound in compound_entry_list:\n        try:\n            form = Formula.parse(str(compound.formula))\n            if form.is_variable():\n                continue\n            elif compound.formula is None:\n                continue\n            elif 'R' in str(compound.formula):\n                continue\n            else:\n                d = OrderedDict()\n                d['id'] = compound.id\n                non_gen_compounds.append(compound.id)\n                if hasattr(compound, 'name') and compound.name is not None:\n                    d['name'] = compound.name\n                if hasattr(compound, 'names') and compound.names is not None:\n                    names_l = []\n                    for i in compound.names:\n                        names_l.append(i)\n                    d['names'] = names_l\n                if hasattr(compound, 'formula'\n                    ) and compound.formula is not None:\n                    d['formula'] = str(compound.formula)\n                if hasattr(compound, 'mol_weight'\n                    ) and compound.mol_weight is not None:\n                    d['mol_weight'] = compound.mol_weight\n                if hasattr(compound, 'comment'\n                    ) and compound.comment is not None:\n                    d['comment'] = str(compound.comment)\n                if hasattr(compound, 'dblinks'\n                    ) and compound.dblinks is not None:\n                    for key, value in compound.dblinks:\n                        if key != 'ChEBI':\n                            d['{}'.format(key)] = value\n                if hasattr(compound, 'chebi') and compound.chebi is not None:\n                    d['ChEBI'] = compound.chebi\n                if hasattr(compound, 'chebi_all'\n                    ) and compound.chebi_all is not None:\n                    d['ChEBI_all'] = compound.chebi_all\n                if hasattr(compound, 'charge') and compound.charge is not None:\n                    d['charge'] = compound.charge\n                yield d\n        except ParseError:\n            logger.warning(\n                'import of {} failed and will not be imported into compounds.yaml or compounds_generic.yaml'\n                .format(compound.id))\n            continue\n","41":"def get_barcode_umis(read, cell_barcode=False):\n    \"\"\" extract the umi +\/- cell barcode from the read name where the barcodes\n    were extracted using umis\"\"\"\n    umi, cell = None, None\n    try:\n        read_name_elements = read.qname.split(':')\n        for element in read_name_elements:\n            if element.startswith('UMI_'):\n                umi = element[4:].encode('utf-8')\n            elif element.startswith('CELL_') and cell_barcode:\n                cell = element[5:].encode('utf-8')\n        if umi is None:\n            raise ValueError()\n        return umi, cell\n    except:\n        raise ValueError(\n            'Could not extract UMI +\/- cell barcode from the read tag')\n","42":"def _skat_parse_line(line, markers_of_interest, samples, gender=None):\n    \"\"\"Parses a single line of the Impute2 file.\n\n    Args:\n        line (str): a line from the Impute2 file\n        markers_of_interest (set): a set of markers that are required for the\n                                   analysis\n        samples (pandas.DataFrame): contains the samples IDs (this is useful to\n                                    make sure we return a dosage vector with\n                                    the appropriate data)\n\n    Returns:\n        tuple: Either None if the marker is not of interest or a tuple of\n               ``(name , dosage_vector)`` where ``name`` is a ``str``\n               representing the variant ID and ``dosage_vector`` is a numpy\n               array containing the dosage values for every sample in the\n               ``samples`` dataframe.\n\n    \"\"\"\n    line = line.split(' ')\n    info_tuple, proba_matrix = impute2.matrix_from_line(line)\n    chrom, name, pos, a1, a2 = info_tuple\n    if name not in markers_of_interest:\n        return None\n    maf, minor_i, major_i = impute2.maf_from_probs(prob_matrix=proba_matrix,\n        a1=0, a2=2, gender=gender, site_name=name)\n    dosage = impute2.dosage_from_probs(homo_probs=proba_matrix[:, minor_i],\n        hetero_probs=proba_matrix[:, 1])\n    return name, dosage\n","43":"def model_reactions(reaction_entry_list):\n    \"\"\"\n    Function to sort the downloaded kegg object into a format\n    that is compatible with the psamm api for storage in\n    a reactions.yaml file.\n    \"\"\"\n    for reaction in reaction_entry_list:\n        d = OrderedDict()\n        d['id'] = reaction.id\n        enzymes_list = []\n        for i in reaction.enzymes:\n            enzymes_list.append(i)\n        pathways_list = []\n        if reaction.pathways is not None:\n            for i in reaction.pathways:\n                pathways_list.append(i[1])\n        if len(pathways_list) == 0:\n            pathways_list = None\n        orth_list = []\n        for i in reaction.orthology:\n            orth_list.append(i)\n        if len(orth_list) == 0:\n            orth_list = None\n        if hasattr(reaction, 'name') and reaction.name is not None:\n            d['name'] = reaction.name\n        if hasattr(reaction, 'names') and reaction.names is not None:\n            names_l = []\n            for i in reaction.names:\n                names_l.append(i)\n            d['names'] = names_l\n        if hasattr(reaction, 'equation') and reaction.equation is not None:\n            d['equation'] = str(reaction.equation)\n        if hasattr(reaction, 'definition') and reaction.definition is not None:\n            d['KEGG_definition'] = reaction.definition\n        if hasattr(reaction, 'enzymes') and reaction.enzymes is not None:\n            d['enzymes'] = enzymes_list\n        if hasattr(reaction, 'pathways') and reaction.pathways is not None:\n            d['pathways'] = pathways_list\n        if hasattr(reaction, 'comment') and reaction.comment is not None:\n            d['comment'] = str(reaction.comment)\n        if hasattr(reaction, 'tcdb_family'\n            ) and reaction.tcdb_family is not None:\n            d['tcdb_family'] = str(reaction.tcdb_family)\n        if hasattr(reaction, 'substrates') and reaction.substrates is not None:\n            d['substrates'] = reaction.substrates\n        if hasattr(reaction, 'genes') and reaction.genes is not None:\n            d['genes'] = str(reaction.genes)\n        if hasattr(reaction, 'orthology') and reaction.orthology is not None:\n            d['orthology'] = orth_list\n        yield d\n","44":"def check_dir(raw, default, expected, sequence=False):\n    \"\"\"\n    Check if value point to a directory\n\n    :param str raw: the value return by the user\n    :param str default: the default value for the option\n    :param expected: not used here to have the same signature for all check_xxx functions\n    :return: value\n    :raise MacsypyError: if the value cannot be cast in right type\n    \"\"\"\n\n    def path(value):\n        if os.path.exists(value):\n            if os.path.isdir(value):\n                return value\n            else:\n                raise ValueError(f\"'{value}' is not a directory.\")\n        else:\n            raise ValueError(f\"'{value}' no such file or directory.\")\n    return _validator(path, raw, default, sequence=sequence)\n","45":"def validate_module(pipeline, module_num, callback):\n    \"\"\"Validate a module and execute the callback on error on the main thread\n\n    pipeline - a pipeline to be validated\n    module_num - the module number of the module to be validated\n    callback - a callback with the signature, \"fn(setting, message, pipeline_data)\"\n    where setting is the setting that is in error and message is the message to\n    display.\n    \"\"\"\n    modules = [m for m in pipeline.modules() if m.module_num == module_num]\n    if len(modules) != 1:\n        return\n    module = modules[0]\n    level = logging.INFO\n    setting_idx = None\n    message = None\n    try:\n        level = logging.ERROR\n        module.test_valid(pipeline)\n        level = logging.WARNING\n        module.test_module_warnings(pipeline)\n        level = logging.INFO\n    except ValidationError as instance:\n        message = instance.message\n        setting_idx = [m.key() for m in module.visible_settings()].index(\n            instance.get_setting().key())\n    except Exception as e:\n        LOGGER.error('Error in validation thread', e)\n    wx.CallAfter(callback, setting_idx, message, level)\n","46":"@click.command()\n@click.argument('fastq', required=True)\n@click.option('--out_dir', default='.')\n@click.option('--nedit', default=0)\n@click.option('--barcodes', type=click.File('r'), required=False)\ndef demultiplex_samples(fastq, out_dir, nedit, barcodes):\n    \"\"\" Demultiplex a fastqtransformed FASTQ file into a FASTQ file for\n    each sample.\n    \"\"\"\n    annotations = detect_fastq_annotations(fastq)\n    re_string = construct_transformed_regex(annotations)\n    parser_re = re.compile(re_string)\n    if barcodes:\n        barcodes = set(barcode.strip() for barcode in barcodes)\n    else:\n        barcodes = set()\n    if nedit == 0:\n        filter_bc = partial(exact_sample_filter, barcodes=barcodes)\n    else:\n        barcodehash = MutationHash(barcodes, nedit)\n        filter_bc = partial(correcting_sample_filter, barcodehash=barcodehash)\n    sample_set = set()\n    batch = collections.defaultdict(list)\n    parsed = 0\n    safe_makedir(out_dir)\n    for read in read_fastq(fastq):\n        parsed += 1\n        read = filter_bc(read)\n        if not read:\n            continue\n        match = parser_re.search(read).groupdict()\n        sample = match['SB']\n        sample_set.add(sample)\n        batch[sample].append(read)\n        if not parsed % 10000000:\n            for sample, reads in batch.items():\n                out_file = os.path.join(out_dir, sample + '.fq')\n                with open(out_file, 'a') as out_handle:\n                    for read in reads:\n                        fixed = filter_bc(read)\n                        if fixed:\n                            out_handle.write(fixed)\n            batch = collections.defaultdict(list)\n    for sample, reads in batch.items():\n        out_file = os.path.join(out_dir, sample + '.fq')\n        with open(out_file, 'a') as out_handle:\n            for read in reads:\n                fixed = filter_bc(read)\n                if fixed:\n                    out_handle.write(read)\n","47":"def integrated_autocorrelation_time(A_n, B_n=None, fast=False, mintime=3):\n    \"\"\"Estimate the integrated autocorrelation time.\n\n    See Also\n    --------\n    statisticalInefficiency\n\n    \"\"\"\n    g = statistical_inefficiency(A_n, B_n, fast, mintime)\n    tau = (g - 1.0) \/ 2.0\n    return tau\n","48":"def get_barcode_read_id(read, cell_barcode=False, sep='_'):\n    \"\"\" extract the umi +\/- cell barcode from the read id using the\n    specified separator \"\"\"\n    try:\n        if cell_barcode:\n            umi = read.qname.split(sep)[-1].encode('utf-8')\n            cell = read.qname.split(sep)[-2].encode('utf-8')\n        else:\n            umi = read.qname.split(sep)[-1].encode('utf-8')\n            cell = None\n        return umi, cell\n    except:\n        raise ValueError(\n            'Could not extract UMI +\/- cell barcode from the readID, please check UMI is encoded in the read name'\n            )\n","49":"def do_list(args: argparse.Namespace) ->None:\n    \"\"\"\n    List installed models.\n\n    :param args: the arguments passed on the command line\n    :type args: :class:`argparse.Namespace` object\n    :rtype: None\n    \"\"\"\n    registry = _find_all_installed_packages(models_dir=args.models_dir)\n    for model_loc in registry.models():\n        try:\n            pack = Package(model_loc.path)\n            pack_vers = pack.metadata['vers']\n            model_path = f'   ({model_loc.path})' if args.long else ''\n            if args.outdated or args.uptodate:\n                remote = RemoteModelIndex(org=args.org)\n                all_versions = remote.list_package_vers(pack.name)\n                specifier = specifiers.SpecifierSet(f'>{pack_vers}')\n                update_vers = list(specifier.filter(all_versions))\n                if args.outdated and update_vers:\n                    print(\n                        f'{model_loc.name}-{update_vers[0]} [{pack_vers}]{model_path}'\n                        )\n                if args.uptodate and not update_vers:\n                    print(f'{model_loc.name}-{pack_vers}{model_path}')\n            else:\n                print(f'{model_loc.name}-{pack_vers}{model_path}')\n        except Exception as err:\n            if args.verbose > 1:\n                _log.warning(str(err))\n","50":"def solve_mbar_for_all_states(u_kn, N_k, f_k, states_with_samples,\n    solver_protocol):\n    \"\"\"Solve for free energies of states with samples, then calculate for\n    empty states.\n\n    Parameters\n    ----------\n    u_kn : np.ndarray, shape=(n_states, n_samples), dtype='float'\n        The reduced potential energies, i.e. -log unnormalized probabilities\n    N_k : np.ndarray, shape=(n_states), dtype='int'\n        The number of samples in each state\n    f_k : np.ndarray, shape=(n_states), dtype='float'\n        The reduced free energies of each state\n    solver_protocol : tuple(dict()), optional, default=None\n        Sequence of dictionaries of steps in solver protocol for final\n        stage of refinement.\n\n    Returns\n    -------\n    f_k : np.ndarray, shape=(n_states), dtype='float'\n        The free energies of states\n    \"\"\"\n    if len(states_with_samples) == 1:\n        f_k_nonzero = np.array([0.0])\n    else:\n        f_k_nonzero, all_results = solve_mbar(u_kn[states_with_samples],\n            N_k[states_with_samples], f_k[states_with_samples],\n            solver_protocol=solver_protocol)\n    f_k[states_with_samples] = np.array(f_k_nonzero)\n    f_k = self_consistent_update(u_kn, N_k, f_k)\n    f_k -= f_k[0]\n    return f_k\n","51":"def fit_mixedlm(data, formula, use_ml, groups, result_col, random_effects,\n    mixedlm_p, interaction, **kwargs):\n    \"\"\"Fit a linear mixed effects model to the data.\n\n    Args:\n        data (pandas.DataFrame): the data to analyse\n        formula (str): the formula for the linear mixed effects model\n        use_ml (bool): whether to use ML instead of REML\n        groups (str): the column containing the groups\n        result_col (str): the column that will contain the results\n        random_effects (pandas.Series): the random effects\n        mixedlm_p (float): the p-value threshold for which loci will be\n                           computed with the real MixedLM analysis\n        interaction (bool): Whether there is an interaction or not\n\n    Returns:\n        list: the results from the linear mixed effects model\n\n    \"\"\"\n    if not interaction:\n        geno = data.reset_index()[['index', '_GenoD']]\n        indexes = geno[['index']].drop_duplicates().index\n        geno = geno.loc[indexes, :]\n        t_data = pd.merge(random_effects, geno.set_index('index'),\n            left_index=True, right_index=True)\n        approximate_r = _get_result_from_linear(smf.ols(formula=\n            'RE ~ _GenoD', data=t_data).fit(), result_col='_GenoD')\n        if approximate_r[5] >= mixedlm_p:\n            result = ['NA'] * (len(approximate_r) - 2)\n            result.append(approximate_r[5])\n            result.append('TS-MixedLM')\n            return result\n    result = _get_result_from_logistic_mixedlm(smf.mixedlm(formula=formula,\n        data=data, groups=groups).fit(reml=not use_ml), result_col=result_col)\n    result.append('MixedLM')\n    return result\n","52":"@on_array(0)\ndef median_absolute_deviation(a, scale_to_sd=True):\n    \"\"\"Compute the median absolute deviation (MAD) of array elements.\n\n    The MAD is defined as: ``median(abs(a - median(a)))``.\n\n    See: https:\/\/en.wikipedia.org\/wiki\/Median_absolute_deviation\n    \"\"\"\n    a_median = np.median(a)\n    mad = np.median(np.abs(a - a_median))\n    if scale_to_sd:\n        mad *= 1.4826\n    return mad\n","53":"def _reaction_to_dicts(reaction):\n    \"\"\"Convert a reaction to reduced left, right dictionaries.\n\n    Returns a pair of (left, right) dictionaries mapping compounds to\n    normalized integer stoichiometric values. If a compound occurs multiple\n    times on one side, the occurences are combined into a single entry in the\n    dictionary.\n    \"\"\"\n\n    def dict_from_iter_sum(it, div):\n        d = {}\n        for k, v in it:\n            if k not in d:\n                d[k] = 0\n            d[k] += int(v \/ div)\n        return d\n    div = reduce(gcd, (abs(v) for _, v in reaction.compounds), 0)\n    if div == 0:\n        raise ValueError('Empty reaction')\n    left = dict_from_iter_sum(reaction.left, div)\n    right = dict_from_iter_sum(reaction.right, div)\n    return left, right\n","54":"@on_array(0)\ndef mean_squared_error(a, initial=None):\n    \"\"\"Mean squared error (MSE).\n\n    By default, assume the input array `a` is the residuals\/deviations\/error,\n    so MSE is calculated from zero. Another reference point for calculating the\n    error can be specified with `initial`.\n    \"\"\"\n    if initial is None:\n        initial = a.mean()\n    if initial:\n        a = a - initial\n    return (a ** 2).mean()\n","55":"def parse_tparam_file(file):\n    \"\"\"Parse a transport parameter file.\n\n    This file contains reaction IDs, net charge transported into\n    the cell, and net protons transported into the cell.\n\n    \"\"\"\n    t_param = {}\n    if file is not None:\n        for row in csv.reader(file, delimiter=str('\\t')):\n            rxn, c, h = row\n            rxn = convert_to_unicode(rxn)\n            t_param[rxn] = Decimal(c), Decimal(h)\n            t_param[u'{}_forward'.format(rxn)] = Decimal(c), Decimal(h)\n            t_param[u'{}_reverse'.format(rxn)] = -Decimal(c), -Decimal(h)\n    return t_param\n","56":"@on_array(0)\ndef gapper_scale(a):\n    \"\"\"Scale estimator based on gaps between order statistics.\n\n    See:\n\n    - Wainer & Thissen (1976)\n    - Beers, Flynn, and Gebhardt (1990)\n    \"\"\"\n    gaps = np.diff(np.sort(a))\n    n = len(a)\n    idx = np.arange(1, n)\n    weights = idx * (n - idx)\n    return (gaps * weights).sum() * np.sqrt(np.pi) \/ (n * (n - 1))\n","57":"def fill_blast_fasta(blast_db, fasta_path, shard_params):\n    \"\"\"\n    Fill the fasta file used as input into blast.\n\n    Use sequences from the sqlite3 DB. We use the shard partitions passed in to\n    determine which sequences to get for this shard.\n    \"\"\"\n    with db.connect(blast_db) as cxn:\n        limit, offset = shard_params\n        with open(fasta_path, 'w') as fasta_file:\n            for row in db_preprocessor.get_sequences_in_shard(cxn, limit,\n                offset):\n                util.write_fasta_record(fasta_file, row[0], row[2], row[1])\n","58":"@click.command()\n@click.argument('fastq', required=True)\n@click.option('--out_dir', default='.')\n@click.option('--readnumber', default='')\n@click.option('--prefix', default='')\n@click.option('--cb_histogram', default=None)\n@click.option('--cb_cutoff', default=0)\ndef demultiplex_cells(fastq, out_dir, readnumber, prefix, cb_histogram,\n    cb_cutoff):\n    \"\"\" Demultiplex a fastqtransformed FASTQ file into a FASTQ file for\n    each cell.\n    \"\"\"\n    annotations = detect_fastq_annotations(fastq)\n    re_string = construct_transformed_regex(annotations)\n    parser_re = re.compile(re_string)\n    readstring = '' if not readnumber else '_R{}'.format(readnumber)\n    filestring = '{prefix}{sample}{readstring}.fq'\n    cb_set = set()\n    if cb_histogram:\n        cb_set = get_cb_depth_set(cb_histogram, cb_cutoff)\n    sample_set = set()\n    batch = collections.defaultdict(list)\n    parsed = 0\n    safe_makedir(out_dir)\n    for read in read_fastq(fastq):\n        parsed += 1\n        match = parser_re.search(read).groupdict()\n        sample = match['CB']\n        if cb_set and sample not in cb_set:\n            continue\n        sample_set.add(sample)\n        batch[sample].append(read)\n        if not parsed % 10000000:\n            for sample, reads in batch.items():\n                out_file = os.path.join(out_dir, filestring.format(**locals()))\n                with open(out_file, 'a') as out_handle:\n                    for read in reads:\n                        out_handle.write(read)\n            batch = collections.defaultdict(list)\n    for sample, reads in batch.items():\n        out_file = os.path.join(out_dir, filestring.format(**locals()))\n        with open(out_file, 'a') as out_handle:\n            for read in reads:\n                out_handle.write(read)\n","59":"def _reference_copies_pure(chrom, ploidy, is_reference_male):\n    \"\"\"Determine the reference number of chromosome copies (pure sample).\n\n    Returns\n    -------\n    int\n        Number of copies in the reference.\n    \"\"\"\n    chrom = chrom.lower()\n    if chrom in ['chry', 'y'] or is_reference_male and chrom in ['chrx', 'x']:\n        ref_copies = ploidy \/\/ 2\n    else:\n        ref_copies = ploidy\n    return ref_copies\n","60":"def parse_rxns_from_KO(rxn_mapping, out, verbose):\n    \"\"\"\n    Functions converts gene associations to EC into gene\n    associations for reaction IDs. Returns a dictionary\n    of Reaction IDs to genes.\n    \"\"\"\n    rxn_dict = defaultdict(lambda : [])\n    if verbose:\n        logger.info('Downloading reactions associated with KO...')\n        logger.info('There are {} KOs download'.format(len(rxn_mapping)))\n        count = 0\n    with open(os.path.join(out, 'log.tsv'), 'a+') as f:\n        for reactions in rxn_mapping:\n            if verbose:\n                if count % 25 == 0:\n                    logger.info('{}\/{} have been downloaded'.format(count,\n                        len(rxn_mapping)))\n                count += 1\n            try:\n                request = REST.kegg_get(reactions)\n            except HTTPError:\n                f.write(''.join(['  - ', reactions, '\\n']))\n                continue\n            entry_line = None\n            section_id = None\n            reaction = {}\n            for lineno, line in enumerate(request):\n                line = line.rstrip()\n                if line == '\/\/\/':\n                    continue\n                if entry_line is None:\n                    entry_line = lineno\n                m = re.match('([A-Z_]+)\\\\s+(.*)', line.rstrip())\n                if m is not None:\n                    section_id = m.group(1).lower()\n                    reaction[section_id] = [m.group(2)]\n                elif section_id is not None:\n                    reaction[section_id].append(line.strip())\n                else:\n                    raise ParseError2(\n                        'Missing section identifier at line                         {}'\n                        .format(lineno))\n            if 'dblinks' in reaction:\n                for i in reaction['dblinks']:\n                    if i[0:2] == 'RN':\n                        listall = re.split(' ', i[1:])\n                        for r in listall:\n                            if r[0] == 'R':\n                                rxn_dict[r] += rxn_mapping[reactions]\n    return rxn_dict\n","61":"def dedupe_tx(dframe):\n    \"\"\"Deduplicate table rows to select one transcript length per gene.\n\n    Choose the lowest-number Entrez ID and the transcript with the greatest\n    support (primarily) and length (secondarily).\n\n    This is done at the end of Ensembl ID deduplication, after filtering on gene\n    names and for single-row tables.\n\n    Returns an integer row index corresponding to the original table.\n    \"\"\"\n    return dframe.sort_values(['entrez_id', 'tx_support', 'tx_length'],\n        ascending=[True, False, False], na_position='last').index[0]\n","62":"def write_figures(prefix, directory, dose_name, dose_data, data,\n    ec50_coeffs, feature_set, log_transform):\n    \"\"\"Write out figure scripts for each measurement\n\n    prefix - prefix for file names\n    directory - write files into this directory\n    dose_name - name of the dose measurement\n    dose_data - doses per image\n    data - data per image\n    ec50_coeffs - coefficients calculated by calculate_ec50\n    feature_set - tuples of object name and feature name in same order as data\n    log_transform - true to log-transform the dose data\n    \"\"\"\n    from matplotlib.figure import Figure\n    from matplotlib.backends.backend_pdf import FigureCanvasPdf\n    if log_transform:\n        dose_data = numpy.log(dose_data)\n    for i, (object_name, feature_name) in enumerate(feature_set):\n        fdata = data[:, i]\n        fcoeffs = ec50_coeffs[i, :]\n        filename = '%s%s_%s.pdf' % (prefix, object_name, feature_name)\n        pathname = os.path.join(directory, filename)\n        f = Figure()\n        canvas = FigureCanvasPdf(f)\n        ax = f.add_subplot(1, 1, 1)\n        x = numpy.linspace(0, numpy.max(dose_data), num=100)\n        y = sigmoid(fcoeffs, x)\n        ax.plot(x, y)\n        dose_y = sigmoid(fcoeffs, dose_data)\n        ax.plot(dose_data, dose_y, 'o')\n        ax.set_xlabel('Dose')\n        ax.set_ylabel('Response')\n        ax.set_title('%s_%s' % (object_name, feature_name))\n        f.savefig(pathname)\n","63":"def add_dividing_lines(labels):\n    \"\"\"\n    Remove pixels from an object that are adjacent to\n    another object\u2019s pixels unless doing so would change the object\u2019s\n    Euler number\n    \"\"\"\n    adjacent_mask = centrosome.cpmorphology.adjacent(labels)\n    thinnable_mask = centrosome.cpmorphology.binary_shrink(labels, 1) != 0\n    out_labels = labels.copy()\n    out_labels[adjacent_mask & ~thinnable_mask] = 0\n    return out_labels\n","64":"def get_formula(phenotype, covars, interaction, gender_c, categorical):\n    \"\"\"Creates the linear\/logistic regression formula (for statsmodel).\n\n    Args:\n        phenotype (str): the phenotype column\n        covars (list): the list of co variable columns\n        interaction (str): the interaction column\n\n    Returns:\n        str: the formula for the statistical analysis\n\n    Note\n    ----\n        The phenotype column needs to be specified. The list of co variables\n        might be empty (if no co variables are necessary). The interaction\n        column can be set to ``None`` if there is no interaction.\n\n    Note\n    ----\n        The gender column should be categorical (hence, the formula requires\n        the gender to be included into ``C()``, *e.g.* ``C(Gender)``).\n\n    \"\"\"\n    formula = '{} ~ _GenoD'.format(phenotype)\n    for covar in covars:\n        if covar == gender_c or covar in categorical:\n            covar = 'C({})'.format(covar)\n        formula += ' + ' + covar\n    if interaction is not None:\n        if interaction == gender_c or interaction in categorical:\n            interaction = 'C({})'.format(interaction)\n        formula += ' + _GenoD*{}'.format(interaction)\n    return formula\n","65":"def get_plink_version(binary):\n    \"\"\"Gets the Plink version from the binary.\n\n    Args:\n        binary (str): the name of the Plink binary\n\n    Returns:\n        str: the version of the Plink software\n\n    This function uses :py:class:`subprocess.Popen` to gather the version of\n    the Plink binary. Since executing the software to gather the version\n    creates an output file, it is deleted.\n\n    Warning\n    -------\n        This function only works as long as the version is returned as\n        ``| PLINK! | NNN |`` (where, ``NNN`` is the version), since we use\n        regular expresion to extract the version number from the standard\n        output of the software.\n\n    \"\"\"\n    command = [binary, '--noweb']\n    proc = Popen(command, stdout=PIPE, stderr=PIPE)\n    output = proc.communicate()[0].decode()\n    if os.path.isfile('plink.log'):\n        os.remove('plink.log')\n    version = re.search('\\\\|\\\\s+PLINK!\\\\s+\\\\|\\\\s+(\\\\S+)\\\\s+\\\\|', output)\n    if version is None:\n        version = 'unknown'\n    else:\n        version = version.group(1)\n    logging.info('Will be using Plink version {}'.format(version))\n    return version\n","66":"def default_weight(element):\n    \"\"\"Return weight of formula element.\n\n    This implements the default weight proposed for MapMaker.\n    \"\"\"\n    if element in (Atom.N, Atom.O, Atom.P):\n        return 0.4\n    elif isinstance(element, Radical):\n        return 40.0\n    return 1.0\n","67":"def pdbfix_templates(templates_full_seq, process_only_these_templates=None,\n    overwrite_structures=False):\n    \"\"\"\n    Parameters\n    ----------\n    templates_full_seq: list of BioPython SeqRecord\n        full UniProt sequence for span of the template (including unresolved residues)\n    process_only_these_templates: list of str\n    overwrite_structures: bool\n    Returns\n    -------\n    missing_residues_list: list of list of OpenMM Residue\n    \"\"\"\n    missing_residues_sublist = []\n    ntemplates = len(templates_full_seq)\n    for template_index in range(mpistate.rank, ntemplates, mpistate.size):\n        template_full_seq = templates_full_seq[template_index]\n        if (process_only_these_templates and template_full_seq.id not in\n            process_only_these_templates):\n            missing_residues_sublist.append(None)\n            continue\n        missing_residues_sublist.append(pdbfix_template(template_full_seq,\n            overwrite_structures=overwrite_structures))\n    missing_residues_gathered = mpistate.comm.gather(missing_residues_sublist,\n        root=0)\n    missing_residues_list = []\n    if mpistate.rank == 0:\n        missing_residues_list = [None] * ntemplates\n        for template_index in range(ntemplates):\n            missing_residues_list[template_index] = missing_residues_gathered[\n                template_index % mpistate.size][template_index \/\/ mpistate.size\n                ]\n    missing_residues_list = mpistate.comm.bcast(missing_residues_list, root=0)\n    return missing_residues_list\n","68":"def check_inputs(x, width, as_series=True, weights=None):\n    \"\"\"Transform width into a half-window size.\n\n    `width` is either a fraction of the length of `x` or an integer size of the\n    whole window. The output half-window size is truncated to the length of `x`\n    if needed.\n    \"\"\"\n    x = np.asfarray(x)\n    wing = _width2wing(width, x)\n    signal = _pad_array(x, wing)\n    if as_series:\n        signal = pd.Series(signal)\n    if weights is None:\n        return x, wing, signal\n    weights = _pad_array(weights, wing)\n    weights[:wing] *= np.linspace(1 \/ wing, 1, wing)\n    weights[-wing:] *= np.linspace(1, 1 \/ wing, wing)\n    if as_series:\n        weights = pd.Series(weights)\n    return x, wing, signal, weights\n","69":"def extract_names(reference: str) ->list:\n    \"\"\"\n    This function attempts to extract the chromosome names from a fasta file\n    :param reference: The fasta file to analyze\n    :return: A list of chromosome names\n    \"\"\"\n    ref_names = []\n    absolute_reference_path = pathlib.Path(reference)\n    if absolute_reference_path.suffix == '.gz':\n        with gzip.open(absolute_reference_path, 'rt') as ref:\n            for line in ref:\n                if line.startswith('>'):\n                    ref_names.append(line[1:].rstrip())\n    else:\n        with open(absolute_reference_path, 'r') as ref:\n            for line in ref:\n                if line.startswith('>'):\n                    ref_names.append(line[1:].rstrip())\n    if not ref_names:\n        print(\n            'Malformed fasta file. Missing properly formatted chromosome names.\\n'\n            )\n        sys.exit(1)\n    return ref_names\n","70":"def find_splice(cigar):\n    \"\"\"Takes a cigar string and finds the first splice position as\n    an offset from the start. To find the 5' end (read coords) of\n    the junction for a reverse read, pass in the reversed cigar tuple\"\"\"\n    offset = 0\n    if cigar[0][0] == 4:\n        offset = cigar[0][1]\n        cigar = cigar[1:]\n    for op, bases in cigar:\n        if op in (3, 4):\n            return offset\n        elif op in (0, 2, 7, 8):\n            offset += bases\n        elif op in (1, 5, 6):\n            continue\n        else:\n            raise ValueError('Bad Cigar operation: %i' % op)\n    return False\n","71":"def mean_ind_of_weighted_list(candidate_list: list) ->int:\n    \"\"\"\n    Returns the index of the mean of a weighted list\n\n    :param candidate_list: weighted list\n    :return: index of mean\n    \"\"\"\n    my_mid = sum(candidate_list) \/ 2.0\n    my_sum = 0.0\n    for i in range(len(candidate_list)):\n        my_sum += candidate_list[i]\n        if my_sum >= my_mid:\n            return i\n","72":"def _skat_run_job(script_filename):\n    \"\"\"Calls Rscript with the generated script and parses the results.\n\n    Args:\n        script_filename (str): the name of the script\n\n    Returns:\n        tuple: two values: the *p-value* and the *q-value* (for SKAT-O, the\n               *q-value* is set to None)\n\n    The results should be somewhere in the standard output. The expected\n    format is: ::\n\n        _PYTHON_HOOK_QVAL:[0.123]\n        _PYTHON_HOOK_PVAL:[0.123]\n\n    If the template script is modified, this format should still be respected.\n\n    It is also noteworthy that this function uses ``Rscript`` to run the\n    analysis. Hence, it should be in the path when using the imputed_stats\n    skat mode.\n\n    \"\"\"\n    proc = Popen(['Rscript', script_filename], stdout=PIPE, stderr=PIPE)\n    out, err = proc.communicate()\n    if err:\n        logging.info('SKAT Warning: ' + err.decode('utf-8'))\n    out = out.decode('utf-8')\n    p_match = re.search('_PYTHON_HOOK_PVAL:\\\\[(.+)\\\\]', out)\n    if p_match is None:\n        raise GenipeError(\n            \"SKAT did not return properly. See script '{}' for details.\".\n            format(script_filename))\n    q_match = re.search('_PYTHON_HOOK_QVAL:\\\\[(.+)\\\\]', out)\n    if q_match is None:\n        raise GenipeError(\n            \"SKAT did not return properly. See script '{}' for details.\".\n            format(script_filename))\n    if q_match.group(1) == 'NA':\n        return float(p_match.group(1)), None\n    else:\n        return float(p_match.group(1)), float(q_match.group(1))\n","73":"def adjust_window_size(desired_window_size, iter_len, minimum=29):\n    \"\"\"\n\n    Dynamically adjusts the sliding window size depending on the total\n    length of values. When there are few values (below 1\/5 of the\n    window size), the window size is decreased.\n\n    \"\"\"\n    if desired_window_size < iter_len \/\/ 5:\n        adjusted_window_size = desired_window_size\n    else:\n        adjusted_window_size = desired_window_size \/\/ 5\n        if adjusted_window_size < minimum:\n            adjusted_window_size = minimum\n    if adjusted_window_size != desired_window_size:\n        print(\n            'Adjusted window size from {0} to {1} because there are only {2} PSMs.'\n            .format(desired_window_size, adjusted_window_size, iter_len))\n    return adjusted_window_size\n","74":"def get_chrom_encoding(reference):\n    \"\"\"Gets the chromosome's encoding (e.g. 1 vs chr1, X vs 23, etc).\n\n    Args:\n        reference (pyfaidx.Fasta): the reference\n\n    Returns:\n        dict: the chromosome encoding\n\n    The encoding is a dictionary where numerical autosomes from 1 to 22 are\n    the keys, and the encoded autosomes (the one present in the reference)\n    are the values. An example would be ``1 -> chr1``.\n\n    \"\"\"\n    encoding = {}\n    for chrom in range(1, 23):\n        if str(chrom) in reference:\n            encoding[str(chrom)] = str(chrom)\n        elif 'chr{}'.format(chrom) in reference:\n            encoding[str(chrom)] = 'chr{}'.format(chrom)\n    for num_chrom, char_chrom in zip(['23', '24'], ['X', 'Y']):\n        if num_chrom in reference:\n            encoding[num_chrom] = num_chrom\n        elif char_chrom in reference:\n            encoding[num_chrom] = char_chrom\n        elif 'chr' + char_chrom in reference:\n            encoding[num_chrom] = 'chr' + char_chrom\n        elif 'chr' + num_chrom in reference:\n            encoding[num_chrom] = 'chr' + num_chrom\n    possibilities = ['26', 'M', 'MT', 'chrM', 'chrMT', 'chr26']\n    for possibility in possibilities:\n        if possibility in reference:\n            encoding['26'] = possibility\n            break\n    for chrom in range(1, 27):\n        if chrom != 25 and str(chrom) not in encoding:\n            logging.warning('{}: chromosome not in reference'.format(chrom))\n    return encoding\n","75":"def naive_bayes(prob_list):\n    \"\"\"\n    Combines independent probabilities a, b, c\n    like this:\n\n                            a*b*c\n    combined_prob = -------------------------\n                    a*b*c + (1-a)*(1-b)*(1-c)\n\n    For a straightforward explanation, see\n    http:\/\/www.paulgraham.com\/naivebayes.html\n    \"\"\"\n    multiplied_probs = functools.reduce(operator.mul, prob_list, 1)\n    multiplied_opposite_probs = functools.reduce(operator.mul, (1 - p for p in\n        prob_list), 1)\n    return multiplied_probs \/ (multiplied_probs + multiplied_opposite_probs)\n","76":"def calculate_ec50(conc, responses, Logarithmic):\n    \"\"\"EC50 Function to fit a dose-response data to a 4 parameter dose-response\n       curve.\n\n       Inputs: 1. a 1 dimensional array of drug concentrations\n               2. the corresponding m x n array of responses\n       Algorithm: generate a set of initial coefficients including the Hill\n                  coefficient\n                  fit the data to the 4 parameter dose-response curve using\n                  nonlinear least squares\n       Output: a matrix of the 4 parameters\n               results[m,1]=min\n               results[m,2]=max\n               results[m,3]=ec50\n               results[m,4]=Hill coefficient\n\n       Original Matlab code Copyright 2004 Carlos Evangelista\n       send comments to CCEvangelista@aol.com\n       \"\"\"\n    if Logarithmic:\n        conc = numpy.log(conc)\n    n = responses.shape[1]\n    results = numpy.zeros((n, 4))\n\n    def error_fn(v, x, y):\n        \"\"\"Least-squares error function\n\n        This measures the least-squares error of fitting the sigmoid\n        with parameters in v to the x and y data.\n        \"\"\"\n        return numpy.sum((sigmoid(v, x) - y) ** 2)\n    for i in range(n):\n        response = responses[:, i]\n        v0 = calc_init_params(conc, response)\n        v = scipy.optimize.fmin(error_fn, v0, args=(conc, response),\n            maxiter=1000, maxfun=1000, disp=False)\n        results[i, :] = v\n    return results\n","77":"def _merge_overlapping(table, bp: int, combine: Dict[str, Callable]):\n    \"\"\"Merge overlapping regions within a chromosome\/strand.\n\n    Assume chromosome and (if relevant) strand are already identical, so only\n    start and end coordinates are considered.\n    \"\"\"\n    merged_rows = [_squash_tuples(row_group, combine) for row_group in\n        _nonoverlapping_groups(table, bp)]\n    return pd.DataFrame.from_records(merged_rows, columns=merged_rows[0].\n        _fields)\n","78":"def multiple_sequence_alignment(seqs_fp, threads=1):\n    \"\"\"Perform multiple sequence alignment on FASTA file using MAFFT.\n\n    Parameters\n    ----------\n    seqs_fp: string\n        filepath to FASTA file for multiple sequence alignment\n    threads: integer, optional\n        number of threads to use. 0 to use all threads\n\n    Returns\n    -------\n    msa_fp : str\n        name of output alignment file or None if error encountered\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    logger.info('multiple_sequence_alignment seqs file %s' % seqs_fp)\n    if threads == 0:\n        threads = -1\n    if stat(seqs_fp).st_size == 0:\n        logger.warning('msa failed. file %s has no reads' % seqs_fp)\n        return None\n    msa_fp = seqs_fp + '.msa'\n    params = ['mafft', '--quiet', '--preservecase', '--parttree', '--auto',\n        '--thread', str(threads), seqs_fp]\n    sout, serr, res = _system_call(params, stdoutfilename=msa_fp)\n    if not res == 0:\n        logger.info('msa failed for file %s (maybe only 1 read?)' % seqs_fp)\n        logger.debug('stderr : %s' % serr)\n        return None\n    return msa_fp\n","79":"def loc_shrink_mean_std(xcol, ymatr):\n    \"\"\"Compute mean and standard deviation per label\n\n    xcol - column of image labels or doses\n    ymatr - a matrix with rows of values per image and columns\n            representing different measurements\n\n    returns xs - a vector of unique doses\n            avers - the average value per label\n            stds - the standard deviation per label\n    \"\"\"\n    ncols = ymatr.shape[1]\n    labels, labnum, xs = loc_vector_labels(xcol)\n    avers = numpy.zeros((labnum, ncols))\n    stds = avers.copy()\n    for ilab in range(labnum):\n        labinds = labels == ilab\n        labmatr = ymatr[labinds, :]\n        if labmatr.shape[0] == 1:\n            avers[ilab, :] = labmatr[0, :]\n        else:\n            avers[ilab, :] = numpy.mean(labmatr, 0)\n            stds[ilab, :] = numpy.std(labmatr, 0)\n    return xs, avers, stds\n","80":"def rolling_outlier_std(x, width, stdevs):\n    \"\"\"Detect outliers by stdev within a rolling window.\n\n    Outliers are the array elements outside `stdevs` standard deviations from\n    the smoothed trend line, as calculated from the trend line residuals.\n\n    Returns\n    -------\n    np.array\n        A boolean array of the same size as `x`, where outlier indices are True.\n    \"\"\"\n    if len(x) <= width:\n        return np.zeros(len(x), dtype=np.bool_)\n    dists = x - savgol(x, width)\n    x_std = rolling_std(dists, width)\n    outliers = np.abs(dists) > x_std * stdevs\n    return outliers\n","81":"def guess_window_size(x, weights=None):\n    \"\"\"Choose a reasonable window size given the signal.\n\n    Inspired by Silverman's rule: bandwidth is proportional to signal's standard\n    deviation and the length of the signal ^ 4\/5.\n    \"\"\"\n    if weights is None:\n        sd = descriptives.biweight_midvariance(x)\n    else:\n        sd = descriptives.weighted_std(x, weights)\n    width = 4 * sd * len(x) ** (4 \/ 5)\n    width = max(3, int(round(width)))\n    width = min(len(x), width)\n    return width\n","82":"def blast_query_against_all_shards(log, assembler):\n    \"\"\"\n    Blast the query against the SRA databases.\n\n    We're using a map-reduce strategy here. We map the blasting of the query\n    sequences and reduce the output into one fasta file.\n    \"\"\"\n    log.info('Blasting query against shards: iteration {}'.format(assembler\n        .state['iteration']))\n    all_shards = shard_fraction(log, assembler)\n    with Pool(processes=assembler.args['cpus']) as pool:\n        results = [pool.apply_async(blast_query_against_one_shard, (\n            assembler.args, assembler.simple_state(), shard)) for shard in\n            all_shards]\n        all_results = [result.get() for result in results]\n    insert_blast_results(all_shards, assembler.args, assembler.simple_state\n        (), log)\n    log.info('All {} blast results completed'.format(len(all_results)))\n","83":"def initialize_population(reference: str, pop_size: int, chrom_names: list\n    ) ->list:\n    \"\"\"\n    The purpose of this function is to evolve the initial population of bacteria. All bacteria are stored as\n    Bacterium objects.\n    :param chrom_names: A list of contigs from the original fasta\n    :param reference: string path to the reference fasta file\n    :param pop_size: size of the population to initialize.\n    :return population: returns a list of bacterium objects.\n    \"\"\"\n    names = []\n    for j in range(pop_size):\n        names.append('bacterium_0_{}'.format(j + 1))\n    population = []\n    for i in range(pop_size):\n        new_member = Bacterium(reference, names[i], chrom_names)\n        population.append(new_member)\n    return population\n","84":"def parse_args(args):\n    \"\"\"\n    parse command line\n\n    :param args: the command line arguments\n    :type args: list of string\n    :return:\n    :rtype: :class:`argparse.Namespace` object\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    theme_option = parser.add_mutually_exclusive_group()\n    theme_option.add_argument('--no-color', action='store_true', default=False)\n    theme_option.add_argument('--white-bg', action='store_true', default=False)\n    theme_option.add_argument('--dark-bg', action='store_true', default=True)\n    parser.add_argument('--defaults', action='store_true', default=False,\n        help='Do not ask questions. Create config file with default values.')\n    parsed_args = parser.parse_args(args)\n    return parsed_args\n","85":"def filter_minreads_samples_from_table(table, minreads=1, inplace=True):\n    \"\"\"Filter samples from biom table that have less than\n    minreads reads total\n\n    Paraneters\n    ----------\n    table : biom.Table\n        the biom table to filter\n    minreads : int (optional)\n        the minimal number of reads in a sample in order to keep it\n    inplace : bool (optional)\n        if True, filter the biom table in place, if false create a new copy\n\n    Returns\n    -------\n    table : biom.Table\n        the filtered biom table\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    logger.debug('filter_minreads_started. minreads=%d' % minreads)\n    samp_sum = table.sum(axis='sample')\n    samp_ids = table.ids(axis='sample')\n    bad_samples = samp_ids[samp_sum < minreads]\n    if len(bad_samples) > 0:\n        logger.warning('removed %d samples with reads per sample<%d' % (len\n            (bad_samples), minreads))\n        table = table.filter(bad_samples, axis='sample', inplace=inplace,\n            invert=True)\n    else:\n        logger.debug('all samples contain > %d reads' % minreads)\n    return table\n","86":"def parse_rxns_from_EC(rxn_mapping, out, verbose):\n    \"\"\"\n    Functions converts gene associations to EC into gene\n    associations for reaction IDs. Returns a dictionary\n    of Reaction IDs to genes.\n    \"\"\"\n    rxn_dict = defaultdict(lambda : [])\n    if verbose:\n        logger.info('Downloading reactions associated with EC...')\n        logger.info('There are {} ECs download'.format(len(rxn_mapping)))\n        count = 0\n    with open(os.path.join(out, 'log.tsv'), 'a+') as f:\n        for reactions in rxn_mapping:\n            if verbose:\n                if count % 25 == 0:\n                    logger.info('{}\/{} have been downloaded'.format(count,\n                        len(rxn_mapping)))\n                count += 1\n            try:\n                request = REST.kegg_get(reactions)\n            except HTTPError:\n                f.write(''.join(['  - ', reactions, '\\n']))\n                continue\n            entry_line = None\n            section_id = None\n            reaction = {}\n            for lineno, line in enumerate(request):\n                line = line.rstrip()\n                if line == '\/\/\/':\n                    continue\n                if entry_line is None:\n                    entry_line = lineno\n                m = re.match('([A-Z_]+)\\\\s+(.*)', line.rstrip())\n                if m is not None:\n                    section_id = m.group(1).lower()\n                    reaction[section_id] = [m.group(2)]\n                elif section_id is not None:\n                    reaction[section_id].append(line.strip())\n                else:\n                    raise ParseError2('Missing section identifier at line {}'\n                        .format(lineno))\n            if 'all_reac' in reaction:\n                listall = re.split(' ', reaction['all_reac'][0])\n                for r in listall:\n                    if r[0] == 'R':\n                        rxn_dict[r] += rxn_mapping[reactions]\n    return rxn_dict\n","87":"@click.command()\n@click.argument('fastq', required=True)\n@click.option('--out_dir', default='.')\n@click.option('--cb_histogram', default=None)\n@click.option('--cb_cutoff', default=0)\ndef kallisto(fastq, out_dir, cb_histogram, cb_cutoff):\n    \"\"\" Convert fastqtransformed file to output format compatible with\n    kallisto.\n    \"\"\"\n    parser_re = re.compile(\n        '(.*):CELL_(?<CB>.*):UMI_(?P<UMI>.*)\\\\n(.*)\\\\n\\\\+\\\\n(.*)\\\\n')\n    if fastq.endswith('gz'):\n        fastq_fh = gzip.GzipFile(fileobj=open(fastq))\n    elif fastq == '-':\n        fastq_fh = sys.stdin\n    else:\n        fastq_fh = open(fastq)\n    cb_depth_set = get_cb_depth_set(cb_histogram, cb_cutoff)\n    cb_set = set()\n    cb_batch = collections.defaultdict(list)\n    parsed = 0\n    for read in stream_fastq(fastq_fh):\n        match = parser_re.search(read).groupdict()\n        umi = match['UMI']\n        cb = match['CB']\n        if cb_depth_set and cb not in cb_depth_set:\n            continue\n        parsed += 1\n        cb_set.add(cb)\n        cb_batch[cb].append((read, umi))\n        if not parsed % 10000000:\n            for cb, chunk in cb_batch.items():\n                write_kallisto_chunk(out_dir, cb, chunk)\n            cb_batch = collections.defaultdict(list)\n    for cb, chunk in cb_batch.items():\n        write_kallisto_chunk(out_dir, cb, chunk)\n    with open(os.path.join(out_dir, 'barcodes.batch'), 'w') as out_handle:\n        out_handle.write('#id umi-file file-1\\n')\n        batchformat = '{cb} {cb}.umi {cb}.fq\\n'\n        for cb in cb_set:\n            out_handle.write(batchformat.format(**locals()))\n","88":"def convert_model_entries(model, convert_id=create_convert_sbml_id_function\n    (), create_unique_id=None, translate_compartment=\n    translate_sbml_compartment, translate_reaction=translate_sbml_reaction,\n    translate_compound=translate_sbml_compound):\n    \"\"\"Convert and decode model entries.\n\n    Model entries are converted to new entries using the translate functions\n    and IDs are converted using the given coversion function. If ID conversion\n    would create a clash of IDs, the ``create_unique_id`` function is called\n    with a container of current IDs and the base ID to generate a unique ID\n    from. The translation functions take an existing entry and the new ID.\n\n    All references within the model are updated to use new IDs: compartment\n    boundaries, limits, exchange, model, biomass reaction, etc.\n\n    Args:\n        model: :class:`NativeModel`.\n    \"\"\"\n\n    def find_new_ids(entries):\n        \"\"\"Create new IDs for entries.\"\"\"\n        id_map = {}\n        new_ids = set()\n        for entry in entries:\n            new_id = convert_id(entry)\n            if new_id in new_ids:\n                if create_unique_id is not None:\n                    new_id = create_unique_id(new_ids, new_id)\n                else:\n                    raise ValueError(\n                        'Entity ID {!r} is not unique after conversion'.\n                        format(entry.id))\n            id_map[entry.id] = new_id\n            new_ids.add(new_id)\n        return id_map\n    compartment_map = find_new_ids(model.compartments)\n    compound_map = find_new_ids(model.compounds)\n    reaction_map = find_new_ids(model.reactions)\n    new_compartments = []\n    for compartment in model.compartments:\n        new_id = compartment_map[compartment.id]\n        new_compartments.append(translate_compartment(compartment, new_id))\n    new_compounds = []\n    for compound in model.compounds:\n        new_id = compound_map[compound.id]\n        new_compounds.append(translate_compound(compound, new_id,\n            compartment_map))\n    new_reactions = []\n    for reaction in model.reactions:\n        new_id = reaction_map[reaction.id]\n        new_entry = translate_reaction(reaction, new_id, compartment_map,\n            compound_map)\n        new_reactions.append(new_entry)\n    model.compartments.clear()\n    model.compartments.update(new_compartments)\n    model.compounds.clear()\n    model.compounds.update(new_compounds)\n    model.reactions.clear()\n    model.reactions.update(new_reactions)\n    new_boundaries = []\n    for boundary in model.compartment_boundaries:\n        c1, c2 = (compartment_map.get(c, c) for c in boundary)\n        new_boundaries.append(tuple(sorted(c1, c2)))\n    model.compartment_boundaries.clear()\n    model.compartment_boundaries.update(new_boundaries)\n    new_limits = []\n    for reaction, lower, upper in itervalues(model.limits):\n        new_reaction_id = reaction_map.get(reaction, reaction)\n        new_limits.append((new_reaction_id, lower, upper))\n    model.limits.clear()\n    model.limits.update((limit[0], limit) for limit in new_limits)\n    new_exchanges = []\n    for compound, reaction, lower, upper in itervalues(model.exchange):\n        new_compound_id = compound.translated(lambda name: compound_map.get\n            (name, name))\n        new_reaction_id = reaction_map.get(reaction, reaction)\n        new_exchanges.append((new_compound_id, new_reaction_id, lower, upper))\n    model.exchange.clear()\n    model.exchange.update((ex[0], ex) for ex in new_exchanges)\n    new_model = []\n    for reaction in model.model:\n        new_id = reaction_map.get(reaction, reaction)\n        new_model.append(new_id)\n    model.model.clear()\n    model.model.update((new_id, None) for new_id in new_model)\n    if model.biomass_reaction is not None:\n        old_id = model.biomass_reaction\n        model.biomass_reaction = reaction_map.get(old_id, old_id)\n    if model.extracellular_compartment is not None:\n        old_id = model.extracellular_compartment\n        model.extracellular_compartment = compartment_map.get(old_id, old_id)\n    if model.default_compartment is not None:\n        old_id = model.default_compartment\n        model.default_compartment = compartment_map.get(old_id, old_id)\n","89":"def getSTARVersion():\n    \"\"\"\n    Tries to find the STAR binary\n    and makes a system call to get its\n    version and return it\n    \"\"\"\n    try:\n        proc = subprocess.Popen(['STAR', '--version'], stdout=subprocess.\n            PIPE, stderr=subprocess.PIPE, shell=False, close_fds=True)\n        stdout, errmsg = proc.communicate()\n        version = stdout\n    except Exception:\n        version = 'Not available'\n    return version.rstrip()\n","90":"def merge_equivalent_compounds(model):\n    \"\"\"Merge equivalent compounds in various compartments.\n\n    Tries to detect and merge compound entries that represent the same\n    compound in different compartments. The entries are only merged if all\n    properties are equivalent. Compound entries must have an ID with a suffix\n    of an underscore followed by the compartment ID. This suffix will be\n    stripped and compounds with identical IDs are merged if the properties\n    are identical.\n\n    Args:\n        model: :class:`NativeModel`.\n    \"\"\"\n\n    def dicts_are_compatible(d1, d2):\n        return all(key not in d1 or key not in d2 or d1[key] == d2[key] for\n            key in set(d1) | set(d2))\n    compound_compartment = {}\n    inelegible = set()\n    for reaction in model.reactions:\n        equation = reaction.equation\n        if equation is None:\n            continue\n        for compound, _ in equation.compounds:\n            compartment = compound.compartment\n            if compartment is not None:\n                compound_compartment[compound.name] = compartment\n                if not compound.name.endswith('_{}'.format(compartment)):\n                    inelegible.add(compound.name)\n    compound_groups = {}\n    for compound_id, compartment in iteritems(compound_compartment):\n        if compound_id in inelegible:\n            continue\n        suffix = '_{}'.format(compound_compartment[compound_id])\n        if compound_id.endswith(suffix):\n            group_name = compound_id[:-len(suffix)]\n            compound_groups.setdefault(group_name, set()).add(compound_id)\n    compound_mapping = {}\n    merged_compounds = {}\n    for group, compound_set in iteritems(compound_groups):\n        merged = []\n        for compound_id in compound_set:\n            props = dict(model.compounds[compound_id].properties)\n            props.pop('id', None)\n            props.pop('compartment', None)\n            for merged_props, merged_set in merged:\n                if dicts_are_compatible(props, merged_props):\n                    merged_set.add(compound_id)\n                    merged_props.update(props)\n                    break\n                else:\n                    keys = set(key for key in set(props) | set(merged_props\n                        ) if key not in props or key not in merged_props or\n                        props[key] != merged_props[key])\n                    logger.info(\n                        'Unable to merge {} into {}, difference in keys: {}'\n                        .format(compound_id, ', '.join(merged_set), ', '.\n                        join(keys)))\n            else:\n                merged.append((props, {compound_id}))\n        if len(merged) == 1:\n            merged_props, merged_set = merged[0]\n            for compound_id in merged_set:\n                compound_mapping[compound_id] = group\n            merged_compounds[group] = merged_props\n        else:\n            for merged_props, merged_set in merged:\n                compartments = set(compound_compartment[c] for c in merged_set)\n                merged_name = '{}_{}'.format(group, '_'.join(sorted(\n                    compartments)))\n                for compound_id in merged_set:\n                    compound_mapping[compound_id] = merged_name\n                merged_compounds[merged_name] = merged_props\n    for reaction in model.reactions:\n        equation = reaction.equation\n        if equation is None:\n            continue\n        reaction.equation = equation.translated_compounds(lambda c:\n            compound_mapping.get(c, c))\n    new_compounds = []\n    for compound in model.compounds:\n        if compound.id not in compound_mapping:\n            new_compounds.append(compound)\n        else:\n            group = compound_mapping[compound.id]\n            if group not in merged_compounds:\n                continue\n            props = merged_compounds.pop(group)\n            props['id'] = group\n            new_compounds.append(DictCompoundEntry(props, filemark=compound\n                .filemark))\n    model.compounds.clear()\n    model.compounds.update(new_compounds)\n    new_exchange = OrderedDict()\n    for compound, reaction_id, lower, upper in itervalues(model.exchange):\n        new_compound = compound.translate(lambda name: compound_mapping.get\n            (name, name))\n        new_exchange[new_compound] = new_compound, reaction_id, lower, upper\n    model.exchange.clear()\n    model.exchange.update(new_exchange)\n","91":"def get_impute2_version(binary):\n    \"\"\"Gets the IMPUTE2 version from the binary.\n\n    Args:\n        binary (str): the name of the IMPUTE2 binary\n\n    Returns:\n        str: the version of the IMPUTE2 software\n\n    This function uses :py:class:`subprocess.Popen` to gather the version of\n    the IMPUTE2 binary. Since executing the software to gather the version\n    creates output files, they are deleted.\n\n    Warning\n    -------\n        This function only works as long as the version is returned as\n        ``IMPUTE version NNN`` (where ``NNN`` is the version), since we use\n        regular expression to extract the version number from the standard\n        output of the software.\n\n    \"\"\"\n    command = [binary]\n    proc = Popen(command, stdout=PIPE)\n    output = proc.communicate()[0].decode()\n    for filename in ['test.impute2_summary', 'test.impute2_warnings']:\n        if os.path.isfile(filename):\n            os.remove(filename)\n    version = re.search('IMPUTE version ([\\\\S]+)', output)\n    if version is None:\n        version = 'unknown'\n    else:\n        version = version.group(1)\n    logging.info('Will be using IMPUTE2 version {}'.format(version))\n    return version\n","92":"def fasta_iter(fasta_file):\n    \"\"\"\n    :param fasta_file: the file containing all input sequences in fasta format.\n    :type fasta_file: file object\n    :author: http:\/\/biostar.stackexchange.com\/users\/36\/brentp\n    :return: for a given fasta file, it returns an iterator which yields tuples\n             (string id, string comment, int sequence length)\n    :rtype: iterator\n    \"\"\"\n    faiter = (x[1] for x in groupby(fasta_file, lambda line: line[0] == '>'))\n    for header in faiter:\n        header = next(header)[1:].strip()\n        header = header.split()\n        _id = header[0]\n        comment = ' '.join(header[1:])\n        try:\n            seq = ''.join(s.strip() for s in next(faiter))\n        except StopIteration:\n            msg = (\n                f\"Error during sequence '{fasta_file.name}' parsing: Check the fasta format.\"\n                )\n            _log.critical(msg)\n            raise MacsypyError(msg)\n        length = len(seq)\n        yield _id, comment, length\n","93":"def get_files_for_table(input_dir, file_end=\n    '.trim.derep.no_artifacts.msa.deblur.no_chimeras'):\n    \"\"\"Get a list of files to add to the output table\n\n    Parameters:\n    -----------\n    input_dir : string\n        name of the directory containing the deblurred fasta files\n    file_end : string\n        the ending of all the fasta files to be added to the table\n        (default '.fasta.trim.derep.no_artifacts.msa.deblur.no_chimeras')\n\n    Returns\n    -------\n    names : list of tuples of (string,string)\n        list of tuples of:\n            name of fasta files to be added to the biom table\n            sampleid (file names without the file_end and path)\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    logger.debug('get_files_for_table input dir %s, file-ending %s' % (\n        input_dir, file_end))\n    names = []\n    for cfile in glob(join(input_dir, '*%s' % file_end)):\n        if not isfile(cfile):\n            continue\n        sample_id = basename(cfile)[:-len(file_end)]\n        sample_id = os.path.splitext(sample_id)[0]\n        names.append((cfile, sample_id))\n    logger.debug('found %d files' % len(names))\n    return names\n","94":"def model_generic_compounds(compound_entry_list):\n    \"\"\"\n    Function to sort the downloaded\tkegg object into a format\n    that is\tcompatible with the psamm api for storage in\n    a generic_compounds.yaml file. This function contains\n    special error handling for improperly formatted compounds\n    \"\"\"\n    non_gen_compounds = []\n    for compound in compound_entry_list:\n        try:\n            d = OrderedDict()\n            d['id'] = compound.id\n            non_gen_compounds.append(compound.id)\n            if hasattr(compound, 'name') and compound.name is not None:\n                d['name'] = compound.name\n            if hasattr(compound, 'names') and compound.names is not None:\n                names_l = []\n                for i in compound.names:\n                    names_l.append(i)\n                d['names'] = names_l\n            if hasattr(compound, 'formula') and compound.formula is not None:\n                d['formula'] = str(compound.formula)\n            if hasattr(compound, 'mol_weight'\n                ) and compound.mol_weight is not None:\n                d['mol_weight'] = compound.mol_weight\n            if hasattr(compound, 'comment') and compound.comment is not None:\n                d['comment'] = str(compound.comment)\n            if hasattr(compound, 'dblinks') and compound.dblinks is not None:\n                for key, value in compound.dblinks:\n                    if key != 'ChEBI':\n                        d['{}'.format(key)] = value\n            if hasattr(compound, 'chebi') and compound.chebi is not None:\n                d['ChEBI'] = compound.chebi\n            if hasattr(compound, 'chebi_all'\n                ) and compound.chebi_all is not None:\n                d['ChEBI_all'] = compound.chebi_all\n            if hasattr(compound, 'charge') and compound.charge is not None:\n                d['charge'] = compound.charge\n            yield d\n        except ParseError:\n            logger.warning(\n                '{} is improperly formatted  and will not be imported into compounds_generic.yaml'\n                .format(compound.id))\n            continue\n","95":"@require_column('ci_lo', 'ci_hi')\ndef ci(segarr):\n    \"\"\"Merge segments by confidence interval (overlapping 0).\n\n    Segments with lower CI above 0 are kept as gains, upper CI below 0 as\n    losses, and the rest with CI overlapping zero are collapsed as neutral.\n    \"\"\"\n    levels = np.zeros(len(segarr))\n    levels[segarr['ci_lo'].values > 0] = 1\n    levels[segarr['ci_hi'].values < 0] = -1\n    return squash_by_groups(segarr, pd.Series(levels))\n","96":"def fasta_extract_regions(fa_fname, intervals):\n    \"\"\"Extract an iterable of regions from an indexed FASTA file.\n\n    Input: FASTA file name; iterable of (seq_id, start, end) (1-based)\n    Output: iterable of string sequences.\n    \"\"\"\n    with pyfaidx.Fasta(fa_fname, as_raw=True) as fa_file:\n        for chrom, subarr in intervals.by_chromosome():\n            logging.info('Extracting sequences from chromosome %s', chrom)\n            for _chrom, start, end in subarr.coords():\n                yield fa_file[_chrom][int(start):int(end)]\n","97":"def flux_minimization(model, fixed, solver, weights={}):\n    \"\"\"Minimize flux of all reactions while keeping certain fluxes fixed.\n\n    The fixed reactions are given in a dictionary as reaction id\n    to value mapping. The weighted L1-norm of the fluxes is minimized.\n\n    Args:\n        model: MetabolicModel to solve.\n        fixed: dict of additional lower bounds on reaction fluxes.\n        solver: LP solver instance to use.\n        weights: dict of weights on the L1-norm terms.\n\n    Returns:\n        An iterator of reaction ID and reaction flux pairs.\n    \"\"\"\n    fba = FluxBalanceProblem(model, solver)\n    for reaction_id, value in iteritems(fixed):\n        flux = fba.get_flux_var(reaction_id)\n        fba.prob.add_linear_constraints(flux >= value)\n    fba.minimize_l1()\n    return ((reaction_id, fba.get_flux(reaction_id)) for reaction_id in\n        model.reactions)\n","98":"def guess_depth_cutoff(cb_histogram):\n    \"\"\" Guesses at an appropriate barcode cutoff\n    \"\"\"\n    with read_cbhistogram(cb_histogram) as fh:\n        cb_vals = [int(p.strip().split()[1]) for p in fh]\n    histo = np.histogram(np.log10(cb_vals), bins=50)\n    vals = histo[0]\n    edges = histo[1]\n    mids = np.array([((edges[i] + edges[i + 1]) \/ 2) for i in range(edges.\n        size - 1)])\n    wdensity = vals * 10 ** mids \/ sum(vals * 10 ** mids)\n    baseline = np.median(wdensity)\n    wdensity = list(wdensity)\n    peak = wdensity.index(max(wdensity[len(wdensity) \/ 2:]))\n    cutoff = None\n    for index, dens in reversed(list(enumerate(wdensity[1:peak]))):\n        if dens < 2 * baseline:\n            cutoff = index\n            break\n    if not cutoff:\n        return None\n    else:\n        cutoff = 10 ** mids[cutoff]\n        logger.info('Setting barcode cutoff to %d' % cutoff)\n        return cutoff\n","99":"def detect_best_flux_limit(model):\n    \"\"\"Detect the best default flux limit to use for model output.\n\n    The default flux limit does not change the model but selecting a good\n    value reduced the amount of output produced and reduces clutter in the\n    output files.\n    \"\"\"\n    flux_limit_count = Counter()\n    for reaction in model.reactions:\n        if reaction.id not in model.limits:\n            continue\n        equation = reaction.properties['equation']\n        if equation is None:\n            continue\n        _, lower, upper = model.limits[reaction.id]\n        if upper is not None and upper > 0 and equation.direction.forward:\n            flux_limit_count[upper] += 1\n        if lower is not None and -lower > 0 and equation.direction.reverse:\n            flux_limit_count[-lower] += 1\n    if len(flux_limit_count) == 0:\n        return None\n    best_flux_limit, _ = flux_limit_count.most_common(1)[0]\n    return best_flux_limit\n","100":"def evolve_population(population: list, generation: int) ->list:\n    \"\"\"\n    This evolves an existing population by doubling them (binary fission), then introducing random mutation to\n    each member of the population.\n    :param generation: Helps determine the starting point of the numbering system so the bacteria have unique names\n    :param population: A list of fasta files representing the bacteria.\n    :return: None\n    \"\"\"\n    children_population = population + population\n    names = []\n    new_population = []\n    for j in range(len(children_population)):\n        names.append('bacterium_{}_{}'.format(generation, j + 1))\n    for i in range(len(children_population)):\n        child = Bacterium(children_population[i].get_file(), names[i],\n            children_population[i].get_chroms())\n        new_population.append(child)\n    return new_population\n","101":"def get_best_hit_4_func(function, hits, key='score'):\n    \"\"\"\n    select the best Loner among several ones encoding for same function\n\n        * score\n        * i_evalue\n        * profile_coverage\n\n    :param str function: the name of the function fulfill by the hits (all hits must have same function)\n    :param hits: the hits to filter.\n    :type hits: sequence of :class:`macsypy.hit.ModelHit` object\n    :param str key: The criterion used to select the best hit 'score', i_evalue', 'profile_coverage'\n    :return: the best hit\n    :rtype: :class:`macsypy.hit.ModelHit` object\n    \"\"\"\n    originals = []\n    exchangeables = []\n    for hit in hits:\n        if hit.gene_ref.name == function:\n            originals.append(hit)\n        else:\n            exchangeables.append(hit)\n    if originals:\n        hits = originals\n    else:\n        hits = exchangeables\n    if key == 'score':\n        hits.sort(key=attrgetter(key), reverse=True)\n    elif key == 'i_eval':\n        hits.sort(key=attrgetter(key))\n    elif key == 'profile_coverage':\n        hits.sort(key=attrgetter(key), reverse=True)\n    else:\n        raise MacsypyError(\n            f\"\"\"The criterion for Loners comparison {key} does not exist or is not available.\n\"\"\"\n            )\n    return hits[0]\n","102":"def fit_cox(data, time_to_event, event, formula, result_col, **kwargs):\n    \"\"\"Fit a Cox' proportional hazard to the data.\n\n    Args:\n        data (pandas.DataFrame): the data to analyse\n        time_to_event (str): the time to event column for the survival analysis\n        event (str): the event column for the survival analysis\n        formula (str): the formula for the data preparation\n        result_col (str): the column that will contain the results\n\n    Returns:\n        numpy.array: the results from the survival analysis\n\n    Note\n    ----\n        Using alpha of 0.95, and default parameters.\n\n    \"\"\"\n    y, X = dmatrices(formula, data=data, return_type='dataframe')\n    data = pd.merge(y, X.drop('Intercept', axis=1), left_index=True,\n        right_index=True)\n    cf = CoxPHFitter()\n    cf.fit(data, duration_col=time_to_event, event_col=event)\n    return cf.summary.loc[result_col, _COX_REQ_COLS].values\n","103":"def _format_time_columns(table, first_col):\n    \"\"\"Colorize the time in the table (columns 2 and up).\n\n    Args:\n        table (list): the data for the tabular\n        first_col (int): the first column containing time\n\n    Returns:\n        list: the same data, but with time column colorized\n\n    \"\"\"\n    for i in range(len(table)):\n        for j in range(first_col, len(table[i])):\n            table[i][j] = utils.colorize_time(table[i][j])\n    return table\n","104":"@jit_or_passthrough\ndef jax_mbar_hessian(u_kn, N_k, f_k):\n    \"\"\"JAX version of mbar_hessian.\n    For parameters, see mbar_hessian\n    N_k must be float (should be cast at a higher level)\n\n    \"\"\"\n    log_denominator_n = logsumexp(f_k - u_kn.T, b=N_k, axis=1)\n    logW = f_k - u_kn.T - log_denominator_n[:, newaxis]\n    W = exp(logW)\n    H = dot(W.T, W)\n    H *= N_k\n    H *= N_k[:, newaxis]\n    H -= diag(W.sum(0) * N_k)\n    return -1.0 * H\n","105":"def assign_ci_start_end(segarr, cnarr):\n    \"\"\"Assign ci_start and ci_end fields to segments.\n\n    Values for each segment indicate the CI boundary points within that segment,\n    i.e. the right CI boundary for the left-side breakpoint (segment start), and\n    left CI boundary for the right-side breakpoint (segment end).\n\n    This is a little unintuitive because the CI refers to the breakpoint, not\n    the segment, but we're storing the info in an array of segments.\n\n    Calculation: Just use the boundaries of the bins left- and right-adjacent to\n    each segment breakpoint.\n    \"\"\"\n    lefts_rights = ((bins.end.iat[0], bins.start.iat[-1]) for _seg, bins in\n        cnarr.by_ranges(segarr, mode='outer'))\n    ci_lefts, ci_rights = zip(*lefts_rights)\n    return segarr.as_dataframe(segarr.data.assign(ci_left=ci_lefts,\n        ci_right=ci_rights))\n","106":"def safe_log2(values, min_log2):\n    \"\"\"Transform values to log2 scale, safely handling zeros.\n\n    Parameters\n    ----------\n    values : np.array\n        Absolute-scale values to transform. Should be non-negative.\n    min_log2 : float\n        Assign input zeros this log2-scaled value instead of -inf. Rather than\n        hard-clipping, input values near 0 (especially below 2^min_log2) will be\n        squeezed a bit above `min_log2` in the log2-scale output.\n    \"\"\"\n    absolute_shift = 2 ** min_log2\n    return np.log2(values + absolute_shift)\n","107":"def flip_markers(required_chrom, prefix, to_flip, db_name, options):\n    \"\"\"Flip markers.\n\n    Args:\n        required_chrom (tuple): the list of chromosomes to flip\n        prefix (str): the prefix template of the input files\n        to_flip (str): the name of the file containing markers to flip\n        db_name (str): the name of the DB saving tasks' information\n        options (argparse.Namespace): the pipeline options\n\n    A template contains the string ``{chrom}``, which will be replaced by the\n    chromosome number (e.g. ``genipe\/chr{chrom}\/chr{chrom}.final`` will be\n    replaced by ``genipe\/chr1\/chr1.final``).\n\n    \"\"\"\n    commands_info = []\n    base_command = ['plink' if options.plink_bin is None else options.\n        plink_bin, '--noweb', '--make-bed']\n    o_prefix = os.path.join(options.out_dir, 'chr{chrom}', 'chr{chrom}.flipped'\n        )\n    for chrom in required_chrom:\n        c_prefix = o_prefix.format(chrom=chrom)\n        remaining_command = ['--bfile', prefix.format(chrom=chrom),\n            '--flip', to_flip.format(chrom=chrom), '--out', c_prefix]\n        commands_info.append({'task_id': 'plink_flip_chr{}'.format(chrom),\n            'name': 'plink flip chr{}'.format(chrom), 'command': \n            base_command + remaining_command, 'task_db': db_name, 'o_files':\n            [(c_prefix + ext) for ext in ('.bed', '.bim', '.fam')]})\n    logging.info('Flipping markers')\n    launcher.launch_tasks(commands_info, options.thread, hpc=options.\n        use_drmaa, hpc_options=options.task_options, out_dir=options.\n        out_dir, preamble=options.preamble)\n    logging.info('Done flipping markers')\n","108":"def getTaggdCountVersion():\n    \"\"\"\n    Tries to find the Taggd binary\n    and makes a system call to get its\n    version and return it\n    \"\"\"\n    version = ''\n    try:\n        proc = subprocess.Popen(['pip', 'show', 'taggd'], stdout=subprocess\n            .PIPE, stderr=subprocess.PIPE, shell=False, close_fds=True)\n        stdout, errmsg = proc.communicate()\n        for line in stdout.split('\\n'):\n            if line.find('Version:') != -1:\n                version = str(line.split()[-1])\n    except Exception:\n        version = 'Not available'\n    return version.rstrip()\n","109":"def remove_chimeras_denovo_from_seqs(seqs_fp, working_dir, threads=1):\n    \"\"\"Remove chimeras de novo using UCHIME (VSEARCH implementation).\n\n    Parameters\n    ----------\n    seqs_fp: string\n        file path to FASTA input sequence file\n    output_fp: string\n        file path to store chimera-free results\n    threads : int\n        number of threads (0 for all cores)\n\n    Returns\n    -------\n    output_fp\n        the chimera removed fasta file name\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    logger.info(\n        'remove_chimeras_denovo_from_seqs seqs file %sto working dir %s' %\n        (seqs_fp, working_dir))\n    output_fp = join(working_dir, '%s.no_chimeras' % basename(seqs_fp))\n    params = ['vsearch', '--uchime_denovo', seqs_fp, '--nonchimeras',\n        output_fp, '-dn', '0.000001', '-xn', '1000', '-minh', '10000000',\n        '--mindiffs', '5', '--fasta_width', '0', '--threads', str(threads)]\n    sout, serr, res = _system_call(params)\n    if not res == 0:\n        logger.error('problem with chimera removal for file %s' % seqs_fp)\n        logger.debug('stdout : %s' % sout)\n        logger.debug('stderr : %s' % serr)\n    return output_fp\n","110":"def getHTSeqCountVersion():\n    \"\"\"\n    Tries to find the HTSeqCount binary\n    and makes a system call to get its\n    version and return it\n    \"\"\"\n    version = ''\n    try:\n        proc = subprocess.Popen(['pip', 'show', 'htseq'], stdout=subprocess\n            .PIPE, stderr=subprocess.PIPE, shell=False, close_fds=True)\n        stdout, errmsg = proc.communicate()\n        for line in stdout.split('\\n'):\n            if line.find('Version:') != -1:\n                version = str(line.split()[-1])\n    except Exception:\n        version = 'Not available'\n    return version.rstrip()\n","111":"def flux_randomization(model, threshold, tfba, solver):\n    \"\"\"Find a random flux solution on the boundary of the solution space.\n\n    The reactions in the threshold dictionary are constrained with the\n    associated lower bound.\n\n    Args:\n        model: MetabolicModel to solve.\n        threshold: dict of additional lower bounds on reaction fluxes.\n        tfba: If True enable thermodynamic constraints.\n        solver: LP solver instance to use.\n\n    Returns:\n        An iterator of reaction ID and reaction flux pairs.\n    \"\"\"\n    optimize = {}\n    for reaction_id in model.reactions:\n        if model.is_reversible(reaction_id):\n            optimize[reaction_id] = 2 * random.random() - 1.0\n        else:\n            optimize[reaction_id] = random.random()\n    fba = _get_fba_problem(model, tfba, solver)\n    for reaction_id, value in iteritems(threshold):\n        fba.prob.add_linear_constraints(fba.get_flux_var(reaction_id) >= value)\n    fba.maximize(optimize)\n    for reaction_id in model.reactions:\n        yield reaction_id, fba.get_flux(reaction_id)\n","112":"def threads_available():\n    \"\"\"\n\n    :return: The maximal number of threads available.\n             It's nice with cluster scheduler or linux.\n             On Mac it use the number of physical cores\n    :rtype: int\n    \"\"\"\n    if hasattr(os, 'sched_getaffinity'):\n        threads_nb = len(os.sched_getaffinity(0))\n    else:\n        threads_nb = os.cpu_count()\n    return threads_nb\n","113":"def _fit_edge(x, y, window_start, window_stop, interp_start, interp_stop,\n    polyorder):\n    \"\"\"\n    Given a 1-D array `x` and the specification of a slice of `x` from\n    `window_start` to `window_stop`, create an interpolating polynomial of the\n    sliced sub-array, and evaluate that polynomial from `interp_start` to\n    `interp_stop`.  Put the result into the corresponding slice of `y`.\n    \"\"\"\n    x_edge = x[window_start:window_stop]\n    poly_coeffs = np.polyfit(np.arange(0, window_stop - window_start),\n        x_edge, polyorder)\n    i = np.arange(interp_start - window_start, interp_stop - window_start)\n    values = np.polyval(poly_coeffs, i)\n    y[interp_start:interp_stop] = values\n","114":"def gen_build_models_metadata(target, target_setup_data,\n    process_only_these_targets, process_only_these_templates,\n    model_seqid_cutoff, write_modeller_restraints_file):\n    \"\"\"\n    Generate build_models metadata for a given target.\n    :param target: BioPython SeqRecord\n    :param target_setup_data:\n    :return: metadata: dict\n    \"\"\"\n    datestamp = ensembler.core.get_utcnow_formatted()\n    nsuccessful_models = subprocess.check_output(['find', target_setup_data\n        .models_target_dir, '-name', 'model.pdb.gz']).count('\\n')\n    target_timedelta = datetime.datetime.utcnow(\n        ) - target_setup_data.target_starttime\n    modeller_version = get_modeller_version()\n    metadata = {'target_id': target.id, 'write_modeller_restraints_file':\n        write_modeller_restraints_file, 'model_seqid_cutoff':\n        model_seqid_cutoff, 'datestamp': datestamp, 'timing': ensembler.\n        core.strf_timedelta(target_timedelta), 'nsuccessful_models':\n        nsuccessful_models, 'process_only_these_targets':\n        process_only_these_targets, 'process_only_these_templates':\n        process_only_these_templates, 'python_version': sys.version.split(\n        '|')[0].strip(), 'python_full_version': ensembler.core.literal_str(\n        sys.version), 'ensembler_version': ensembler.version.short_version,\n        'ensembler_commit': ensembler.version.git_revision,\n        'modeller_version': modeller_version if modeller_version is not\n        None else '', 'biopython_version': Bio.__version__}\n    return metadata\n","115":"def kill_proc_tree(pid, sig=signal.SIGTERM, include_parent=True, timeout=\n    None, on_kill=None):\n    \"\"\"Kill a process tree (including grandchildren etc.) with signal \"sig\".\n\n    Return a (killed, alive) tuple. \"on_terminate\", if specified, is a callback\n    function which is called as soon as a child terminates.\n    \"\"\"\n    try:\n        parent = psutil.Process(pid)\n        processes = parent.children(recursive=True)\n    except psutil.NoSuchProcess:\n        return 0, 0\n    if include_parent:\n        processes.append(parent)\n    for proc in processes:\n        try:\n            proc.send_signal(sig)\n        except psutil.NoSuchProcess:\n            pass\n    try:\n        killed, alive = psutil.wait_procs(processes, timeout=timeout,\n            callback=on_kill)\n    except psutil.NoSuchProcess:\n        return 0, 0\n    return len(killed), len(alive)\n","116":"def parse_compound(s, global_compartment=None):\n    \"\"\"Parse a compound specification.\n\n    If no compartment is specified in the string, the global compartment\n    will be used.\n    \"\"\"\n    m = re.match('^\\\\|(.*)\\\\|$', s)\n    if m:\n        s = m.group(1)\n    m = re.match('^(.+)\\\\[(\\\\S+)\\\\]$', s)\n    if m:\n        compound_id = convert_to_unicode(m.group(1))\n        compartment = m.group(2)\n    else:\n        compound_id = convert_to_unicode(s)\n        compartment = global_compartment\n    return Compound(compound_id, compartment=compartment)\n","117":"def correct_cnr(cnr, do_gc, do_txlen, max_log2):\n    \"\"\"Apply bias corrections & smoothing.\n\n    - Biases: 'gc', 'length'\n    - Smoothing: rolling triangle window using weights.\n    \"\"\"\n    cnr.center_all()\n    if any((do_gc, do_txlen)):\n        if do_gc and 'gc' in cnr:\n            cnr = center_by_window(cnr, 0.1, cnr['gc'])\n        if do_txlen and 'tx_length' in cnr:\n            cnr = center_by_window(cnr, 0.1, cnr['tx_length'])\n        cnr.center_all()\n    if max_log2:\n        cnr[cnr['log2'] > max_log2, 'log2'] = max_log2\n    return cnr\n","118":"def reorder_chromosome_23(chrom, to_skip, prefix, base_command):\n    \"\"\"Reorders chromosome 23 markers.\n\n    Args:\n        chrom (int): the chromosome to reorder\n        to_skip (set): the set of regions to skip (if necessary)\n        prefix (str): the prefix of the output files\n        base_command (list): the base command\n\n    Returns:\n        list: a list of command information to run for chromosome 23\n\n    \"\"\"\n    command_info = []\n    if chrom == 23:\n        remaining_command = ['--bfile', prefix + '_not_ordered', '--out',\n            prefix]\n        command_info.append({'task_id': 'plink_reorder_chr{}'.format(chrom),\n            'name': 'plink reorder chr{}'.format(chrom), 'command': \n            base_command + remaining_command, 'o_files': [(prefix + ext) for\n            ext in ('.bed', '.bim', '.fam')]})\n    elif chrom == 25:\n        new_prefix = os.path.join(os.path.dirname(prefix) + '{suffix}', os.\n            path.basename(prefix) + '{suffix}')\n        for suffix in ('_1', '_2'):\n            if '{}{}'.format(chrom, suffix) in to_skip:\n                continue\n            remaining_command = ['--bfile', new_prefix.format(suffix=suffix\n                ) + '_not_ordered', '--out', new_prefix.format(suffix=suffix)]\n            command_info.append({'task_id': 'plink_reorder_chr{}{}'.format(\n                chrom, suffix), 'name': 'plink reorder chr{}{}'.format(\n                chrom, suffix), 'command': base_command + remaining_command,\n                'o_files': [(new_prefix.format(suffix=suffix) + ext) for\n                ext in ('.bed', '.bim', '.fam')]})\n    else:\n        raise GenipeError('{}: not a valid chromosome 23 region'.format(chrom))\n    return command_info\n","119":"def getwriter(matrix_outfile):\n    \"\"\"\n    Factory function to get the correct writer depending on the file ending\n\n    Args:\n        matrix_outfile(str): Filename of output - used to determine output format. Valid formats are .xlsx .xls .csv or .tsv\n    \"\"\"\n    if matrix_outfile.endswith('xls'):\n        matrix_writer = XlsWriter(matrix_outfile)\n    elif matrix_outfile.endswith('xlsx'):\n        matrix_writer = XlsxWriter(matrix_outfile)\n    elif matrix_outfile.endswith('tsv'):\n        matrix_writer = CsvWriter(matrix_outfile, delim='\\t')\n    elif matrix_outfile.endswith('csv'):\n        matrix_writer = CsvWriter(matrix_outfile, delim=',')\n    else:\n        raise Exception(\n            'Unknown matrix extension, must be .xlsx .xls .csv or .tsv')\n    return matrix_writer\n","120":"def add_sam2rnf_parser(subparsers, subcommand, help, description,\n    simulator_name=None):\n    \"\"\"Add another parser for a SAM2RNF-like command.\n\n\tArgs:\n\t\tsubparsers (subparsers): File name of the genome from which read tuples are created (FASTA file).\n\t\tsimulator_name (str): Name of the simulator used in comments.\n\t\"\"\"\n    parser_sam2rnf = subparsers.add_parser(subcommand, help=help,\n        description=description)\n    parser_sam2rnf.set_defaults(func=sam2rnf)\n    parser_sam2rnf.add_argument('-s', '--sam', type=str, metavar='file',\n        dest='sam_fn', required=True, help=\n        'Input SAM\/BAM with true (expected) alignments of the reads  (- for standard input).'\n        )\n    _add_shared_params(parser_sam2rnf, unmapped_switcher=True)\n    parser_sam2rnf.add_argument('-n', '--simulator-name', type=str, metavar\n        ='str', dest='simulator_name', default=simulator_name, help=\n        'Name of the simulator (for RNF).' if simulator_name is not None else\n        argparse.SUPPRESS)\n","121":"def parse_dgf(mm, dgf_file):\n    \"\"\"A function that will parse a supplied deltaG of formation file.\n\n    Compound IDs in this file do not need to contain the compartments.\n    compound deltaGf values should be in Kcal\/mol\n\n    Args:\n        mm: a metabolic model object\n        dgf_file: a file that containing 2 columns of\n            compound ids and deltaGf values\n    \"\"\"\n    cpd_dgf_dict = {}\n    for row in csv.reader(dgf_file, delimiter=str('\\t')):\n        for cpt in mm.compartments:\n            try:\n                dg = Decimal(row[1])\n                derr = Decimal(row[2])\n                cpd_dgf_dict[Compound(row[0], cpt)] = dg, derr\n            except:\n                logger.info(\n                    u'Compound {} has an assigned detltGf value of {}. This is not an number and will be treated as a missing value.'\n                    .format(Compound(row[0], cpt), row[1]))\n    return cpd_dgf_dict\n","122":"def v_factors(xcol, ymatr):\n    \"\"\"xcol is (Nobservations,1) column vector of grouping values\n           (in terms of dose curve it may be Dose).\n       ymatr is (Nobservations, Nmeasures) matrix, where rows correspond to\n           observations and columns corresponds to different measures.\n\n        Calculate the V factor = 1-6 * mean standard deviation \/ range\n    \"\"\"\n    xs, avers, stds = loc_shrink_mean_std(xcol, ymatr)\n    vrange = numpy.max(avers, 0) - numpy.min(avers, 0)\n    vstd = numpy.zeros(len(vrange))\n    vstd[vrange == 0] = 1\n    vstd[vrange != 0] = numpy.mean(stds[:, vrange != 0], 0)\n    vrange[vrange == 0] = 1e-06\n    v = 1 - 6 * (vstd \/ vrange)\n    return v\n","123":"def fileOk(_file):\n    \"\"\"\n    Checks file exists and is not zero size\n    :param file: a file name\n    :return: True if the file is correct\n    \"\"\"\n    return _file is not None and os.path.isfile(_file) and not os.path.getsize(\n        _file) == 0\n","124":"def parse_bed_track(line):\n    \"\"\"Parse the \"name\" field of a BED track definition line.\n\n    Example:\n    track name=146793_BastianLabv2_P2_target_region description=\"146793_BastianLabv2_P2_target_region\"\n    \"\"\"\n    fields = shlex.split(line)\n    assert fields[0] == 'track'\n    for field in fields[1:]:\n        if '=' in field:\n            key, val = field.split('=', 1)\n            if key == 'name':\n                return val\n    raise ValueError('No name defined for this track')\n","125":"def rescale_baf(purity, observed_baf, normal_baf=0.5):\n    \"\"\"Adjust B-allele frequencies for sample purity.\n\n    Math::\n\n        t_baf*purity + n_baf*(1-purity) = obs_baf\n        obs_baf - n_baf * (1-purity) = t_baf * purity\n        t_baf = (obs_baf - n_baf * (1-purity))\/purity\n    \"\"\"\n    tumor_baf = (observed_baf - normal_baf * (1 - purity)) \/ purity\n    return tumor_baf\n","126":"def parse_arch_path(path: str) ->Tuple[str, str]:\n    \"\"\"\n\n    :param str path: the path to the archive\n    :return: the name of the package and it's version\n    :rtype: tuple\n    :raise ValueError: if the extension of the package is neither '.tar.gz' nor '.tgz'\n                       or if the package does not seem to include version 'pack_name-<vers>.ext'\n    \"\"\"\n    pack_vers_name = os.path.basename(path)\n    if pack_vers_name.endswith('.tar.gz'):\n        pack_vers_name = pack_vers_name[:-7]\n    elif pack_vers_name.endswith('.tgz'):\n        pack_vers_name = pack_vers_name[:-4]\n    else:\n        raise ValueError(f'{path} does not seem to be a package (a tarball).')\n    *pack_name, vers = pack_vers_name.split('-')\n    if not pack_name:\n        raise ValueError(f'{path} does not seem to not be versioned.')\n    pack_name = '-'.join(pack_name)\n    return pack_name, vers\n","127":"def create_metadata_table(cxn, args):\n    \"\"\"\n    Create the metadata table.\n\n    Information used to tell how aTRAM was set up.\n    \"\"\"\n    cxn.executescript(\n        \"\"\"\n        DROP TABLE IF EXISTS metadata;\n\n        CREATE TABLE metadata (\n            label TEXT,\n            value TEXT);\n        \"\"\"\n        )\n    with cxn:\n        sql = 'INSERT INTO metadata (label, value) VALUES (?, ?);'\n        cxn.execute(sql, ('version', DB_VERSION))\n        cxn.execute(sql, ('single_ends', bool(args.get('single_ends'))))\n","128":"def theta_read_counts(log2_ratio, nbins, avg_depth=500, avg_bin_width=200,\n    read_len=100):\n    \"\"\"Calculate segments' read counts from log2-ratios.\n\n    Math:\n        nbases = read_length * read_count\n    and\n        nbases = bin_width * read_depth\n    where\n        read_depth = read_depth_ratio * avg_depth\n\n    So:\n        read_length * read_count = bin_width * read_depth\n        read_count = bin_width * read_depth \/ read_length\n    \"\"\"\n    read_depth = 2 ** log2_ratio * avg_depth\n    read_count = nbins * avg_bin_width * read_depth \/ read_len\n    return read_count.round().fillna(0).astype('int')\n","129":"def translate_region_to_bins(region, bins):\n    \"\"\"Map genomic coordinates to bin indices.\n\n    Return a tuple of (chrom, start, end), just like unpack_range.\n    \"\"\"\n    if region is None:\n        return Region(None, None, None)\n    chrom, start, end = unpack_range(region)\n    if start is None and end is None:\n        return Region(chrom, start, end)\n    if start is None:\n        start = 0\n    if end is None:\n        end = float('inf')\n    c_bin_starts = bins.data.loc[bins.data.chromosome == chrom, 'start'].values\n    r_start, r_end = np.searchsorted(c_bin_starts, [start, end])\n    return Region(chrom, r_start, r_end)\n","130":"def count_mapped_read_sam(samout):\n    \"\"\"Return the number of mapped reads to the genome.\n\n    \"\"\"\n    if not os.path.exists(samout):\n        raise Exception(\"can't open SAM\")\n    mapped = set()\n    for x in fileinput.input(samout):\n        if not x or x.startswith('@'):\n            continue\n        x = x.rstrip().split('\\t')\n        if x[2] != '*':\n            mapped.add(x[0])\n    cnt = sum([int(n.split('_')[1]) for n in mapped])\n    return cnt\n","131":"def check_positive_int(raw, default, expected, sequence=False):\n    \"\"\"\n    Check if value can be cast in integer >=0\n\n    :param str raw: the value return by the user\n    :param int default: the default value for the option\n    :param expected: not used here to have the same signature for all check_xxx functions\n    :return: value\n    :raise MacsypyError: if the value cannot be cast in right type\n    \"\"\"\n\n    def positive_int(value):\n        casted = int(str(value))\n        if casted < 0:\n            raise ValueError(f\"'{value}' is not >=0\")\n        return casted\n    return _validator(positive_int, raw, default, sequence=sequence)\n","132":"def __non_zero_std__(inArray):\n    \"\"\"Return the standard deviation of non-zero values of an array.\n\n    Keyword Arguments:\n        inArray -- intensities of one frame\n    \"\"\"\n    return inArray[numpy.nonzero(inArray)[0]].std()\n","133":"def looking_at_escape(s, state):\n    \"\"\"Return # of characters in an escape\n\n    s - string to look at\n    state - the current search state\n\n    returns either None or the # of characters in the escape\n    \"\"\"\n    if s[0] != '\\\\':\n        return\n    if len(s) < 2:\n        raise ValueError('Unterminated escape sequence')\n    if s[:2] in HARDCODE_ESCAPES:\n        return 2\n    if state.in_brackets:\n        if s[1] in OCTAL_DIGITS:\n            for i in range(2, min(4, len(s))):\n                if s[i] != OCTAL_DIGITS:\n                    return i\n        if s[1] in DECIMAL_DIGITS:\n            raise ValueError(\n                'Numeric escapes within brackets must be octal values: e.g., [\\\\21] for ^Q'\n                )\n    elif s[1] == 0:\n        for i in range(2, min(4, len(s))):\n            if s[i] != OCTAL_DIGITS:\n                return i\n    elif s[1] in DECIMAL_DIGITS:\n        if len(s) > 2 and s[2] in DECIMAL_DIGITS:\n            group_number = int(s[1:3])\n            length = 2\n        else:\n            group_number = int(s[1])\n            length = 1\n        if group_number > state.group_count:\n            raise ValueError('Only %d groups at this point' % state.group_count\n                )\n        return length\n    if s[1] == 'x':\n        if s[2] in HEXIDECIMAL_DIGITS and s[3] in HEXIDECIMAL_DIGITS:\n            return 4\n        raise ValueError('Hexidecimal escapes are two digits long: eg. \\\\x1F')\n    return 2\n","134":"def is_reversed(chrom, pos, a1, a2, reference, encoding):\n    \"\"\"Checks the strand using a reference, returns False if problem.\n\n    Args:\n        chrom (str): the chromosome\n        pos (int): the position\n        a1 (str): the first allele\n        a2 (str): the second allele\n        reference (pyfaidx.Fasta): the reference\n        encoding (dict): the chromosome encoding in the reference\n\n    Returns:\n        bool: ``True`` if it's the complement, ``False`` otherwise (also\n              returns ``False`` if there was a problem)\n\n    The encoding (used to encode chromosome to search in the reference) is the\n    dictionary returned by the :py:func:`get_chrom_encoding` function.\n\n    \"\"\"\n    a1 = a1.upper()\n    a2 = a2.upper()\n    if a1 not in _complement or a2 not in _complement:\n        return False\n    if chrom not in encoding:\n        return False\n    ref = reference[encoding[chrom]][pos - 1]\n    if ref is None:\n        return False\n    ref = ref.upper()\n    if ref not in _complement:\n        return False\n    if a1 == ref or a2 == ref:\n        return False\n    if _complement[a1] == ref or _complement[a2] == ref:\n        return True\n    raise GenipeError('chr{}: {}: {}: {}\/{}: invalid'.format(chrom, pos,\n        ref, a1, a2))\n","135":"def shrink_defined_pixels(labels, fill, iterations):\n    \"\"\"\n    Remove pixels around the perimeter of an object unless\n    doing so would change the object\u2019s Euler number `iterations` times. \n    Processing stops automatically when there are no more pixels to\n    remove.\n    \"\"\"\n    if fill:\n        labels = centrosome.cpmorphology.fill_labeled_holes(labels)\n    return centrosome.cpmorphology.binary_shrink(labels, iterations=iterations)\n","136":"def group_bed_tracks(bedfile):\n    \"\"\"Group the parsed rows in a BED file by track.\n\n    Yields (track_name, iterable_of_lines), much like itertools.groupby.\n    \"\"\"\n    with as_handle(bedfile, 'r') as handle:\n        curr_track = 'DEFAULT'\n        curr_lines = []\n        for line in handle:\n            if line.startswith('track'):\n                if curr_lines:\n                    yield curr_track, curr_lines\n                    curr_lines = []\n                curr_track = parse_bed_track(line)\n            else:\n                curr_lines.append(line)\n        yield curr_track, curr_lines\n","137":"def __non_zero_mean__(inArray):\n    \"\"\"Return the mean of non-zero values of an array.\n\n    Keyword Arguments:\n        inArray -- intensities of one frame\n    \"\"\"\n    return inArray[numpy.nonzero(inArray)[0]].mean()\n","138":"def get_exchange_reactions(model):\n    \"\"\"Yield IDs of all exchange reactions from model.\n\n    This helper function would be useful when creating\n    :class:`.ReactionDeletionStrategy` objects.\n\n    Args:\n        model: :class:`psamm.metabolicmodel.MetabolicModel`.\n    \"\"\"\n    for reaction_id in model.reactions:\n        if model.is_exchange(reaction_id):\n            yield reaction_id\n","139":"def _log2_ratio_to_absolute_pure(log2_ratio, ref_copies):\n    \"\"\"Transform a log2 ratio to absolute linear scale (for a pure sample).\n\n    Purity adjustment is skipped. This is appropriate if the sample is germline\n    or if scaling for tumor heterogeneity was done beforehand.\n\n    .. math :: n = r*2^v\n    \"\"\"\n    ncopies = ref_copies * 2 ** log2_ratio\n    return ncopies\n","140":"def get_gene_associations(model):\n    \"\"\"Create gene association for class :class:`.GeneDeletionStrategy`.\n\n    Return a dict mapping reaction IDs to\n    :class:`psamm.expression.boolean.Expression` objects,\n    representing relationships between reactions and related genes. This helper\n    function should be called when creating :class:`.GeneDeletionStrategy`\n    objects.\n\n    Args:\n        model: :class:`psamm.datasource.native.NativeModel`.\n    \"\"\"\n    for reaction in model.reactions:\n        assoc = None\n        if reaction.genes is None:\n            continue\n        elif isinstance(reaction.genes, string_types):\n            assoc = boolean.Expression(reaction.genes)\n        else:\n            variables = [boolean.Variable(g) for g in reaction.genes]\n            assoc = boolean.Expression(boolean.And(*variables))\n        yield reaction.id, assoc\n","141":"def do_reference_flat(targets, antitargets=None, fa_fname=None,\n    male_reference=False):\n    \"\"\"Compile a neutral-coverage reference from the given intervals.\n\n    Combines the intervals, shifts chrX values if requested, and calculates GC\n    and RepeatMasker content from the genome FASTA sequence.\n    \"\"\"\n    ref_probes = bed2probes(targets)\n    if antitargets:\n        ref_probes.add(bed2probes(antitargets))\n    ref_probes['log2'] = ref_probes.expect_flat_log2(male_reference)\n    ref_probes['depth'] = np.exp2(ref_probes['log2'])\n    if fa_fname:\n        gc, rmask = get_fasta_stats(ref_probes, fa_fname)\n        ref_probes['gc'] = gc\n        ref_probes['rmask'] = rmask\n    else:\n        logging.info(\n            'No FASTA reference genome provided; skipping GC, RM calculations')\n    ref_probes.sort_columns()\n    return ref_probes\n","142":"@on_array()\ndef modal_location(a):\n    \"\"\"Return the modal value of an array's values.\n\n    The \"mode\" is the location of peak density among the values, estimated using\n    a Gaussian kernel density estimator.\n\n    Parameters\n    ----------\n    a : np.array\n        A 1-D array of floating-point values, e.g. bin log2 ratio values.\n    \"\"\"\n    sarr = np.sort(a)\n    kde = stats.gaussian_kde(sarr)\n    y = kde.evaluate(sarr)\n    peak = sarr[y.argmax()]\n    return peak\n","143":"def do_help(args: argparse.Namespace) ->None:\n    \"\"\"\n    Display on stdout the content of readme file\n    if the readme file does nopt exists display a message to the user see :meth:`macsypy.package.help`\n\n    :param args: the arguments passed on the command line (the package name)\n    :type args: :class:`argparse.Namespace` object\n    :return: None\n    :raise ValueError: if the package name is not known.\n    \"\"\"\n    pack_name = args.package\n    inst_pack_loc = _find_installed_package(pack_name, models_dir=args.\n        models_dir)\n    if inst_pack_loc:\n        pack = Package(inst_pack_loc.path)\n        print(pack.help())\n    else:\n        _log.error(f\"Models '{pack_name}' not found locally.\")\n        sys.tracebacklimit = 0\n        raise ValueError()\n","144":"def get_best_hits(hits, key='score'):\n    \"\"\"\n    If several hits match the same protein, keep only the best match based either on\n\n        * score\n        * i_evalue\n        * profile_coverage\n\n    :param hits: the hits to filter, all hits must match the same protein.\n    :type hits: [ :class:`macsypy.hit.CoreHit` object, ...]\n    :param str key: The criterion used to select the best hit 'score', i_evalue', 'profile_coverage'\n    :return: the list of the best hits\n    :rtype: [ :class:`macsypy.hit.CoreHit` object, ...]\n    \"\"\"\n    hits_register = {}\n    for hit in hits:\n        register_key = hit.replicon_name, hit.position\n        if register_key in hits_register:\n            hits_register[register_key].append(hit)\n        else:\n            hits_register[register_key] = [hit]\n    best_hits = []\n    for hits_on_same_prot in hits_register.values():\n        if key == 'score':\n            hits_on_same_prot.sort(key=attrgetter(key), reverse=True)\n        elif key == 'i_eval':\n            hits_on_same_prot.sort(key=attrgetter(key))\n        elif key == 'profile_coverage':\n            hits_on_same_prot.sort(key=attrgetter(key), reverse=True)\n        else:\n            raise MacsypyError(\n                f\"\"\"The criterion for Hits comparison {key} does not exist or is not available.\nIt must be either \"score\", \"i_eval\" or \"profile_coverage\".\"\"\"\n                )\n        best_hits.append(hits_on_same_prot[0])\n    return best_hits\n","145":"def load_bed_clusters(bedfile):\n    \"\"\"\n    Reads a BED file, using the fourth column as cluster number\n    Arguments: bedfile - a 4-column BED file\n    Returns: a hash with cluster numbers as key, and a list of genomic locations as value\n    \"\"\"\n    cluster_data = {}\n    track = pybedtools.BedTool(bedfile)\n    for f in track:\n        cluster_data.setdefault(_convert_value(f.score), []).append(\n            '{0}:{1}-{2}'.format(f.chrom, f.start, f.end))\n    return cluster_data\n","146":"def _jaccard_similarity(f1, f2, weight_func):\n    \"\"\"Calculate generalized Jaccard similarity of formulas.\n\n    Returns the weighted similarity value or None if there is no overlap\n    at all. If the union of the formulas has a weight of zero (i.e. the\n    denominator in the Jaccard similarity is zero), a value of zero is\n    returned.\n    \"\"\"\n    elements = set(f1)\n    elements.update(f2)\n    count, w_count, w_total = 0, 0, 0\n    for element in elements:\n        mi = min(f1.get(element, 0), f2.get(element, 0))\n        mx = max(f1.get(element, 0), f2.get(element, 0))\n        count += mi\n        w = weight_func(element)\n        w_count += w * mi\n        w_total += w * mx\n    if count == 0:\n        return None\n    return 0.0 if w_total == 0.0 else w_count \/ w_total\n","147":"def init_logger(level='INFO', out=True):\n    \"\"\"\n\n    :param level: The logger threshold could be a positive int or string\n                  among: 'CRITICAL', 'ERROR', 'WARNING', 'INFO', 'DEBUG'\n    :param out: if the log message must be displayed\n    :return: logger\n    :rtype: :class:`logging.Logger` instance\n    \"\"\"\n    logger = colorlog.getLogger('macsyprofile')\n    if isinstance(level, str):\n        level = getattr(logging, level)\n    if out:\n        stdout_handler = colorlog.StreamHandler(sys.stderr)\n        if level <= logging.DEBUG:\n            msg_formatter = (\n                '%(log_color)s%(levelname)-8s : %(module)s: L %(lineno)d :%(reset)s %(message)s'\n                )\n        else:\n            msg_formatter = '%(log_color)s%(message)s'\n        stdout_formatter = colorlog.ColoredFormatter(msg_formatter, datefmt\n            =None, reset=True, log_colors={'DEBUG': 'cyan', 'INFO': 'green',\n            'WARNING': 'yellow', 'ERROR': 'red', 'CRITICAL': 'bold_red'},\n            secondary_log_colors={}, style='%')\n        stdout_handler.setFormatter(stdout_formatter)\n        logger.addHandler(stdout_handler)\n    else:\n        null_handler = logging.NullHandler()\n        logger.addHandler(null_handler)\n    logger.setLevel(level)\n    return logger\n","148":"def print_current_params(params, old_params=None):\n    \"\"\"\n    Function to print current params\n\n    Keyword Arguments:\n            params (dict): parameter dict to print\n\n    \"\"\"\n    skipped_unchanged_params = 0\n    print('\\tCurrent parameters:')\n    print('\\t-->>')\n    for k, v in sorted(params.items()):\n        if old_params is not None and k in old_params.keys():\n            if params[k] == old_params[k]:\n                skipped_unchanged_params += 1\n        if isinstance(v, str) and len(v) > 70:\n            printable_v = v[:10] + ' ... ' + v[-50:]\n        else:\n            printable_v = v\n        print('{0: >42} : {1}'.format(k, printable_v))\n    if old_params is not None and skipped_unchanged_params > 0:\n        print(\n            '\\t{0} parameters have not been changed since last printout, thus skipped '\n            .format(skipped_unchanged_params))\n    print()\n","149":"def check_choice(raw, default, expected, sequence=False):\n    \"\"\"\n    Check if value is in list of expected values\n\n    :param str raw: the value return by the user\n    :param str default: the default value for the option\n    :param expected: the allowed vlaues for this option\n    :return: value\n    :raise MacsypyError: if the value cannot be cast in right type\n    \"\"\"\n\n    def isin(value):\n        if value not in expected:\n            raise ValueError(f'Authorized values are {expected}.')\n        if value.lower() == 'none':\n            value = None\n        return value\n    return _validator(isin, raw, default, sequence=sequence)\n","150":"def sliding_window_slow(iterable, window_size, flexible=True):\n    \"\"\"\n    Sliding window generator:\n    Slow but readable version using list slicing\n    currently not used.\n    \"\"\"\n    if flexible:\n        window_size = adjust_window_size(window_size, len(iterable))\n    if window_size % 2 == 0:\n        print(\n            'Warning! Window size must be uneven (to determine a central value). Adjusted window size from {0} to {1}.'\n            .format(window_size, window_size + 1))\n        window_size += 1\n    half_window_size = int((window_size - 1) \/ 2)\n    for center_i, center_value in enumerate(iterable):\n        start_i = center_i - half_window_size\n        if start_i < 0:\n            start_i = 0\n        stop_i = center_i + half_window_size + 1\n        yield iterable[start_i:stop_i], center_value\n","151":"def dereplicate_seqs(seqs_fp, output_fp, min_size=2, use_log=False, threads=1):\n    \"\"\"Dereplicate FASTA sequences and remove singletons using VSEARCH.\n\n    Parameters\n    ----------\n    seqs_fp : string\n        filepath to FASTA sequence file\n    output_fp : string\n        file path to dereplicated sequences (FASTA format)\n    min_size : integer, optional\n        discard sequences with an abundance value smaller\n        than integer\n    use_log: boolean, optional\n        save the vsearch logfile as well (to output_fp.log)\n        default=False\n    threads : int, optional\n        number of threads to use (0 for all available)\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    logger.info('dereplicate seqs file %s' % seqs_fp)\n    log_name = '%s.log' % output_fp\n    params = ['vsearch', '--derep_fulllength', seqs_fp, '--output',\n        output_fp, '--sizeout', '--fasta_width', '0', '--minuniquesize',\n        str(min_size), '--quiet', '--threads', str(threads)]\n    if use_log:\n        params.extend(['--log', log_name])\n    sout, serr, res = _system_call(params)\n    if not res == 0:\n        logger.error('Problem running vsearch dereplication on file %s' %\n            seqs_fp)\n        logger.debug('parameters used:\\n%s' % params)\n        logger.debug('stdout: %s' % sout)\n        logger.debug('stderr: %s' % serr)\n        return\n","152":"def set_loglevel(loglevel):\n    \"\"\"\n    Set minimum level for logging\n    >>> set_loglevel('info')   # log all messages except debugging messages. This is generally the default.\n    >>> set_loglevel('debug')   # log all messages, including debugging messages\n\n    Parameters\n    ----------\n    loglevel: str\n        {debug|info|warning|error|critical}\n    \"\"\"\n    if loglevel is not None:\n        loglevel_obj = getattr(logging, loglevel.upper())\n        logger.setLevel(loglevel_obj)\n","153":"def gene_metrics_by_gene(cnarr, threshold, skip_low=False):\n    \"\"\"Identify genes where average bin copy ratio value exceeds `threshold`.\n\n    NB: Adjust the sample's sex-chromosome log2 values beforehand with shift_xx,\n    otherwise all chrX\/chrY genes may be reported gained\/lost.\n    \"\"\"\n    for row in group_by_genes(cnarr, skip_low):\n        if abs(row.log2) >= threshold and row.gene:\n            yield row\n","154":"def log2_ratios(cnarr, absolutes, ploidy, is_reference_male, min_abs_val=\n    0.001, round_to_int=False):\n    \"\"\"Convert absolute copy numbers to log2 ratios.\n\n    Optionally round copy numbers to integers.\n\n    Account for reference sex & ploidy of sex chromosomes.\n    \"\"\"\n    if round_to_int:\n        absolutes = absolutes.round()\n    ratios = np.log2(np.maximum(absolutes \/ ploidy, min_abs_val))\n    if is_reference_male:\n        ratios[(cnarr.chromosome == cnarr._chr_x_label).values] += 1.0\n    ratios[(cnarr.chromosome == cnarr._chr_y_label).values] += 1.0\n    return ratios\n","155":"def get_var_bound(prob, var, objective_sense):\n    \"\"\"Gets upper or lower bound of a variable in an LP problem.\n\n    Args:\n        prob: :class:`psamm.lpsolver.lp.Problem`.\n        var: LP problem variable\n        objective_sense: :class:`psamm.lpsolver.lp.ObjectiveSense`\n    \"\"\"\n    prob.set_objective(var)\n    result = prob.solve_unchecked(objective_sense)\n    if not result.success:\n        logger.error(u'Solution not optimal: {}'.format(result.status))\n        quit()\n    return result.get_value(var)\n","156":"def kaiser(x, width=None, weights=None, do_fit_edges=False):\n    \"\"\"Smooth the values in `x` with the Kaiser windowed filter.\n\n    See: https:\/\/en.wikipedia.org\/wiki\/Kaiser_window\n\n    Parameters\n    ----------\n    x : array-like\n        1-dimensional numeric data set.\n    width : float\n        Fraction of x's total length to include in the rolling window (i.e. the\n        proportional window width), or the integer size of the window.\n    \"\"\"\n    if len(x) < 2:\n        return x\n    if width is None:\n        width = guess_window_size(x, weights)\n    x, wing, *padded = check_inputs(x, width, False, weights)\n    window = np.kaiser(2 * wing + 1, 14)\n    if weights is None:\n        signal, = padded\n        y = convolve_unweighted(window, signal, wing)\n    else:\n        signal, weights = padded\n        y, _w = convolve_weighted(window, signal, weights)\n    if do_fit_edges:\n        _fit_edges(x, y, wing)\n    return y\n"},"comment_lines":{"0":2,"1":12,"2":8,"3":4,"4":1,"5":4,"6":7,"7":7,"8":1,"9":6,"10":4,"11":9,"12":6,"13":3,"14":4,"15":10,"16":2,"17":2,"18":1,"19":3,"20":1,"21":13,"22":2,"23":4,"24":4,"25":4,"26":2,"27":8,"28":3,"29":3,"30":3,"31":8,"32":8,"33":3,"34":6,"35":7,"36":4,"37":2,"38":4,"39":2,"40":2,"41":1,"42":15,"43":2,"44":6,"45":6,"46":1,"47":4,"48":1,"49":4,"50":18,"51":14,"52":4,"53":5,"54":4,"55":3,"56":5,"57":3,"58":1,"59":5,"60":2,"61":8,"62":9,"63":2,"64":19,"65":17,"66":2,"67":8,"68":4,"69":2,"70":2,"71":3,"72":19,"73":2,"74":10,"75":8,"76":16,"77":3,"78":12,"79":8,"80":8,"81":3,"82":3,"83":5,"84":5,"85":15,"86":2,"87":1,"88":12,"89":2,"90":10,"91":17,"92":5,"93":15,"94":3,"95":3,"96":3,"97":12,"98":1,"99":4,"100":4,"101":11,"102":14,"103":7,"104":2,"105":10,"106":9,"107":11,"108":2,"109":14,"110":2,"111":12,"112":3,"113":3,"114":3,"115":3,"116":3,"117":3,"118":9,"119":3,"120":4,"121":8,"122":5,"123":2,"124":3,"125":6,"126":4,"127":2,"128":11,"129":2,"130":2,"131":6,"132":3,"133":5,"134":15,"135":3,"136":2,"137":3,"138":6,"139":5,"140":9,"141":3,"142":8,"143":6,"144":10,"145":2,"146":5,"147":4,"148":3,"149":6,"150":2,"151":15,"152":7,"153":3,"154":4,"155":5,"156":10},"tokens":{"0":124,"1":263,"2":329,"3":135,"4":351,"5":95,"6":208,"7":148,"8":244,"9":178,"10":84,"11":299,"12":186,"13":126,"14":210,"15":314,"16":154,"17":159,"18":262,"19":134,"20":106,"21":260,"22":128,"23":251,"24":202,"25":275,"26":71,"27":188,"28":158,"29":130,"30":136,"31":317,"32":233,"33":138,"34":172,"35":129,"36":300,"37":278,"38":219,"39":176,"40":489,"41":159,"42":308,"43":509,"44":165,"45":248,"46":438,"47":87,"48":151,"49":275,"50":311,"51":395,"52":106,"53":192,"54":101,"55":146,"56":114,"57":135,"58":401,"59":111,"60":419,"61":128,"62":346,"63":110,"64":283,"65":284,"66":68,"67":347,"68":194,"69":187,"70":172,"71":100,"72":383,"73":171,"74":376,"75":134,"76":339,"77":104,"78":293,"79":243,"80":148,"81":129,"82":189,"83":174,"84":163,"85":263,"86":399,"87":423,"88":921,"89":95,"90":879,"91":272,"92":238,"93":280,"94":447,"95":124,"96":127,"97":202,"98":248,"99":210,"100":178,"101":301,"102":223,"103":112,"104":159,"105":205,"106":134,"107":403,"108":132,"109":316,"110":133,"111":238,"112":87,"113":178,"114":341,"115":198,"116":140,"117":174,"118":406,"119":175,"120":259,"121":219,"122":213,"123":69,"124":123,"125":128,"126":227,"127":117,"128":158,"129":160,"130":118,"131":155,"132":54,"133":393,"134":329,"135":95,"136":122,"137":53,"138":85,"139":98,"140":173,"141":217,"142":120,"143":178,"144":343,"145":120,"146":220,"147":284,"148":214,"149":149,"150":198,"151":340,"152":109,"153":96,"154":158,"155":121,"156":239},"lines":{"0":15,"1":39,"2":40,"3":18,"4":42,"5":14,"6":26,"7":23,"8":24,"9":21,"10":12,"11":46,"12":20,"13":14,"14":26,"15":39,"16":20,"17":18,"18":33,"19":14,"20":10,"21":29,"22":16,"23":27,"24":25,"25":31,"26":9,"27":22,"28":23,"29":20,"30":20,"31":46,"32":24,"33":20,"34":19,"35":17,"36":42,"37":27,"38":27,"39":25,"40":57,"41":21,"42":48,"43":58,"44":19,"45":31,"46":50,"47":12,"48":18,"49":28,"50":41,"51":53,"52":12,"53":24,"54":12,"55":16,"56":13,"57":14,"58":39,"59":14,"60":51,"61":19,"62":43,"63":16,"64":39,"65":40,"66":10,"67":32,"68":22,"69":23,"70":26,"71":13,"72":53,"73":23,"74":48,"75":19,"76":44,"77":11,"78":34,"79":24,"80":17,"81":14,"82":22,"83":17,"84":26,"85":31,"86":49,"87":40,"88":125,"89":16,"90":125,"91":41,"92":28,"93":33,"94":44,"95":14,"96":11,"97":26,"98":25,"99":28,"100":17,"101":35,"102":27,"103":15,"104":16,"105":19,"106":14,"107":50,"108":19,"109":39,"110":19,"111":30,"112":13,"113":19,"114":30,"115":28,"116":18,"117":17,"118":55,"119":19,"120":28,"121":25,"122":22,"123":7,"124":14,"125":13,"126":21,"127":18,"128":25,"129":18,"130":15,"131":16,"132":8,"133":45,"134":54,"135":13,"136":18,"137":8,"138":12,"139":10,"140":23,"141":23,"142":16,"143":19,"144":35,"145":11,"146":24,"147":38,"148":29,"149":17,"150":25,"151":41,"152":14,"153":9,"154":19,"155":14,"156":29},"parameters":{"0":0,"1":3,"2":3,"3":1,"4":1,"5":2,"6":2,"7":2,"8":4,"9":4,"10":2,"11":3,"12":4,"13":2,"14":4,"15":4,"16":4,"17":2,"18":2,"19":2,"20":2,"21":1,"22":3,"23":1,"24":4,"25":1,"26":2,"27":2,"28":2,"29":1,"30":1,"31":3,"32":1,"33":2,"34":1,"35":1,"36":2,"37":2,"38":6,"39":2,"40":1,"41":2,"42":4,"43":1,"44":4,"45":3,"46":4,"47":4,"48":3,"49":1,"50":5,"51":8,"52":2,"53":1,"54":2,"55":1,"56":1,"57":3,"58":6,"59":3,"60":3,"61":1,"62":8,"63":1,"64":5,"65":1,"66":1,"67":3,"68":4,"69":1,"70":1,"71":1,"72":1,"73":3,"74":1,"75":1,"76":3,"77":3,"78":2,"79":2,"80":3,"81":2,"82":2,"83":3,"84":1,"85":3,"86":3,"87":4,"88":6,"89":0,"90":1,"91":1,"92":1,"93":2,"94":1,"95":1,"96":2,"97":4,"98":1,"99":1,"100":2,"101":3,"102":5,"103":2,"104":3,"105":2,"106":2,"107":5,"108":0,"109":3,"110":0,"111":4,"112":0,"113":7,"114":6,"115":5,"116":2,"117":4,"118":4,"119":1,"120":5,"121":2,"122":2,"123":1,"124":1,"125":3,"126":1,"127":2,"128":5,"129":2,"130":1,"131":4,"132":1,"133":2,"134":6,"135":3,"136":1,"137":1,"138":1,"139":2,"140":1,"141":4,"142":1,"143":1,"144":2,"145":1,"146":3,"147":2,"148":2,"149":4,"150":3,"151":5,"152":1,"153":3,"154":6,"155":3,"156":4},"functions":{"0":13,"1":28,"2":45,"3":14,"4":36,"5":8,"6":19,"7":13,"8":23,"9":23,"10":10,"11":24,"12":23,"13":15,"14":23,"15":25,"16":18,"17":13,"18":36,"19":14,"20":11,"21":24,"22":11,"23":23,"24":23,"25":23,"26":6,"27":19,"28":15,"29":11,"30":11,"31":25,"32":23,"33":18,"34":13,"35":11,"36":23,"37":36,"38":17,"39":15,"40":58,"41":18,"42":24,"43":58,"44":23,"45":30,"46":36,"47":10,"48":18,"49":23,"50":23,"51":24,"52":13,"53":14,"54":13,"55":13,"56":13,"57":11,"58":36,"59":13,"60":58,"61":14,"62":22,"63":14,"64":24,"65":25,"66":5,"67":34,"68":18,"69":14,"70":18,"71":6,"72":24,"73":14,"74":25,"75":11,"76":22,"77":8,"78":19,"79":22,"80":18,"81":18,"82":14,"83":14,"84":23,"85":19,"86":58,"87":36,"88":78,"89":10,"90":78,"91":25,"92":20,"93":19,"94":58,"95":9,"96":15,"97":21,"98":36,"99":20,"100":14,"101":36,"102":24,"103":8,"104":23,"105":17,"106":14,"107":25,"108":10,"109":19,"110":10,"111":21,"112":6,"113":18,"114":34,"115":13,"116":8,"117":14,"118":25,"119":17,"120":30,"121":13,"122":22,"123":10,"124":9,"125":13,"126":23,"127":11,"128":17,"129":10,"130":7,"131":23,"132":4,"133":37,"134":25,"135":14,"136":9,"137":4,"138":12,"139":13,"140":12,"141":15,"142":13,"143":23,"144":36,"145":9,"146":14,"147":16,"148":13,"149":23,"150":14,"151":19,"152":14,"153":7,"154":13,"155":13,"156":18},"globals":{"0":2,"1":5,"2":15,"3":1,"4":4,"5":3,"6":2,"7":4,"8":11,"9":1,"10":1,"11":7,"12":1,"13":0,"14":1,"15":1,"16":0,"17":2,"18":4,"19":0,"20":2,"21":7,"22":0,"23":1,"24":1,"25":1,"26":0,"27":2,"28":0,"29":2,"30":2,"31":1,"32":1,"33":0,"34":2,"35":2,"36":1,"37":4,"38":2,"39":0,"40":1,"41":0,"42":7,"43":1,"44":1,"45":1,"46":4,"47":1,"48":0,"49":1,"50":11,"51":7,"52":0,"53":1,"54":0,"55":1,"56":0,"57":0,"58":4,"59":0,"60":1,"61":1,"62":3,"63":0,"64":7,"65":1,"66":0,"67":1,"68":0,"69":0,"70":0,"71":1,"72":7,"73":0,"74":1,"75":2,"76":3,"77":0,"78":2,"79":3,"80":0,"81":0,"82":0,"83":0,"84":1,"85":2,"86":1,"87":4,"88":13,"89":0,"90":13,"91":1,"92":2,"93":2,"94":1,"95":0,"96":0,"97":2,"98":4,"99":3,"100":0,"101":1,"102":7,"103":1,"104":11,"105":2,"106":1,"107":1,"108":0,"109":2,"110":0,"111":2,"112":0,"113":0,"114":1,"115":0,"116":1,"117":1,"118":1,"119":0,"120":0,"121":1,"122":3,"123":0,"124":0,"125":0,"126":1,"127":0,"128":2,"129":1,"130":0,"131":1,"132":0,"133":21,"134":1,"135":0,"136":0,"137":0,"138":1,"139":0,"140":1,"141":0,"142":0,"143":1,"144":1,"145":0,"146":1,"147":1,"148":2,"149":1,"150":0,"151":2,"152":0,"153":0,"154":0,"155":1,"156":0},"imports":{"0":7,"1":17,"2":15,"3":6,"4":19,"5":8,"6":18,"7":8,"8":12,"9":12,"10":4,"11":24,"12":12,"13":9,"14":19,"15":19,"16":8,"17":7,"18":19,"19":9,"20":5,"21":24,"22":8,"23":19,"24":19,"25":19,"26":5,"27":18,"28":7,"29":5,"30":5,"31":19,"32":19,"33":10,"34":8,"35":5,"36":19,"37":19,"38":9,"39":9,"40":22,"41":6,"42":24,"43":22,"44":12,"45":7,"46":19,"47":4,"48":6,"49":19,"50":12,"51":24,"52":4,"53":8,"54":4,"55":14,"56":4,"57":8,"58":19,"59":4,"60":22,"61":6,"62":24,"63":4,"64":24,"65":19,"66":4,"67":28,"68":6,"69":9,"70":6,"71":6,"72":24,"73":6,"74":19,"75":5,"76":24,"77":6,"78":18,"79":24,"80":6,"81":6,"82":8,"83":9,"84":12,"85":18,"86":22,"87":19,"88":17,"89":5,"90":17,"91":19,"92":5,"93":18,"94":22,"95":5,"96":9,"97":5,"98":19,"99":24,"100":9,"101":6,"102":24,"103":7,"104":12,"105":9,"106":6,"107":19,"108":5,"109":18,"110":5,"111":5,"112":5,"113":6,"114":28,"115":13,"116":7,"117":6,"118":19,"119":4,"120":6,"121":14,"122":24,"123":5,"124":4,"125":4,"126":22,"127":3,"128":9,"129":7,"130":7,"131":12,"132":3,"133":3,"134":19,"135":4,"136":4,"137":3,"138":6,"139":4,"140":6,"141":9,"142":4,"143":19,"144":6,"145":6,"146":8,"147":15,"148":7,"149":12,"150":6,"151":18,"152":10,"153":6,"154":4,"155":14,"156":6}}