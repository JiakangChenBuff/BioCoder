{"golden":{"0":"def parse_requirements(requirements):\n    \"\"\"Parse a requirement into a module and version parts.\"\"\"\n    reqs = {}\n    for req in requirements.splitlines():\n        match = re.match('^([^>=<]+)([>=<]+)([^>=<]+)$', req)\n        module = match.group(1)\n        compare = match.group(2)\n        version = LooseVersion(match.group(3))\n        reqs[module] = {'compare': compare, 'version': version}\n    return reqs\n","1":"def is_fastq_file(args, file_name):\n    \"\"\"Check if this a FASTQ file.\"\"\"\n    if args.get('fasta'):\n        return False\n    if args.get('fastq'):\n        return True\n    parts = file_name.lower().split('.')\n    index = -2 if re.search('[zp2]$', parts[-1]) and len(parts) > 2 else -1\n    return parts[index].startswith('f') and parts[index].endswith('q')\n","2":"def shard_file_size(args, file_name):\n    \"\"\"Calculate shard file size for FASTA\/Q files in raw or zipped format.\"\"\"\n    file_size = getsize(file_name)\n    if args.get('gzip'):\n        with gzip.open(file_name, 'rb') as zippy:\n            file_size = zippy.seek(0, io.SEEK_END)\n    elif args.get('bzip'):\n        with bz2.open(file_name, 'rb') as zippy:\n            file_size = zippy.seek(0, io.SEEK_END)\n    if is_fastq_file(args, file_name):\n        file_size \/= 2\n    return file_size\n","3":"def fasta_file_has_protein(query_files):\n    \"\"\"Search for protein characters in a fasta file.\"\"\"\n    for query_file in query_files:\n        with open(query_file) as in_file:\n            for query in SeqIO.parse(in_file, 'fasta'):\n                if is_protein(str(query.seq)):\n                    return True\n    return False\n","4":"def parse_contig_file_name(ref_names, taxon_names, contig_file):\n    \"\"\"Extract the reference & taxon names from the contig file name.\"\"\"\n    sep = '[_. ]'\n    ref_names = [(x, re.sub(sep, sep, x) + sep) for x in ref_names]\n    ref_names = sorted(ref_names, key=lambda x: (len(x[1]), x), reverse=True)\n    taxon_names = [(x, re.sub(sep, sep, x) + sep) for x in taxon_names]\n    taxon_names = sorted(taxon_names, key=lambda x: (len(x[1]), x), reverse\n        =True)\n    ref_name = [x[0] for x in ref_names if re.search(x[1], contig_file)]\n    taxon_name = [x[0] for x in taxon_names if re.search(x[1], contig_file)]\n    ref_name += [None]\n    taxon_name += [None]\n    return ref_name[0], taxon_name[0]\n","5":"def name_contig(taxon_name, ref_name, header, names_seen):\n    \"\"\"Shorten contig names.\"\"\"\n    global DEFAULT\n    DEFAULT += 1\n    match = ITERATION.search(header)\n    iteration = 'i{}'.format(match[1]) if match else ''\n    match = COVERAGE.search(header)\n    coverage = 'c{}'.format(round(float(match[1]))) if match else ''\n    match = SCORE.search(header)\n    score = 's{}'.format(round(float(match[1]))) if match else ''\n    contig = '{}{}{}'.format(iteration, coverage, score)\n    contig = contig if contig else str(DEFAULT)\n    name = '{}@{}_{}'.format(taxon_name, ref_name, contig)\n    name = re.sub('[^\\\\w@]+', '_', name.strip())\n    name = handle_duplicate_name(name, names_seen)\n    return name\n","6":"def handle_duplicate_name(contig_name, names_seen):\n    \"\"\"Add a tiebreaker to a duplicate contig name.\"\"\"\n    name = re.sub('_v\\\\d+$', '', contig_name, re.IGNORECASE)\n    names_seen[name] += 1\n    if names_seen[name] > 1:\n        name += '_v{}'.format(names_seen[name])\n    return name\n","7":"def default_shard_count(args, sra_files):\n    \"\"\"Calculate the default number of shards.\"\"\"\n    shard_count = args['shard_count']\n    if not shard_count:\n        total_fasta_size = 0\n        for file_name in sra_files:\n            total_fasta_size += util.shard_file_size(args, file_name)\n        shard_count = int(total_fasta_size \/ 250000000.0)\n        shard_count = shard_count if shard_count else 1\n    return shard_count\n","8":"def touchup_blast_db_names(blast_dbs):\n    \"\"\"Allow users to enter blast DB names with various suffixes.\"\"\"\n    pattern = re.compile(\n        '^ (.*?)(  \\\\.atram(_preprocessor)?\\\\.log | \\\\.blast_\\\\d{3}\\\\.(nhr|nin|nsq) | \\\\.sqlite\\\\.db  )?$'\n        , re.I | re.X)\n    db_names = []\n    for blast_db in blast_dbs:\n        db_names.append(re.sub(pattern, '\\\\1', blast_db))\n    return db_names\n","9":"def expand(labels, distance):\n    \"\"\"\n    Expand labels by a specified distance.\n    \"\"\"\n    background = labels == 0\n    distances, (i, j) = scipy.ndimage.distance_transform_edt(background,\n        return_indices=True)\n    out_labels = labels.copy()\n    mask = background & (distances <= distance)\n    out_labels[mask] = labels[i[mask], j[mask]]\n    return out_labels\n","10":"def discard_objects(labels_x, labels_y):\n    \"\"\"\n    Discard objects that overlap with objects in the initial set\n    \"\"\"\n    output = numpy.zeros_like(labels_x)\n    indices_x = numpy.unique(labels_x)\n    indices_x = indices_x[indices_x > 0]\n    indices_y = numpy.unique(labels_y)\n    indices_y = indices_y[indices_y > 0]\n    undisputed = numpy.logical_xor(labels_x > 0, labels_y > 0)\n    undisputed_x = numpy.setdiff1d(indices_x, labels_x[~undisputed])\n    mask = numpy.isin(labels_x, undisputed_x)\n    output = numpy.where(mask, labels_x, output)\n    labels_x[mask] = 0\n    undisputed_y = numpy.setdiff1d(indices_y, labels_y[~undisputed])\n    mask = numpy.isin(labels_y, undisputed_y)\n    output = numpy.where(mask, labels_y, output)\n    labels_y[mask] = 0\n    return numpy.where(labels_x > 0, labels_x, output)\n","11":"def segment_objects(labels_x, labels_y, dimensions):\n    \"\"\"\n    Combine object sets and re-draw segmentation for overlapping\n    objects.\n    \"\"\"\n    output = numpy.zeros_like(labels_x)\n    labels_y[labels_y > 0] += labels_x.max()\n    indices_x = numpy.unique(labels_x)\n    indices_x = indices_x[indices_x > 0]\n    indices_y = numpy.unique(labels_y)\n    indices_y = indices_y[indices_y > 0]\n    undisputed = numpy.logical_xor(labels_x > 0, labels_y > 0)\n    undisputed_x = numpy.setdiff1d(indices_x, labels_x[~undisputed])\n    mask = numpy.isin(labels_x, undisputed_x)\n    output = numpy.where(mask, labels_x, output)\n    labels_x[mask] = 0\n    undisputed_y = numpy.setdiff1d(indices_y, labels_y[~undisputed])\n    mask = numpy.isin(labels_y, undisputed_y)\n    output = numpy.where(mask, labels_y, output)\n    labels_y[mask] = 0\n    to_segment = numpy.logical_or(labels_x > 0, labels_y > 0)\n    disputed = numpy.logical_and(labels_x > 0, labels_y > 0)\n    seeds = numpy.add(labels_x, labels_y)\n    will_be_lost = numpy.setdiff1d(labels_x[disputed], labels_x[~disputed])\n    for label in will_be_lost:\n        x_mask = labels_x == label\n        y_lab = numpy.unique(labels_y[x_mask])\n        if not y_lab or len(y_lab) > 1:\n            continue\n        else:\n            y_mask = labels_y == y_lab[0]\n            if numpy.array_equal(x_mask, y_mask):\n                output[x_mask] = label\n                to_segment[x_mask] = False\n    seeds[disputed] = 0\n    if dimensions == 2:\n        distances, (i, j) = scipy.ndimage.distance_transform_edt(seeds == 0,\n            return_indices=True)\n        output[to_segment] = seeds[i[to_segment], j[to_segment]]\n    elif dimensions == 3:\n        distances, (i, j, v) = scipy.ndimage.distance_transform_edt(seeds ==\n            0, return_indices=True)\n        output[to_segment] = seeds[i[to_segment], j[to_segment], v[to_segment]]\n    return output\n","12":"def fill_object_holes(labels, diameter, planewise=False):\n    array = labels.copy()\n    radius = diameter \/ 2.0\n    if labels.ndim == 2 or labels.shape[-1] in (3, 4) or planewise:\n        factor = radius ** 2\n    else:\n        factor = 4.0 \/ 3.0 * radius ** 3\n    min_obj_size = numpy.pi * factor\n    if planewise and labels.ndim != 2 and labels.shape[-1] not in (3, 4):\n        for plane in array:\n            for obj in numpy.unique(plane):\n                if obj == 0:\n                    continue\n                filled_mask = skimage.morphology.remove_small_holes(plane ==\n                    obj, min_obj_size)\n                plane[filled_mask] = obj\n        return array\n    else:\n        for obj in numpy.unique(array):\n            if obj == 0:\n                continue\n            filled_mask = skimage.morphology.remove_small_holes(array ==\n                obj, min_obj_size)\n            array[filled_mask] = obj\n    return array\n","13":"def fill_convex_hulls(labels):\n    data = skimage.measure.regionprops(labels)\n    output = numpy.zeros_like(labels)\n    for prop in data:\n        label = prop['label']\n        bbox = prop['bbox']\n        cmask = prop['convex_image']\n        if len(bbox) <= 4:\n            output[bbox[0]:bbox[2], bbox[1]:bbox[3]][cmask] = label\n        else:\n            output[bbox[0]:bbox[3], bbox[1]:bbox[4], bbox[2]:bbox[5]][cmask\n                ] = label\n    return output\n","14":"def morphology_closing(image, structuring_element=skimage.morphology.disk(1)):\n    if structuring_element.ndim == 3 and image.ndim == 2:\n        raise ValueError('Cannot apply a 3D structuring element to a 2D image')\n    planewise = structuring_element.ndim == 2 and image.ndim == 3\n    if planewise:\n        output = numpy.zeros_like(image)\n        for index, plane in enumerate(image):\n            output[index] = skimage.morphology.closing(plane,\n                structuring_element)\n        return output\n    else:\n        return skimage.morphology.closing(image, structuring_element)\n","15":"def reduce_noise(image, patch_size, patch_distance, cutoff_distance,\n    channel_axis=None):\n    denoised = skimage.restoration.denoise_nl_means(image=image, patch_size\n        =patch_size, patch_distance=patch_distance, h=cutoff_distance,\n        channel_axis=channel_axis, fast_mode=True)\n    return denoised\n","16":"def apply_threshold(image, threshold, mask=None, smoothing=0):\n    if mask is None:\n        mask = numpy.full(image.shape, True)\n    if smoothing == 0:\n        return (image >= threshold) & mask, 0\n    else:\n        sigma = smoothing \/ 0.6744 \/ 2.0\n    blurred_image = centrosome.smooth.smooth_with_function_and_mask(image, \n        lambda x: scipy.ndimage.gaussian_filter(x, sigma, mode='constant',\n        cval=0), mask)\n    return (blurred_image >= threshold) & mask, sigma\n","17":"def fillobjects(labels, mode='holes', diameter=64.0, planewise=False):\n    if mode.casefold() == 'holes':\n        return fill_object_holes(labels, diameter, planewise)\n    elif mode.casefold() in ('convex hull', 'convex_hull'):\n        return fill_convex_hulls(labels)\n    else:\n        raise ValueError(\n            f\"Mode '{mode}' is not supported. Available modes are: 'holes' and 'convex_hull'.\"\n            )\n","18":"def dilation(x_data, structuring_element):\n    is_strel_2d = structuring_element.ndim == 2\n    is_img_2d = x_data.ndim == 2\n    if is_strel_2d and not is_img_2d:\n        y_data = numpy.zeros_like(x_data)\n        for index, plane in enumerate(x_data):\n            y_data[index] = skimage.morphology.dilation(plane,\n                structuring_element)\n        return y_data\n    if not is_strel_2d and is_img_2d:\n        raise NotImplementedError(\n            'A 3D structuring element cannot be applied to a 2D image.')\n    y_data = skimage.morphology.dilation(x_data, structuring_element)\n    return y_data\n","19":"def erosion(x_data, structuring_element):\n    is_strel_2d = structuring_element.ndim == 2\n    is_img_2d = x_data.ndim == 2\n    if is_strel_2d and not is_img_2d:\n        y_data = numpy.zeros_like(x_data)\n        for index, plane in enumerate(x_data):\n            y_data[index] = skimage.morphology.erosion(plane,\n                structuring_element)\n        return y_data\n    if not is_strel_2d and is_img_2d:\n        raise NotImplementedError(\n            'A 3D structuring element cannot be applied to a 2D image.')\n    y_data = skimage.morphology.erosion(x_data, structuring_element)\n    return y_data\n","20":"def binary_erosion(x_data, structuring_element):\n    is_strel_2d = structuring_element.ndim == 2\n    is_img_2d = x_data.ndim == 2\n    if is_strel_2d and not is_img_2d:\n        y_data = numpy.zeros_like(x_data)\n        for index, plane in enumerate(x_data):\n            y_data[index] = skimage.morphology.binary_erosion(plane,\n                structuring_element)\n        return y_data\n    if not is_strel_2d and is_img_2d:\n        raise NotImplementedError(\n            'A 3D structuring element cannot be applied to a 2D image.')\n    y_data = skimage.morphology.binary_erosion(x_data, structuring_element)\n    return y_data\n","21":"def morphological_gradient(x_data, structuring_element):\n    is_strel_2d = structuring_element.ndim == 2\n    is_img_2d = x_data.ndim == 2\n    if is_strel_2d and not is_img_2d:\n        y_data = numpy.zeros_like(x_data)\n        for index, plane in enumerate(x_data):\n            y_data[index] = scipy.ndimage.morphological_gradient(plane,\n                footprint=structuring_element)\n        return y_data\n    if not is_strel_2d and is_img_2d:\n        raise NotImplementedError(\n            'A 3D structuring element cannot be applied to a 2D image.')\n    y_data = scipy.ndimage.morphological_gradient(x_data, footprint=\n        structuring_element)\n    return y_data\n","22":"def get_text(self, start_idx=0, end_idx=None):\n    \"\"\"Return the text representation of the tokens between the given indices\n\n        start_idx - index of first token in string\n        end_idx - index of last token or -1 for all\n        \"\"\"\n    value = ''\n    if end_idx is None:\n        end_idx = len(self.__tokens)\n    for token in self.__tokens[start_idx:end_idx]:\n        if isinstance(token, self.MetadataToken):\n            value += token.value\n        else:\n            value += token\n    return value\n","23":"@staticmethod\ndef splitpath(path):\n    slash = path.rfind('\/')\n    if slash == -1:\n        if path.lower().startswith(OMERO_SCHEME):\n            return [path[:len(OMERO_SCHEME)], path[len(OMERO_SCHEME):]]\n        return '', path\n    else:\n        return path[:slash], path[slash + 1:]\n","24":"@staticmethod\ndef in_bounds(image, xi, yi):\n    \"\"\"Return false if xi or yi are outside of the bounds of the image\"\"\"\n    return not (image is None or xi >= image.shape[1] or yi >= image.shape[\n        0] or xi < 0 or yi < 0)\n","25":"@staticmethod\ndef compute_samples(lower_bound, upper_bound, number_samples, is_int):\n    \"\"\"Computes samples in the range [lower_bound, upper_bound].\n\n        This method computes an returns a list of uniformly distributed samples\n        in the range [lower_bound, upper_bound]. The returned list will be of\n        size 'number_samples'. If 'is_int' is true, samples will be rounded to\n        the nearest integer, except for the last, which will be rounded to\n        'upper_bound'.\n        \"\"\"\n    samples = []\n    if number_samples > 1:\n        delta = (upper_bound - lower_bound) \/ float(number_samples - 1)\n        for i in range(number_samples):\n            sample = lower_bound + i * delta\n            if is_int:\n                if i == number_samples:\n                    sample = upper_bound\n                else:\n                    sample = int(sample)\n            samples.append(sample)\n    else:\n        samples.append(lower_bound)\n    return samples\n","26":"@property\ndef outlines(self):\n    \"\"\"Get a mask of all the points on the border of objects\"\"\"\n    if self._outlines is None:\n        for i, labels in enumerate(self.labels):\n            if i == 0:\n                self._outlines = centrosome.outline.outline(labels) != 0\n            else:\n                self._outlines |= centrosome.outline.outline(labels) != 0\n        if self.line_width is not None and self.line_width > 1:\n            hw = float(self.line_width) \/ 2\n            d = scipy.ndimage.distance_transform_edt(~self._outlines)\n            dti, dtj = numpy.where((d < hw + 0.5) & ~self._outlines)\n            self._outlines = self._outlines.astype(numpy.float32)\n            self._outlines[dti, dtj] = numpy.minimum(1, hw + 0.5 - d[dti, dtj])\n    return self._outlines.astype(numpy.float32)\n","27":"def format_plate_data_as_array(plate_dict, plate_type):\n    \"\"\" Returns an array shaped like the given plate type with the values from\n    plate_dict stored in it.  Wells without data will be set to np.NaN\n    plate_dict  -  dict mapping well names to data. eg: d[\"A01\"] --> data\n                   data values must be of numerical or string types\n    plate_type  - '96' (return 8x12 array) or '384' (return 16x24 array)\n    \"\"\"\n    if plate_type == '96':\n        plate_shape = 8, 12\n    elif plate_type == '384':\n        plate_shape = 16, 24\n    alphabet = 'ABCDEFGHIJKLMNOP'\n    data = numpy.zeros(plate_shape)\n    data[:] = numpy.nan\n    display_error = True\n    for well, val in list(plate_dict.items()):\n        r = alphabet.index(well[0].upper())\n        c = int(well[1:]) - 1\n        if r >= data.shape[0] or c >= data.shape[1]:\n            if display_error:\n                LOGGER.warning(\n                    'A well value (%s) does not fit in the given plate type.\\n'\n                     % well)\n                display_error = False\n            continue\n        data[r, c] = val\n    return data\n","28":"def __search_fn(html, text):\n    \"\"\"\n    Find the beginning and ending indices of case insensitive matches of \"text\"\n    within the text-data of the HTML, searching only in its body and excluding\n    text in the HTML tags.\n\n    :param html: an HTML document\n    :param text: a search string\n    :return:\n    \"\"\"\n    start_match = re.search('<\\\\s*body[^>]*?>', html, re.IGNORECASE)\n    if start_match is None:\n        start = 0\n    else:\n        start = start_match.end()\n    end_match = re.search('<\\\\\\\\\\\\s*body', html, re.IGNORECASE)\n    if end_match is None:\n        end = len(html)\n    else:\n        end = end_match.start()\n    escaped_text = re.escape(text)\n    if ' ' in escaped_text:\n        escaped_text = escaped_text.replace('\\\\ ', '\\\\s+')\n    pattern = '(<[^>]*?>|%s)' % escaped_text\n    return [(x.start() + start, x.end() + start) for x in re.finditer(\n        pattern, html[start:end], re.IGNORECASE) if x.group(1)[0] != '<']\n","29":"def resize(self, pixel_data, labels):\n    initial_shape = labels.shape\n    final_shape = pixel_data.shape\n    if pixel_data.ndim > labels.ndim:\n        final_shape = final_shape[:-1]\n    adjust = numpy.subtract(final_shape, initial_shape)\n    cropped = skimage.util.crop(labels, [(0, dim_adjust) for dim_adjust in\n        numpy.abs(numpy.minimum(adjust, numpy.zeros_like(adjust)))])\n    return numpy.pad(cropped, [(0, dim_adjust) for dim_adjust in numpy.\n        maximum(adjust, numpy.zeros_like(adjust))], mode='constant',\n        constant_values=0)\n","30":"def combine_arrays(self, labels_x, labels_y):\n    output = numpy.zeros_like(labels_x)\n    method = self.merge_method.value\n    labels_y[labels_y > 0] += labels_x.max()\n    if method == 'Preserve':\n        return numpy.where(labels_x > 0, labels_x, labels_y)\n    indices_x = numpy.unique(labels_x)\n    indices_x = indices_x[indices_x > 0]\n    indices_y = numpy.unique(labels_y)\n    indices_y = indices_y[indices_y > 0]\n    undisputed = numpy.logical_xor(labels_x > 0, labels_y > 0)\n    undisputed_x = numpy.setdiff1d(indices_x, labels_x[~undisputed])\n    mask = numpy.isin(labels_x, undisputed_x)\n    output = numpy.where(mask, labels_x, output)\n    labels_x[mask] = 0\n    undisputed_y = numpy.setdiff1d(indices_y, labels_y[~undisputed])\n    mask = numpy.isin(labels_y, undisputed_y)\n    output = numpy.where(mask, labels_y, output)\n    labels_y[mask] = 0\n    is_2d = labels_x.ndim == 2\n    if method == 'Discard':\n        return numpy.where(labels_x > 0, labels_x, output)\n    elif method == 'Segment':\n        to_segment = numpy.logical_or(labels_x > 0, labels_y > 0)\n        disputed = numpy.logical_and(labels_x > 0, labels_y > 0)\n        seeds = numpy.add(labels_x, labels_y)\n        will_be_lost = numpy.setdiff1d(labels_x[disputed], labels_x[~disputed])\n        for label in will_be_lost:\n            x_mask = labels_x == label\n            y_lab = numpy.unique(labels_y[x_mask])\n            if not y_lab or len(y_lab) > 1:\n                continue\n            else:\n                y_mask = labels_y == y_lab[0]\n                if numpy.array_equal(x_mask, y_mask):\n                    output[x_mask] = label\n                    to_segment[x_mask] = False\n        seeds[disputed] = 0\n        if is_2d:\n            distances, (i, j) = scipy.ndimage.distance_transform_edt(seeds ==\n                0, return_indices=True)\n            output[to_segment] = seeds[i[to_segment], j[to_segment]]\n        else:\n            distances, (i, j, v) = scipy.ndimage.distance_transform_edt(\n                seeds == 0, return_indices=True)\n            output[to_segment] = seeds[i[to_segment], j[to_segment], v[\n                to_segment]]\n    elif method == 'Merge':\n        to_segment = numpy.logical_or(labels_x > 0, labels_y > 0)\n        if is_2d:\n            distances, (i, j) = scipy.ndimage.distance_transform_edt(\n                labels_x == 0, return_indices=True)\n            output[to_segment] = labels_x[i[to_segment], j[to_segment]]\n        else:\n            distances, (i, j, v) = scipy.ndimage.distance_transform_edt(\n                labels_x == 0, return_indices=True)\n            output[to_segment] = labels_x[i[to_segment], j[to_segment], v[\n                to_segment]]\n    return output\n","31":"def smooth_with_convex_hull(self, pixel_data, mask):\n    \"\"\"Use the convex hull transform to smooth the image\"\"\"\n    image = centrosome.cpmorphology.grey_erosion(pixel_data, 2, mask)\n    image = centrosome.filter.convex_hull_transform(image, mask=mask)\n    image = centrosome.cpmorphology.grey_dilation(image, 2, mask)\n    return image\n","32":"def smooth_with_splines(self, pixel_data, mask):\n    if self.automatic_splines:\n        shortest_side = min(pixel_data.shape)\n        if shortest_side < 200:\n            scale = 1\n        else:\n            scale = float(shortest_side) \/ 200\n        result = centrosome.bg_compensate.backgr(pixel_data, mask, scale=scale)\n    else:\n        mode = self.spline_bg_mode.value\n        spline_points = self.spline_points.value\n        threshold = self.spline_threshold.value\n        convergence = self.spline_convergence.value\n        iterations = self.spline_maximum_iterations.value\n        rescale = self.spline_rescale.value\n        result = centrosome.bg_compensate.backgr(pixel_data, mask, mode=\n            mode, thresh=threshold, splinepoints=spline_points, scale=\n            rescale, maxiter=iterations, convergence=convergence)\n    mean_intensity = numpy.mean(result[mask])\n    result[mask] -= mean_intensity\n    return result\n","33":"def linear_costes(self, fi, si, scale_max=255):\n    \"\"\"\n        Finds the Costes Automatic Threshold for colocalization using a linear algorithm.\n        Candiate thresholds are gradually decreased until Pearson R falls below 0.\n        If \"Fast\" mode is enabled the \"steps\" between tested thresholds will be increased\n        when Pearson R is much greater than 0.\n        \"\"\"\n    i_step = 1 \/ scale_max\n    non_zero = (fi > 0) | (si > 0)\n    xvar = numpy.var(fi[non_zero], axis=0, ddof=1)\n    yvar = numpy.var(si[non_zero], axis=0, ddof=1)\n    xmean = numpy.mean(fi[non_zero], axis=0)\n    ymean = numpy.mean(si[non_zero], axis=0)\n    z = fi[non_zero] + si[non_zero]\n    zvar = numpy.var(z, axis=0, ddof=1)\n    covar = 0.5 * (zvar - (xvar + yvar))\n    denom = 2 * covar\n    num = yvar - xvar + numpy.sqrt((yvar - xvar) * (yvar - xvar) + 4 * (\n        covar * covar))\n    a = num \/ denom\n    b = ymean - a * xmean\n    img_max = max(fi.max(), si.max())\n    i = i_step * (img_max \/\/ i_step + 1)\n    num_true = None\n    fi_max = fi.max()\n    si_max = si.max()\n    costReg, _ = scipy.stats.pearsonr(fi, si)\n    thr_fi_c = i\n    thr_si_c = a * i + b\n    while i > fi_max and a * i + b > si_max:\n        i -= i_step\n    while i > i_step:\n        thr_fi_c = i\n        thr_si_c = a * i + b\n        combt = (fi < thr_fi_c) | (si < thr_si_c)\n        try:\n            if (positives := numpy.count_nonzero(combt)) != num_true:\n                costReg, _ = scipy.stats.pearsonr(fi[combt], si[combt])\n                num_true = positives\n            if costReg <= 0:\n                break\n            elif self.fast_costes.value == M_ACCURATE or i < i_step * 10:\n                i -= i_step\n            elif costReg > 0.45:\n                i -= i_step * 10\n            elif costReg > 0.35:\n                i -= i_step * 5\n            elif costReg > 0.25:\n                i -= i_step * 2\n            else:\n                i -= i_step\n        except ValueError:\n            break\n    return thr_fi_c, thr_si_c\n","34":"def get_categories(self, pipeline, object_name):\n    \"\"\"Return the categories supported by this module for the given object\n\n        object_name - name of the measured object or IMAGE\n        \"\"\"\n    if object_name == 'Image' and self.wants_images(\n        ) or object_name != 'Image' and self.wants_objects(\n        ) and object_name in self.objects_list.value:\n        return ['Correlation']\n    return []\n","35":"def v_factors(xcol, ymatr):\n    \"\"\"xcol is (Nobservations,1) column vector of grouping values\n           (in terms of dose curve it may be Dose).\n       ymatr is (Nobservations, Nmeasures) matrix, where rows correspond to\n           observations and columns corresponds to different measures.\n\n        Calculate the V factor = 1-6 * mean standard deviation \/ range\n    \"\"\"\n    xs, avers, stds = loc_shrink_mean_std(xcol, ymatr)\n    vrange = numpy.max(avers, 0) - numpy.min(avers, 0)\n    vstd = numpy.zeros(len(vrange))\n    vstd[vrange == 0] = 1\n    vstd[vrange != 0] = numpy.mean(stds[:, vrange != 0], 0)\n    vrange[vrange == 0] = 1e-06\n    v = 1 - 6 * (vstd \/ vrange)\n    return v\n","36":"def loc_shrink_mean_std(xcol, ymatr):\n    \"\"\"Compute mean and standard deviation per label\n\n    xcol - column of image labels or doses\n    ymatr - a matrix with rows of values per image and columns\n            representing different measurements\n\n    returns xs - a vector of unique doses\n            avers - the average value per label\n            stds - the standard deviation per label\n    \"\"\"\n    ncols = ymatr.shape[1]\n    labels, labnum, xs = loc_vector_labels(xcol)\n    avers = numpy.zeros((labnum, ncols))\n    stds = avers.copy()\n    for ilab in range(labnum):\n        labinds = labels == ilab\n        labmatr = ymatr[labinds, :]\n        if labmatr.shape[0] == 1:\n            avers[ilab, :] = labmatr[0, :]\n        else:\n            avers[ilab, :] = numpy.mean(labmatr, 0)\n            stds[ilab, :] = numpy.std(labmatr, 0)\n    return xs, avers, stds\n","37":"def calculate_ec50(conc, responses, Logarithmic):\n    \"\"\"EC50 Function to fit a dose-response data to a 4 parameter dose-response\n       curve.\n\n       Inputs: 1. a 1 dimensional array of drug concentrations\n               2. the corresponding m x n array of responses\n       Algorithm: generate a set of initial coefficients including the Hill\n                  coefficient\n                  fit the data to the 4 parameter dose-response curve using\n                  nonlinear least squares\n       Output: a matrix of the 4 parameters\n               results[m,1]=min\n               results[m,2]=max\n               results[m,3]=ec50\n               results[m,4]=Hill coefficient\n\n       Original Matlab code Copyright 2004 Carlos Evangelista\n       send comments to CCEvangelista@aol.com\n       \"\"\"\n    if Logarithmic:\n        conc = numpy.log(conc)\n    n = responses.shape[1]\n    results = numpy.zeros((n, 4))\n\n    def error_fn(v, x, y):\n        \"\"\"Least-squares error function\n\n        This measures the least-squares error of fitting the sigmoid\n        with parameters in v to the x and y data.\n        \"\"\"\n        return numpy.sum((sigmoid(v, x) - y) ** 2)\n    for i in range(n):\n        response = responses[:, i]\n        v0 = calc_init_params(conc, response)\n        v = scipy.optimize.fmin(error_fn, v0, args=(conc, response),\n            maxiter=1000, maxfun=1000, disp=False)\n        results[i, :] = v\n    return results\n","38":"def calc_init_params(x, y):\n    \"\"\"This generates the min, max, x value at the mid-y value, and Hill\n      coefficient. These values are starting points for the sigmoid fitting.\n\n      x & y are the points to be fit\n      returns minimum, maximum, ec50 and hill coefficient starting points\n      \"\"\"\n    min_0 = min(y)\n    max_0 = max(y)\n    YvalueAt50thPercentile = (min(y) + max(y)) \/ 2\n    DistanceToCentralYValue = numpy.abs(y - YvalueAt50thPercentile)\n    LocationOfNearest = numpy.argmin(DistanceToCentralYValue)\n    XvalueAt50thPercentile = x[LocationOfNearest]\n    if XvalueAt50thPercentile == min(x) or XvalueAt50thPercentile == max(x):\n        ec50 = (min(x) + max(x)) \/ 2\n    else:\n        ec50 = XvalueAt50thPercentile\n    min_idx = numpy.argmin(x)\n    max_idx = numpy.argmax(x)\n    x0 = x[min_idx]\n    x1 = x[max_idx]\n    y0 = y[min_idx]\n    y1 = y[max_idx]\n    if x0 == x1:\n        raise ValueError(\n            \"All doses or labels for all image sets are %s. Can't calculate dose-response curves.\"\n             % x0)\n    elif y1 > y0:\n        hillc = -1\n    else:\n        hillc = 1\n    return min_0, max_0, ec50, hillc\n","39":"def get_bit_depth(self):\n    if self.save_image_or_figure in (IF_IMAGE, IF_MOVIE\n        ) and self.get_file_format() in (FF_TIFF, FF_H5):\n        return self.bit_depth.value\n    else:\n        return BIT_DEPTH_8\n","40":"def gradient_image(pixels, gradient_choice, automatic_smoothing, scale):\n    if automatic_smoothing:\n        fft = numpy.fft.fft2(pixels)\n        power2 = numpy.sqrt((fft * fft.conjugate()).real)\n        mode = numpy.argwhere(power2 == power2.max())[0]\n        scale = numpy.sqrt(numpy.sum((mode + 0.5) ** 2))\n    gradient_magnitude = scipy.ndimage.gaussian_gradient_magnitude(pixels,\n        scale)\n    if gradient_choice == GRADIENT_MAGNITUDE:\n        gradient_image = gradient_magnitude\n    else:\n        x = scipy.ndimage.correlate1d(gradient_magnitude, [-1, 0, 1], 1)\n        y = scipy.ndimage.correlate1d(gradient_magnitude, [-1, 0, 1], 0)\n        norm = numpy.sqrt(x ** 2 + y ** 2)\n        if gradient_choice == GRADIENT_DIRECTION_X:\n            gradient_image = 0.5 + x \/ norm \/ 2\n        else:\n            gradient_image = 0.5 + y \/ norm \/ 2\n    return gradient_image\n","41":"def planewise_morphology_closing(x_data, structuring_element):\n    y_data = numpy.zeros_like(x_data)\n    for index, plane in enumerate(x_data):\n        y_data[index] = skimage.morphology.closing(plane, structuring_element)\n    return y_data\n","42":"def get_measurements(self, pipeline, object_name, category):\n    if object_name == IMAGE and category == C_METADATA:\n        return [FTR_SITE, FTR_ROW, FTR_COLUMN, FTR_WELL, FTR_PLATE]\n    return []\n","43":"def resize(data, size):\n    return scipy.ndimage.zoom(data, numpy.divide(numpy.multiply(1.0, size),\n        data.shape), order=0, mode='nearest')\n","44":"def get_maxima(self, image, labeled_image, maxima_mask, image_resize_factor):\n    if image_resize_factor < 1.0:\n        shape = numpy.array(image.shape) * image_resize_factor\n        i_j = numpy.mgrid[0:shape[0], 0:shape[1]].astype(float\n            ) \/ image_resize_factor\n        resized_image = scipy.ndimage.map_coordinates(image, i_j)\n        resized_labels = scipy.ndimage.map_coordinates(labeled_image, i_j,\n            order=0).astype(labeled_image.dtype)\n    else:\n        resized_image = image\n        resized_labels = labeled_image\n    if maxima_mask is not None:\n        binary_maxima_image = centrosome.cpmorphology.is_local_maximum(\n            resized_image, resized_labels, maxima_mask)\n        binary_maxima_image[resized_image <= 0] = 0\n    else:\n        binary_maxima_image = (resized_image > 0) & (labeled_image > 0)\n    if image_resize_factor < 1.0:\n        inverse_resize_factor = float(image.shape[0]) \/ float(\n            binary_maxima_image.shape[0])\n        i_j = numpy.mgrid[0:image.shape[0], 0:image.shape[1]].astype(float\n            ) \/ inverse_resize_factor\n        binary_maxima_image = scipy.ndimage.map_coordinates(binary_maxima_image\n            .astype(float), i_j) > 0.5\n        assert binary_maxima_image.shape[0] == image.shape[0]\n        assert binary_maxima_image.shape[1] == image.shape[1]\n    shrunk_image = centrosome.cpmorphology.binary_shrink(binary_maxima_image)\n    return shrunk_image\n","45":"def filter_on_size(self, labeled_image, object_count):\n    \"\"\" Filter the labeled image based on the size range\n\n        labeled_image - pixel image labels\n        object_count - # of objects in the labeled image\n        returns the labeled image, and the labeled image with the\n        small objects removed\n        \"\"\"\n    if self.exclude_size.value and object_count > 0:\n        areas = scipy.ndimage.measurements.sum(numpy.ones(labeled_image.\n            shape), labeled_image, numpy.array(list(range(0, object_count +\n            1)), dtype=numpy.int32))\n        areas = numpy.array(areas, dtype=int)\n        min_allowed_area = numpy.pi * (self.size_range.min * self.\n            size_range.min) \/ 4\n        max_allowed_area = numpy.pi * (self.size_range.max * self.\n            size_range.max) \/ 4\n        area_image = areas[labeled_image]\n        labeled_image[area_image < min_allowed_area] = 0\n        small_removed_labels = labeled_image.copy()\n        labeled_image[area_image > max_allowed_area] = 0\n    else:\n        small_removed_labels = labeled_image.copy()\n    return labeled_image, small_removed_labels\n","46":"def planewise_morphology_opening(x_data, structuring_element):\n    y_data = numpy.zeros_like(x_data)\n    for index, plane in enumerate(x_data):\n        y_data[index] = skimage.morphology.opening(plane, structuring_element)\n    return y_data\n","47":"def _neighbors(image):\n    \"\"\"\n\n    Counts the neighbor pixels for each pixel of an image:\n\n            x = [\n                [0, 1, 0],\n                [1, 1, 1],\n                [0, 1, 0]\n            ]\n\n            _neighbors(x)\n\n            [\n                [0, 3, 0],\n                [3, 4, 3],\n                [0, 3, 0]\n            ]\n\n    :type image: numpy.ndarray\n\n    :param image: A two-or-three dimensional image\n\n    :return: neighbor pixels for each pixel of an image\n\n    \"\"\"\n    padding = numpy.pad(image, 1, 'constant')\n    mask = padding > 0\n    padding = padding.astype(float)\n    if image.ndim == 2:\n        response = 3 ** 2 * scipy.ndimage.uniform_filter(padding) - 1\n        labels = (response * mask)[1:-1, 1:-1]\n        return labels.astype(numpy.uint16)\n    elif image.ndim == 3:\n        response = 3 ** 3 * scipy.ndimage.uniform_filter(padding) - 1\n        labels = (response * mask)[1:-1, 1:-1, 1:-1]\n        return labels.astype(numpy.uint16)\n","48":"@property\ndef repeat_count(self):\n    \"\"\"\"\"\"\n    if self.repeats_choice == R_ONCE:\n        return 1\n    elif self.repeats_choice == R_FOREVER:\n        return 10000\n    elif self.repeats_choice == R_CUSTOM:\n        return self.custom_repeats.value\n    else:\n        raise ValueError('Unsupported repeat choice: %s' % self.\n            repeats_choice.value)\n    \"\"\"The thresholding algorithm to run\"\"\"\n    return self.threshold_method.value.split(' ')[0]\n","49":"def get_measurement_name(self, feature):\n    if self.distance_method == D_EXPAND:\n        scale = S_EXPANDED\n    elif self.distance_method == D_WITHIN:\n        scale = str(self.distance.value)\n    elif self.distance_method == D_ADJACENT:\n        scale = S_ADJACENT\n    if self.neighbors_are_objects:\n        return '_'.join((C_NEIGHBORS, feature, scale))\n    else:\n        return '_'.join((C_NEIGHBORS, feature, self.neighbors_name.value,\n            scale))\n","50":"@staticmethod\ndef find_adjacent_one(img1, numbering1, img2, numbering2, oi, oj):\n    \"\"\"Find correlated pairs of foreground points at given offsets\n\n        img1, img2 - binary images to be correlated\n        numbering1, numbering2 - indexes to be returned for pairs\n        oi, oj - offset for second image\n\n        returns two vectors: index in first and index in second\n        \"\"\"\n    i1, i2 = IdentifyDeadWorms.get_slices(oi)\n    j1, j2 = IdentifyDeadWorms.get_slices(oj)\n    match = img1[i1, j1] & img2[i2, j2]\n    return numbering1[i1, j1][match], numbering2[i2, j2][match]\n","51":"def get_feature_name_matrix(self):\n    \"\"\"Get a 2x2 matrix of feature names for two measurements\"\"\"\n    if self.wants_custom_names:\n        return numpy.array([[self.low_low_custom_name.value, self.\n            low_high_custom_name.value], [self.high_low_custom_name.value,\n            self.high_high_custom_name.value]])\n    else:\n        m1 = self.first_measurement.value\n        m2 = self.second_measurement.value\n        return numpy.array([['_'.join((m1, a1, m2, a2)) for a2 in ('low',\n            'high')] for a1 in ('low', 'high')])\n","52":"def fill_holes(image, diameter):\n    radius = diameter \/ 2.0\n    if image.dtype.kind == 'f':\n        image = skimage.img_as_bool(image)\n    if image.ndim == 2 or image.shape[-1] in (3, 4):\n        factor = radius ** 2\n    else:\n        factor = 4.0 \/ 3.0 * radius ** 3\n    size = numpy.pi * factor\n    return skimage.morphology.remove_small_holes(image, size)\n","53":"@property\ndef threshold_scale(self):\n    \"\"\"The \"scale\" for the threshold = minor parameterizations\"\"\"\n    threshold_algorithm = self.threshold_algorithm\n    if threshold_algorithm == centrosome.threshold.TM_OTSU:\n        if self.two_class_otsu == O_TWO_CLASS:\n            scale = '2'\n        else:\n            scale = '3'\n            if self.assign_middle_to_foreground == O_FOREGROUND:\n                scale += 'F'\n            else:\n                scale += 'B'\n        if self.use_weighted_variance == O_WEIGHTED_VARIANCE:\n            scale += 'W'\n        else:\n            scale += 'S'\n        return scale\n    elif threshold_algorithm == centrosome.threshold.TM_MOG:\n        return str(int(self.object_fraction.value * 100))\n","54":"def compute_rand_index(self, test_labels, ground_truth_labels, mask):\n    \"\"\"Calculate the Rand Index\n\n        http:\/\/en.wikipedia.org\/wiki\/Rand_index\n\n        Given a set of N elements and two partitions of that set, X and Y\n\n        A = the number of pairs of elements in S that are in the same set in\n            X and in the same set in Y\n        B = the number of pairs of elements in S that are in different sets\n            in X and different sets in Y\n        C = the number of pairs of elements in S that are in the same set in\n            X and different sets in Y\n        D = the number of pairs of elements in S that are in different sets\n            in X and the same set in Y\n\n        The rand index is:   A + B\n                             -----\n                            A+B+C+D\n\n\n        The adjusted rand index is the rand index adjusted for chance\n        so as not to penalize situations with many segmentations.\n\n        Jorge M. Santos, Mark Embrechts, \"On the Use of the Adjusted Rand\n        Index as a Metric for Evaluating Supervised Classification\",\n        Lecture Notes in Computer Science,\n        Springer, Vol. 5769, pp. 175-184, 2009. Eqn # 6\n\n        ExpectedIndex = best possible score\n\n        ExpectedIndex = sum(N_i choose 2) * sum(N_j choose 2)\n\n        MaxIndex = worst possible score = 1\/2 (sum(N_i choose 2) + sum(N_j choose 2)) * total\n\n        A * total - ExpectedIndex\n        -------------------------\n        MaxIndex - ExpectedIndex\n\n        returns a tuple of the Rand Index and the adjusted Rand Index\n        \"\"\"\n    ground_truth_labels = ground_truth_labels[mask].astype(numpy.uint32)\n    test_labels = test_labels[mask].astype(numpy.uint32)\n    if len(test_labels) > 0:\n        N_ij = scipy.sparse.coo_matrix((numpy.ones(len(test_labels)), (\n            ground_truth_labels, test_labels))).toarray()\n\n        def choose2(x):\n            \"\"\"Compute # of pairs of x things = x * (x-1) \/ 2\"\"\"\n            return x * (x - 1) \/ 2\n        A = numpy.sum(choose2(N_ij))\n        N_i = numpy.sum(N_ij, 1)\n        N_j = numpy.sum(N_ij, 0)\n        C = numpy.sum((N_i[:, numpy.newaxis] - N_ij) * N_ij) \/ 2\n        D = numpy.sum((N_j[numpy.newaxis, :] - N_ij) * N_ij) \/ 2\n        total = choose2(len(test_labels))\n        B = total - A - C - D\n        rand_index = (A + B) \/ total\n        expected_index = numpy.sum(choose2(N_i)) * numpy.sum(choose2(N_j))\n        max_index = (numpy.sum(choose2(N_i)) + numpy.sum(choose2(N_j))\n            ) * total \/ 2\n        adjusted_rand_index = (A * total - expected_index) \/ (max_index -\n            expected_index)\n    else:\n        rand_index = adjusted_rand_index = numpy.nan\n    return rand_index, adjusted_rand_index\n","55":"def channels_and_image_names(self):\n    \"\"\"Return tuples of channel indexes and the image names for output\"\"\"\n    if self.rgb_or_channels == CH_RGB:\n        rgb = (self.use_red.value, self.red_name.value, 'Red'), (self.\n            use_green.value, self.green_name.value, 'Green'), (self.\n            use_blue.value, self.blue_name.value, 'Blue')\n        return [(i, name, title) for i, (use_it, name, title) in enumerate(\n            rgb) if use_it]\n    if self.rgb_or_channels == CH_HSV:\n        hsv = (self.use_hue.value, self.hue_name.value, 'Hue'), (self.\n            use_saturation.value, self.saturation_name.value, 'Saturation'), (\n            self.use_value.value, self.value_name.value, 'Value')\n        return [(i, name, title) for i, (use_it, name, title) in enumerate(\n            hsv) if use_it]\n    result = []\n    for channel in self.channels:\n        choice = channel.channel_choice.value\n        channel_idx = self.get_channel_idx_from_choice(choice)\n        if channel_idx < len(self.channel_names):\n            channel_name = self.channel_names[channel_idx]\n        else:\n            channel_name = 'Channel: ' + str(choice)\n        result.append((channel_idx, channel.image_name.value, channel_name))\n    return result\n","56":"@property\ndef wants_well_tables(self):\n    \"\"\"Return true if user wants any well tables\"\"\"\n    if self.db_type == DB_SQLITE:\n        return False\n    else:\n        return (self.wants_agg_mean_well or self.wants_agg_median_well or\n            self.wants_agg_std_dev_well)\n","57":"def ignore_object(self, object_name, strict=False):\n    \"\"\"Ignore objects (other than 'Image') if this returns true\n\n        If strict is True, then we ignore objects based on the object selection\n        \"\"\"\n    if object_name in (EXPERIMENT, NEIGHBORS):\n        return True\n    if strict and self.objects_choice == O_NONE:\n        return True\n    if strict and self.objects_choice == O_SELECT and object_name != 'Image':\n        return object_name not in self.objects_list.selections\n    return False\n","58":"def random_number_generator(seed):\n    \"\"\"This is a very repeatable pseudorandom number generator\n\n    seed - a string to seed the generator\n\n    yields integers in the range 0-65535 on iteration\n    \"\"\"\n    m = hashlib.md5()\n    m.update(seed.encode())\n    while True:\n        digest = m.digest()\n        m.update(digest)\n        yield digest[0] + 256 * digest[1]\n","59":"def convert_to_objects(data, cast_to_bool, preserve_label, background,\n    connectivity):\n    connectivity = None if connectivity == 0 else connectivity\n    caster = skimage.img_as_bool if cast_to_bool else skimage.img_as_uint\n    data = caster(data)\n    if preserve_label and not cast_to_bool:\n        return data\n    return skimage.measure.label(data, background=background, connectivity=\n        connectivity)\n","60":"def help_settings(self):\n    return [self.x_name, self.y_name, self.mode, self.filter_choice, self.\n        per_object_assignment, self.rules_directory, self.rules_file_name,\n        self.rules_class, self.keep_removed_objects, self.\n        removed_objects_name, self.enclosing_object_name, self.\n        additional_object_button, self.allow_fuzzy]\n","61":"def flood(self, i, at, a, b, c, d, z):\n    z[i] = at\n    if a[i] != -1 and z[a[i]] == 0:\n        z = self.flood(a[i], at, a, b, c, d, z)\n    if b[i] != -1 and z[b[i]] == 0:\n        z = self.flood(b[i], at, a, b, c, d, z)\n    if c[i] != -1 and z[c[i]] == 0:\n        z = self.flood(c[i], at, a, b, c, d, z)\n    if c[i] != -1 and z[c[i]] == 0:\n        z = self.flood(c[i], at, a, b, c, d, z)\n    return z\n","62":"def upgrade_settings(self, setting_values, variable_revision_number,\n    module_name):\n    if variable_revision_number == 1:\n        setting_values = setting_values + ['100', '100']\n        variable_revision_number = 2\n    if variable_revision_number == 2:\n        setting_values = setting_values + ['40', '40', '40', '50', '50',\n            '50', '5']\n        variable_revision_number = 3\n    if variable_revision_number == 3:\n        setting_values = setting_values[:7] + [M_BOTH, '3', '2,10'\n            ] + setting_values[9:]\n        variable_revision_number = 4\n    if variable_revision_number == 4:\n        setting_values = setting_values + ['No', 'Yes', '1', 'No', '100']\n        variable_revision_number = 5\n    if variable_revision_number == 5:\n        setting_values = setting_values + ['80', '40']\n        variable_revision_number = 6\n    if variable_revision_number == 6:\n        setting_values = setting_values + [30.0, False, 15.0, 25.0]\n        variable_revision_number = 7\n    return setting_values, variable_revision_number\n","63":"def upgrade_settings(self, setting_values, variable_revision_number,\n    module_name):\n    if variable_revision_number == 1:\n        setting_values += ['No', 'RelabeledNucleiOutlines']\n        variable_revision_number = 2\n    if variable_revision_number == 1:\n        setting_values += [UNIFY_DISTANCE, 'None']\n        variable_revision_number = 3\n    if variable_revision_number == 3:\n        setting_values = setting_values + [UM_DISCONNECTED]\n        variable_revision_number = 4\n    if variable_revision_number == 4:\n        setting_values = setting_values[:8] + setting_values[10:]\n        variable_revision_number = 5\n    if variable_revision_number == 5:\n        if setting_values[2] == 'Unify':\n            setting_values[2] = 'Merge'\n        variable_revision_number = 6\n    return setting_values, variable_revision_number\n","64":"def copy_labels(labels, segmented):\n    \"\"\"Carry differences between orig_segmented and new_segmented into \"labels\"\n\n    labels - labels matrix similarly segmented to \"segmented\"\n    segmented - the newly numbered labels matrix (a subset of pixels are labeled)\n    \"\"\"\n    max_labels = len(numpy.unique(segmented))\n    seglabel = scipy.ndimage.minimum(labels, segmented, numpy.arange(1, \n        max_labels + 1))\n    labels_new = labels.copy()\n    labels_new[segmented != 0] = seglabel[segmented[segmented != 0] - 1]\n    return labels_new\n","65":"def fast_selection(self, costs, path_segment_matrix, segment_lengths,\n    overlap_weight, leftover_weight, max_num_worms):\n    \"\"\"Select the best subset of paths using a breadth-first search\n\n        costs - the shape costs of every path\n\n        path_segment_matrix - an N x M matrix where N are the segments\n        and M are the paths. A cell is true if a path includes the segment\n\n        segment_lengths - the length of each segment\n\n        overlap_weight - the penalty per pixel of an overlap\n\n        leftover_weight - the penalty per pixel of an excluded segment\n\n        max_num_worms - maximum # of worms allowed in returned match.\n        \"\"\"\n    current_best_subset = []\n    current_best_cost = numpy.sum(segment_lengths) * leftover_weight\n    current_costs = costs\n    current_path_segment_matrix = path_segment_matrix.astype(int)\n    current_path_choices = numpy.eye(len(costs), dtype=bool)\n    for i in range(min(max_num_worms, len(costs))):\n        (current_best_subset, current_best_cost,\n            current_path_segment_matrix, current_path_choices) = (self.\n            select_one_level(costs, path_segment_matrix, segment_lengths,\n            current_best_subset, current_best_cost,\n            current_path_segment_matrix, current_path_choices,\n            overlap_weight, leftover_weight))\n        if numpy.prod(current_path_choices.shape) == 0:\n            break\n    return current_best_subset, current_best_cost\n","66":"def search_recur(self, path_segment_matrix, segment_lengths, path_raw_costs,\n    overlap_weight, leftover_weight, current_subset, last_chosen,\n    current_cost, current_segment_coverings, current_best_subset,\n    current_best_cost, branching_factors, current_level):\n    \"\"\"Perform a recursive depth-first search on sets of paths\n\n        Perform a depth-first search recursively,  keeping the best (so far)\n        found subset of paths in current_best_subset, current_cost.\n\n        path_segment_matrix, segment_lengths, path_raw_costs, overlap_weight,\n        leftover_weight, branching_factor are essentially static.\n\n        current_subset is the currently considered subset, as an array of\n        indices, each index corresponding to a path in path_segment_matrix.\n\n        To avoid picking out the same subset twice, we insist that in all\n        subsets, indices are listed in increasing order.\n\n        Note that the shape cost term and the overlap cost term need not be\n        re-calculated each time, but can be calculated incrementally, as more\n        paths are added to the subset in consideration. Thus, current_cost holds\n        the sum of the shape cost and overlap cost terms for current_subset.\n\n        current_segments_coverings, meanwhile, is a logical array of length equal\n        to the number of segments in the graph, keeping track of the segments\n        covered by paths in current_subset.\"\"\"\n    this_cost = current_cost + leftover_weight * numpy.sum(segment_lengths[\n        ~current_segment_coverings])\n    if this_cost < current_best_cost:\n        current_best_cost = this_cost\n        current_best_subset = current_subset\n    if current_level < len(branching_factors):\n        this_branch_factor = branching_factors[current_level]\n    else:\n        this_branch_factor = branching_factors[-1]\n    current_overlapped_costs = path_raw_costs[last_chosen:] + numpy.sum(\n        current_segment_coverings[:, numpy.newaxis] * segment_lengths[:,\n        numpy.newaxis] * path_segment_matrix[:, last_chosen:], 0\n        ) * overlap_weight\n    order = numpy.lexsort([current_overlapped_costs])\n    order = order[numpy.arange(len(order)) + 1 < this_branch_factor]\n    for index in order:\n        new_cost = current_cost + current_overlapped_costs[index]\n        if new_cost >= current_best_cost:\n            break\n        path_index = last_chosen + index\n        current_best_subset, current_best_cost = self.search_recur(\n            path_segment_matrix, segment_lengths, path_raw_costs,\n            overlap_weight, leftover_weight, current_subset + [path_index],\n            path_index, new_cost, current_segment_coverings |\n            path_segment_matrix[:, path_index], current_best_subset,\n            current_best_cost, branching_factors, current_level + 1)\n    return current_best_subset, current_best_cost\n","67":"def angle_features(self):\n    \"\"\"Return a list of angle feature names\"\"\"\n    try:\n        return ['_'.join((F_ANGLE, str(n))) for n in range(1, self.\n            ncontrol_points() - 1)]\n    except:\n        LOGGER.error(\n            'Failed to get # of control points from training file. Unknown number of angle measurements'\n            , exc_info=True)\n        return []\n","68":"@on_array()\ndef biweight_location(a, initial=None, c=6.0, epsilon=0.001, max_iter=5):\n    \"\"\"Compute the biweight location for an array.\n\n    The biweight is a robust statistic for estimating the central location of a\n    distribution.\n    \"\"\"\n\n    def biloc_iter(a, initial):\n        d = a - initial\n        mad = np.median(np.abs(d))\n        w = d \/ max(c * mad, epsilon)\n        w = (1 - w ** 2) ** 2\n        mask = w < 1\n        weightsum = w[mask].sum()\n        if weightsum == 0:\n            return initial\n        return initial + (d[mask] * w[mask]).sum() \/ weightsum\n    if initial is None:\n        initial = np.median(a)\n    for _i in range(max_iter):\n        result = biloc_iter(a, initial)\n        if abs(result - initial) <= epsilon:\n            break\n        initial = result\n    return result\n","69":"@on_array()\ndef modal_location(a):\n    \"\"\"Return the modal value of an array's values.\n\n    The \"mode\" is the location of peak density among the values, estimated using\n    a Gaussian kernel density estimator.\n\n    Parameters\n    ----------\n    a : np.array\n        A 1-D array of floating-point values, e.g. bin log2 ratio values.\n    \"\"\"\n    sarr = np.sort(a)\n    kde = stats.gaussian_kde(sarr)\n    y = kde.evaluate(sarr)\n    peak = sarr[y.argmax()]\n    return peak\n","70":"@on_weighted_array()\ndef weighted_median(a, weights):\n    \"\"\"Weighted median of a 1-D numeric array.\"\"\"\n    order = a.argsort()\n    a = a[order]\n    weights = weights[order]\n    midpoint = 0.5 * weights.sum()\n    if (weights > midpoint).any():\n        return a[weights.argmax()]\n    cumulative_weight = weights.cumsum()\n    midpoint_idx = cumulative_weight.searchsorted(midpoint)\n    if midpoint_idx > 0 and cumulative_weight[midpoint_idx - 1\n        ] - midpoint < sys.float_info.epsilon:\n        return a[midpoint_idx - 1:midpoint_idx + 1].mean()\n    return a[midpoint_idx]\n","71":"@on_array(0)\ndef biweight_midvariance(a, initial=None, c=9.0, epsilon=0.001):\n    \"\"\"Compute the biweight midvariance for an array.\n\n    The biweight midvariance is a robust statistic for determining the\n    midvariance (i.e. the standard deviation) of a distribution.\n\n    See:\n\n    - https:\/\/en.wikipedia.org\/wiki\/Robust_measures_of_scale#The_biweight_midvariance\n    - https:\/\/astropy.readthedocs.io\/en\/latest\/_modules\/astropy\/stats\/funcs.html\n    \"\"\"\n    if initial is None:\n        initial = biweight_location(a)\n    d = a - initial\n    mad = np.median(np.abs(d))\n    w = d \/ max(c * mad, epsilon)\n    mask = np.abs(w) < 1\n    if w[mask].sum() == 0:\n        return mad * 1.4826\n    n = mask.sum()\n    d_ = d[mask]\n    w_ = (w ** 2)[mask]\n    return np.sqrt(n * (d_ ** 2 * (1 - w_) ** 4).sum() \/ ((1 - w_) * (1 - 5 *\n        w_)).sum() ** 2)\n","72":"def SegmentByPeaks(data, peaks, weights=None):\n    \"\"\"Average the values of the probes within each segment.\n\n    Parameters\n    ----------\n    data : array\n        the probe array values\n    peaks : array\n        Positions of copy number breakpoints in the original array\n\n    Source: SegmentByPeaks.R\n    \"\"\"\n    segs = np.zeros_like(data)\n    for seg_start, seg_end in zip(np.insert(peaks, 0, 0), np.append(peaks,\n        len(data))):\n        if weights is not None and weights[seg_start:seg_end].sum() > 0:\n            val = np.average(data[seg_start:seg_end], weights=weights[\n                seg_start:seg_end])\n        else:\n            val = np.mean(data[seg_start:seg_end])\n        segs[seg_start:seg_end] = val\n    return segs\n","73":"def HaarConv(signal, weight, stepHalfSize):\n    \"\"\"Convolve haar wavelet function with a signal, applying circular padding.\n\n    Parameters\n    ----------\n    signal : const array of floats\n    weight : const array of floats (optional)\n    stepHalfSize : int\n\n    Returns\n    -------\n    array\n        Of floats, representing the convolved signal.\n\n    Source: HaarSeg.c\n    \"\"\"\n    signalSize = len(signal)\n    if stepHalfSize > signalSize:\n        logging.debug('Error?: stepHalfSize (%s) > signalSize (%s)',\n            stepHalfSize, signalSize)\n        return np.zeros(signalSize, dtype=np.float_)\n    result = np.zeros(signalSize, dtype=np.float_)\n    if weight is not None:\n        highWeightSum = weight[:stepHalfSize].sum()\n        highNonNormed = (weight[:stepHalfSize] * signal[:stepHalfSize]).sum()\n        lowWeightSum = highWeightSum\n        lowNonNormed = -highNonNormed\n    for k in range(1, signalSize):\n        highEnd = k + stepHalfSize - 1\n        if highEnd >= signalSize:\n            highEnd = signalSize - 1 - (highEnd - signalSize)\n        lowEnd = k - stepHalfSize - 1\n        if lowEnd < 0:\n            lowEnd = -lowEnd - 1\n        if weight is None:\n            result[k] = result[k - 1] + signal[highEnd] + signal[lowEnd\n                ] - 2 * signal[k - 1]\n        else:\n            lowNonNormed += signal[lowEnd] * weight[lowEnd] - signal[k - 1\n                ] * weight[k - 1]\n            highNonNormed += signal[highEnd] * weight[highEnd] - signal[k - 1\n                ] * weight[k - 1]\n            lowWeightSum += weight[k - 1] - weight[lowEnd]\n            highWeightSum += weight[highEnd] - weight[k - 1]\n            result[k] = math.sqrt(stepHalfSize \/ 2) * (lowNonNormed \/\n                lowWeightSum + highNonNormed \/ highWeightSum)\n    if weight is None:\n        stepNorm = math.sqrt(2.0 * stepHalfSize)\n        result[1:signalSize] \/= stepNorm\n    return result\n","74":"def FindLocalPeaks(signal):\n    \"\"\"Find local maxima on positive values, local minima on negative values.\n\n    First and last index are never considered extramum.\n\n    Parameters\n    ----------\n    signal : const array of floats\n\n    Returns\n    -------\n    peakLoc : array of ints\n        Locations of extrema in `signal`\n\n    Source: HaarSeg.c\n    \"\"\"\n    maxSuspect = minSuspect = None\n    peakLoc = []\n    for k in range(1, len(signal) - 1):\n        sig_prev, sig_curr, sig_next = signal[k - 1:k + 2]\n        if sig_curr > 0:\n            if sig_curr > sig_prev and sig_curr > sig_next:\n                peakLoc.append(k)\n            elif sig_curr > sig_prev and sig_curr == sig_next:\n                maxSuspect = k\n            elif sig_curr == sig_prev and sig_curr > sig_next:\n                if maxSuspect is not None:\n                    peakLoc.append(maxSuspect)\n                    maxSuspect = None\n            elif sig_curr == sig_prev and sig_curr < sig_next:\n                maxSuspect = None\n        elif sig_curr < 0:\n            if sig_curr < sig_prev and sig_curr < sig_next:\n                peakLoc.append(k)\n            elif sig_curr < sig_prev and sig_curr == sig_next:\n                minSuspect = k\n            elif sig_curr == sig_prev and sig_curr < sig_next:\n                if minSuspect is not None:\n                    peakLoc.append(minSuspect)\n                    minSuspect = None\n            elif sig_curr == sig_prev and sig_curr > sig_next:\n                minSuspect = None\n    return np.array(peakLoc, dtype=np.int_)\n","75":"def UnifyLevels(baseLevel, addonLevel, windowSize):\n    \"\"\"Unify several decomposition levels.\n\n    Merge the two lists of breakpoints, but drop addonLevel values that are too\n    close to baseLevel values.\n\n    Parameters\n    ----------\n    baseLevel : const array of ints\n    addonLevel : const array of ints\n    windowSize : int\n\n    Returns\n    -------\n    joinedLevel : array of ints\n\n    Source: HaarSeg.c\n    \"\"\"\n    if not len(addonLevel):\n        return baseLevel\n    joinedLevel = []\n    addon_idx = 0\n    for base_elem in baseLevel:\n        while addon_idx < len(addonLevel):\n            addon_elem = addonLevel[addon_idx]\n            if addon_elem < base_elem - windowSize:\n                joinedLevel.append(addon_elem)\n                addon_idx += 1\n            elif base_elem - windowSize <= addon_elem <= base_elem + windowSize:\n                addon_idx += 1\n            else:\n                assert base_elem + windowSize < addon_elem\n                break\n        joinedLevel.append(base_elem)\n    last_pos = baseLevel[-1] + windowSize if len(baseLevel) else -1\n    while addon_idx < len(addonLevel) and addonLevel[addon_idx] <= last_pos:\n        addon_idx += 1\n    if addon_idx < len(addonLevel):\n        joinedLevel.extend(addonLevel[addon_idx:])\n    return np.array(sorted(joinedLevel), dtype=np.int_)\n","76":"def AdjustBreaks(signal, peakLoc):\n    \"\"\"Improve localization of breaks. Suboptimal, but linear-complexity.\n\n    We try to move each break 1 sample left\/right, choosing the offset which\n    leads to minimum data error.\n\n    Parameters\n    ----------\n    signal: const array of floats\n    peakLoc: const array of ints\n\n    Source: HaarSeg.c\n    \"\"\"\n    newPeakLoc = peakLoc.copy()\n    for k, npl_k in enumerate(newPeakLoc):\n        n1 = npl_k if k == 0 else npl_k - newPeakLoc[k - 1]\n        n2 = (len(signal) if k + 1 == len(newPeakLoc) else newPeakLoc[k + 1]\n            ) - npl_k\n        bestScore = float('Inf')\n        bestOffset = 0\n        for p in (-1, 0, 1):\n            if n1 == 1 and p == -1 or n2 == 1 and p == 1:\n                continue\n            signal_n1_to_p = signal[npl_k - n1:npl_k + p]\n            s1 = signal_n1_to_p.sum() \/ (n1 + p)\n            ss1 = ((signal_n1_to_p - s1) ** 2).sum()\n            signal_p_to_n2 = signal[npl_k + p:npl_k + n2]\n            s2 = signal_p_to_n2.sum() \/ (n2 - p)\n            ss2 = ((signal_p_to_n2 - s2) ** 2).sum()\n            score = ss1 + ss2\n            if score < bestScore:\n                bestScore = score\n                bestOffset = p\n        if bestOffset != 0:\n            newPeakLoc[k] += bestOffset\n    return newPeakLoc\n","77":"def table2coords(seg_table):\n    \"\"\"Return x, y arrays for plotting.\"\"\"\n    x = []\n    y = []\n    for start, size, val in seg_table:\n        x.append(start)\n        x.append(start + size)\n        y.append(val)\n        y.append(val)\n    return x, y\n","78":"def _mirrored_baf(vals, above_half=None):\n    shift = (vals - 0.5).abs()\n    if above_half is None:\n        above_half = vals.median() > 0.5\n    if above_half:\n        return 0.5 + shift\n    return 0.5 - shift\n","79":"def fbase(fname):\n    \"\"\"Strip directory and all extensions from a filename.\"\"\"\n    base = os.path.basename(fname)\n    if base.endswith('.gz'):\n        base = base[:-3]\n    known_multipart_exts = ('.antitargetcoverage.cnn',\n        '.targetcoverage.cnn', '.antitargetcoverage.csv',\n        '.targetcoverage.csv', '.recal.bam', '.deduplicated.realign.bam')\n    for ext in known_multipart_exts:\n        if base.endswith(ext):\n            base = base[:-len(ext)]\n            break\n    else:\n        base = base.rsplit('.', 1)[0]\n    return base\n","80":"def detect_bedcov_columns(text):\n    \"\"\"Determine which 'bedcov' output columns to keep.\n\n    Format is the input BED plus a final appended column with the count of\n    basepairs mapped within each row's region. The input BED might have 3\n    columns (regions without names), 4 (named regions), or more (arbitrary\n    columns after 'gene').\n    \"\"\"\n    firstline = text[:text.index('\\n')]\n    tabcount = firstline.count('\\t')\n    if tabcount < 3:\n        raise RuntimeError(f'Bad line from bedcov:\\n{firstline!r}')\n    if tabcount == 3:\n        return ['chromosome', 'start', 'end', 'basecount']\n    if tabcount == 4:\n        return ['chromosome', 'start', 'end', 'gene', 'basecount']\n    fillers = [f'_{i}' for i in range(1, tabcount - 3)]\n    return ['chromosome', 'start', 'end', 'gene'] + fillers + ['basecount']\n","81":"def unpipe_name(name):\n    \"\"\"Fix the duplicated gene names Picard spits out.\n\n    Return a string containing the single gene name, sans duplications and pipe\n    characters.\n\n    Picard CalculateHsMetrics combines the labels of overlapping intervals\n    by joining all labels with '|', e.g. 'BRAF|BRAF' -- no two distinct\n    targeted genes actually overlap, though, so these dupes are redundant.\n    Meaningless target names are dropped, e.g. 'CGH|FOO|-' resolves as 'FOO'.\n    In case of ambiguity, the longest name is taken, e.g. \"TERT|TERT Promoter\"\n    resolves as \"TERT Promoter\".\n    \"\"\"\n    if '|' not in name:\n        return name\n    gene_names = set(name.split('|'))\n    if len(gene_names) == 1:\n        return gene_names.pop()\n    cleaned_names = gene_names.difference(params.IGNORE_GENE_NAMES)\n    if cleaned_names:\n        gene_names = cleaned_names\n    new_name = sorted(gene_names, key=len, reverse=True)[0]\n    if len(gene_names) > 1:\n        logging.warning('WARNING: Ambiguous gene name %r; using %r', name,\n            new_name)\n    return new_name\n","82":"def parse_theta_results(fname):\n    \"\"\"Parse THetA results into a data structure.\n\n    Columns: NLL, mu, C, p*\n    \"\"\"\n    with open(fname) as handle:\n        header = next(handle).rstrip().split('\\t')\n        body = next(handle).rstrip().split('\\t')\n        assert len(body) == len(header) == 4\n        nll = float(body[0])\n        mu = body[1].split(',')\n        mu_normal = float(mu[0])\n        mu_tumors = list(map(float, mu[1:]))\n        copies = body[2].split(':')\n        if len(mu_tumors) == 1:\n            copies = [[(int(c) if c.isdigit() else None) for c in copies]]\n        else:\n            copies = [[(int(c) if c.isdigit() else None) for c in subcop] for\n                subcop in zip(*[c.split(',') for c in copies])]\n        probs = body[3].split(',')\n        if len(mu_tumors) == 1:\n            probs = [(float(p) if not p.isalpha() else None) for p in probs]\n        else:\n            probs = [[(float(p) if not p.isalpha() else None) for p in\n                subprob] for subprob in zip(*[p.split(',') for p in probs])]\n    return {'NLL': nll, 'mu_normal': mu_normal, 'mu_tumors': mu_tumors, 'C':\n        copies, 'p*': probs}\n","83":"def kmeans(samples, k=None):\n    from scipy.cluster import vq\n    if k is None:\n        from math import log\n        k = max(1, int(round(log(len(samples), 3))))\n    print('Clustering', len(samples), 'samples by k-means, where k =', k)\n    obs = pca_sk(samples, 3)\n    obs = vq.whiten(obs)\n    _centroids, labels = vq.kmeans2(obs, k, minit='++')\n    from collections import defaultdict\n    clusters = defaultdict(list)\n    for idx, label in enumerate(labels):\n        clusters[label].append(idx)\n    clusters = clusters.values()\n    return clusters\n","84":"def markov(samples, inflation=5, max_iterations=100, by_pca=True):\n    \"\"\"Markov-cluster control samples by their read depths' correlation.\n\n    Each of the matrices in the resulting iterable (list) can be processed the\n    same as the input to calculate average log2 and spread values for that\n    cluster.\n\n    Parameters\n    ----------\n    samples : array\n        Matrix of samples' read depths or normalized log2 values, as columns.\n    inflation : float\n        Inflation parameter for MCL. Must be >1; higher more granular clusters.\n    by_pca : bool\n        If true, similarity is by PCA; otherwise, by Pearson correlation.\n\n    Return\n    ------\n    results : list\n        A list of matrices representing non-overlapping column-subsets of the\n        input, where each set of samples represents a cluster.\n    \"\"\"\n    if inflation <= 1:\n        raise ValueError('inflation must be > 1')\n    if by_pca:\n        pca_matrix = pca_sk(samples, 2)\n        from scipy.spatial import distance\n        dists = distance.squareform(distance.pdist(pca_matrix))\n        M = 1 - dists \/ dists.max()\n    else:\n        M = np.corrcoef(samples)\n    M, clusters = mcl(M, max_iterations, inflation)\n    return clusters\n","85":"def inflate(A, inflation):\n    \"\"\"Apply cluster inflation with the given element-wise exponent.\n\n    From the mcl manual:\n\n    This value is the main handle for affecting cluster granularity.\n    This parameter is the usually only one that may require tuning.\n\n    By default it is set to 2.0 and this is a good way to start. If you want to\n    explore cluster structure in graphs with MCL, vary this parameter to obtain\n    clusterings at different levels of granularity.  It is usually chosen\n    somewhere in the range [1.2-5.0]. -I 5.0 will tend to result in fine-grained\n    clusterings, and -I 1.2 will tend to result in very coarse grained\n    clusterings. A good set of starting values is 1.4, 2, 4, and 6.\n    Your mileage will vary depending on the characteristics of your data.\n\n    Low values for -I, like -I 1.2, will use more CPU\/RAM resources.\n\n    Use mcl's cluster validation tools 'clm dist' and 'clm info' to test the\n    quality and coherency of your clusterings.\n    \"\"\"\n    return normalize(np.power(A, inflation))\n","86":"def pca_sk(data, n_components=None):\n    \"\"\"Principal component analysis using scikit-learn.\n\n    Parameters\n    ----------\n    data : 2D NumPy array\n    n_components : int\n\n    Returns: PCA-transformed data with `n_components` columns.\n    \"\"\"\n    from sklearn.decomposition import PCA\n    return PCA(n_components=n_components).fit_transform(data)\n","87":"def mask_bad_bins(cnarr):\n    \"\"\"Flag the bins with excessively low or inconsistent coverage.\n\n    Returns\n    -------\n    np.array\n        A boolean array where True indicates bins that failed the checks.\n    \"\"\"\n    mask = (cnarr['log2'] < params.MIN_REF_COVERAGE) | (cnarr['log2'] > -\n        params.MIN_REF_COVERAGE) | (cnarr['spread'] > params.MAX_REF_SPREAD)\n    if 'depth' in cnarr:\n        mask |= cnarr['depth'] == 0\n    if 'gc' in cnarr:\n        assert params.GC_MIN_FRACTION >= 0 and params.GC_MIN_FRACTION <= 1\n        assert params.GC_MAX_FRACTION >= 0 and params.GC_MAX_FRACTION <= 1\n        lower_gc_bound = min(params.GC_MIN_FRACTION, params.GC_MAX_FRACTION)\n        upper_gc_bound = max(params.GC_MIN_FRACTION, params.GC_MAX_FRACTION)\n        mask |= (cnarr['gc'] > upper_gc_bound) | (cnarr['gc'] < lower_gc_bound)\n    return mask\n","88":"def edge_losses(target_sizes, insert_size):\n    \"\"\"Calculate coverage losses at the edges of baited regions.\n\n    Letting i = insert size and t = target size, the proportional loss of\n    coverage near the two edges of the baited region (combined) is:\n\n    .. math :: i\/2t\n\n    If the \"shoulders\" extend outside the bait $(t < i), reduce by:\n\n    .. math :: (i-t)^2 \/ 4it\n\n    on each side, or (i-t)^2 \/ 2it total.\n    \"\"\"\n    losses = insert_size \/ (2 * target_sizes)\n    small_mask = target_sizes < insert_size\n    t_small = target_sizes[small_mask]\n    losses[small_mask] -= (insert_size - t_small) ** 2 \/ (2 * insert_size *\n        t_small)\n    return losses\n","89":"def summarize_info(all_logr, all_depths):\n    \"\"\"Average & spread of log2ratios and depths for a group of samples.\n\n    Can apply to all samples, or a given cluster of samples.\n    \"\"\"\n    logging.info('Calculating average bin coverages')\n    cvg_centers = np.apply_along_axis(descriptives.biweight_location, 0,\n        all_logr)\n    depth_centers = np.apply_along_axis(descriptives.biweight_location, 0,\n        all_depths)\n    logging.info('Calculating bin spreads')\n    spreads = np.array([descriptives.biweight_midvariance(a, initial=i) for\n        a, i in zip(all_logr.T, cvg_centers)])\n    result = {'log2': cvg_centers, 'depth': depth_centers, 'spread': spreads}\n    return result\n","90":"def create_clusters(logr_matrix, min_cluster_size, sample_ids):\n    \"\"\"Extract and summarize clusters of samples in logr_matrix.\n\n    1. Calculate correlation coefficients between all samples (columns).\n    2. Cluster the correlation matrix.\n    3. For each resulting sample cluster (down to a minimum size threshold),\n       calculate the central log2 value for each bin, similar to the full pool.\n       Also print the sample IDs in each cluster, if feasible.\n\n    Also recalculate and store the 'spread' of each cluster, though this might\n    not be necessary\/good.\n\n    Return a DataFrame of just the log2 values. Column names are ``log2_i``\n    where i=1,2,... .\n    \"\"\"\n    from .cluster import kmeans\n    logr_matrix = logr_matrix[1:, :]\n    print('Clustering', len(logr_matrix), 'samples...')\n    clusters = kmeans(logr_matrix)\n    cluster_cols = {}\n    sample_ids = np.array(sample_ids)\n    for i, clust_idx in enumerate(clusters):\n        i += 1\n        if len(clust_idx) < min_cluster_size:\n            logging.info('Skipping cluster #%d, size %d < min. %d', i, len(\n                clust_idx), min_cluster_size)\n            continue\n        logging.info('Summarizing cluster #%d of %d samples', i, len(clust_idx)\n            )\n        samples = sample_ids[clust_idx]\n        logging.info('\\n'.join([('\\t' + s) for s in samples]))\n        clust_matrix = logr_matrix[clust_idx, :]\n        clust_info = summarize_info(clust_matrix, [])\n        cluster_cols.update({f'log2_{i}': clust_info['log2'], f'spread_{i}':\n            clust_info['spread']})\n    return cluster_cols\n","91":"def shorten_labels(gene_labels):\n    \"\"\"Reduce multi-accession interval labels to the minimum consistent.\n\n    So: BED or interval_list files have a label for every region. We want this\n    to be a short, unique string, like the gene name. But if an interval list is\n    instead a series of accessions, including additional accessions for\n    sub-regions of the gene, we can extract a single accession that covers the\n    maximum number of consecutive regions that share this accession.\n\n    e.g.::\n\n        ...\n        mRNA|JX093079,ens|ENST00000342066,mRNA|JX093077,ref|SAMD11,mRNA|AF161376,mRNA|JX093104\n        ens|ENST00000483767,mRNA|AF161376,ccds|CCDS3.1,ref|NOC2L\n        ...\n\n    becomes::\n\n        ...\n        mRNA|AF161376\n        mRNA|AF161376\n        ...\n    \"\"\"\n    longest_name_len = 0\n    curr_names = set()\n    curr_gene_count = 0\n    for label in gene_labels:\n        next_names = set(label.rstrip().split(','))\n        assert len(next_names)\n        overlap = curr_names.intersection(next_names)\n        if overlap:\n            curr_names = filter_names(overlap)\n            curr_gene_count += 1\n        else:\n            for _i in range(curr_gene_count):\n                out_name = shortest_name(curr_names)\n                yield out_name\n                longest_name_len = max(longest_name_len, len(out_name))\n            curr_gene_count = 1\n            curr_names = next_names\n    for _i in range(curr_gene_count):\n        out_name = shortest_name(curr_names)\n        yield out_name\n        longest_name_len = max(longest_name_len, len(out_name))\n    logging.info('Longest name length: %d', longest_name_len)\n","92":"def convolve_weighted(window, signal, weights, n_iter=1):\n    \"\"\"Convolve a weighted window over a weighted signal array.\n\n    Source: https:\/\/stackoverflow.com\/a\/46232913\/10049\n    \"\"\"\n    assert len(weights) == len(signal\n        ), f'len(weights) = {len(weights)}, len(signal) = {len(signal)}, window_size = {len(window)}'\n    y, w = signal, weights\n    window \/= window.sum()\n    for _i in range(n_iter):\n        logging.debug('Iteration %d: len(y)=%d, len(w)=%d', _i, len(y), len(w))\n        D = np.convolve(w * y, window, mode='same')\n        N = np.convolve(w, window, mode='same')\n        y = D \/ N\n        w = np.convolve(window, w, mode='same')\n    return y, w\n","93":"def savgol(x, total_width=None, weights=None, window_width=7, order=3, n_iter=1\n    ):\n    \"\"\"Savitzky-Golay smoothing.\n\n    Fitted polynomial order is typically much less than half the window width.\n\n    `total_width` overrides `n_iter`.\n    \"\"\"\n    if len(x) < 2:\n        return x\n    if total_width is None:\n        total_width = n_iter * window_width\n    if weights is None:\n        x, total_wing, signal = check_inputs(x, total_width, False)\n    else:\n        x, total_wing, signal, weights = check_inputs(x, total_width, False,\n            weights)\n    total_width = 2 * total_wing + 1\n    window_width = min(window_width, total_width)\n    order = min(order, window_width \/\/ 2)\n    n_iter = max(1, min(1000, total_width \/\/ window_width))\n    logging.debug(\n        'Smoothing in %s iterations with window width %s and order %s for effective bandwidth %s'\n        , n_iter, window_width, order, total_width)\n    if weights is None:\n        y = signal\n        for _i in range(n_iter):\n            y = savgol_filter(y, window_width, order, mode='interp')\n    else:\n        window = savgol_coeffs(window_width, order)\n        y, _w = convolve_weighted(window, signal, weights, n_iter)\n    bad_idx = (y > x.max()) | (y < x.min())\n    if bad_idx.any():\n        logging.warning(\n            'Smoothing overshot at %s \/ %s indices: (%s, %s) vs. original (%s, %s)'\n            , bad_idx.sum(), len(bad_idx), y.min(), y.max(), x.min(), x.max())\n    return y[total_wing:-total_wing]\n","94":"def outlier_iqr(a, c=3.0):\n    \"\"\"Detect outliers as a multiple of the IQR from the median.\n\n    By convention, \"outliers\" are points more than 1.5 * IQR from the median,\n    and \"extremes\" or extreme outliers are those more than 3.0 * IQR.\n    \"\"\"\n    a = np.asarray(a)\n    dists = np.abs(a - np.median(a))\n    iqr = descriptives.interquartile_range(a)\n    return dists > c * iqr\n","95":"def outlier_mad_median(a):\n    \"\"\"MAD-Median rule for detecting outliers.\n\n    X_i is an outlier if::\n\n         | X_i - M |\n        _____________  > K ~= 2.24\n\n         MAD \/ 0.6745\n\n    where $K = sqrt( X^2_{0.975,1} )$,\n    the square root of the 0.975 quantile of a chi-squared distribution with 1\n    degree of freedom.\n\n    This is a very robust rule with the highest possible breakdown point of 0.5.\n\n    Returns\n    -------\n    np.array\n        A boolean array of the same size as `a`, where outlier indices are True.\n\n    References\n    ----------\n    - Davies & Gather (1993) The Identification of Multiple Outliers.\n    - Rand R. Wilcox (2012) Introduction to robust estimation and hypothesis\n      testing. Ch.3: Estimating measures of location and scale.\n    \"\"\"\n    K = 2.24\n    a = np.asarray(a)\n    dists = np.abs(a - np.median(a))\n    mad = descriptives.median_absolute_deviation(a)\n    return dists \/ mad > K\n","96":"def rolling_outlier_quantile(x, width, q, m):\n    \"\"\"Detect outliers by multiples of a quantile in a window.\n\n    Outliers are the array elements outside `m` times the `q`'th\n    quantile of deviations from the smoothed trend line, as calculated from\n    the trend line residuals. (For example, take the magnitude of the 95th\n    quantile times 5, and mark any elements greater than that value as\n    outliers.)\n\n    This is the smoothing method used in BIC-seq (doi:10.1073\/pnas.1110574108)\n    with the parameters width=200, q=.95, m=5 for WGS.\n\n    Returns\n    -------\n    np.array\n        A boolean array of the same size as `x`, where outlier indices are True.\n    \"\"\"\n    if len(x) <= width:\n        return np.zeros(len(x), dtype=np.bool_)\n    dists = np.abs(x - savgol(x, width))\n    quants = rolling_quantile(dists, width, q)\n    outliers = dists > quants * m\n    return outliers\n","97":"def rolling_outlier_std(x, width, stdevs):\n    \"\"\"Detect outliers by stdev within a rolling window.\n\n    Outliers are the array elements outside `stdevs` standard deviations from\n    the smoothed trend line, as calculated from the trend line residuals.\n\n    Returns\n    -------\n    np.array\n        A boolean array of the same size as `x`, where outlier indices are True.\n    \"\"\"\n    if len(x) <= width:\n        return np.zeros(len(x), dtype=np.bool_)\n    dists = x - savgol(x, width)\n    x_std = rolling_std(dists, width)\n    outliers = np.abs(dists) > x_std * stdevs\n    return outliers\n","98":"def absolute_clonal(cnarr, ploidy, purity, is_reference_male, is_sample_female\n    ):\n    \"\"\"Calculate absolute copy number values from segment or bin log2 ratios.\"\"\"\n    absolutes = np.zeros(len(cnarr), dtype=np.float_)\n    for i, row in enumerate(cnarr):\n        ref_copies, expect_copies = _reference_expect_copies(row.chromosome,\n            ploidy, is_sample_female, is_reference_male)\n        absolutes[i] = _log2_ratio_to_absolute(row.log2, ref_copies,\n            expect_copies, purity)\n    return absolutes\n","99":"def absolute_reference(cnarr, ploidy, is_reference_male):\n    \"\"\"Absolute integer number of reference copies in each bin.\n\n    I.e. the given ploidy for autosomes, 1 or 2 X according to the reference\n    sex, and always 1 copy of Y.\n    \"\"\"\n    ref_copies = np.repeat(ploidy, len(cnarr))\n    is_x = (cnarr.chromosome == cnarr._chr_x_label).values\n    is_y = (cnarr.chromosome == cnarr._chr_y_label).values\n    if is_reference_male:\n        ref_copies[is_x] = ploidy \/\/ 2\n    ref_copies[is_y] = ploidy \/\/ 2\n    return ref_copies\n","100":"def _reference_expect_copies(chrom, ploidy, is_sample_female, is_reference_male\n    ):\n    \"\"\"Determine the number copies of a chromosome expected and in reference.\n\n    For sex chromosomes, these values may not be the same ploidy as the\n    autosomes. The \"reference\" number is the chromosome's ploidy in the\n    CNVkit reference, while \"expect\" is the chromosome's neutral ploidy in the\n    given sample, based on the specified sex of each. E.g., given a female\n    sample and a male reference, on chromosome X the \"reference\" value is 1 but\n    \"expect\" is 2.\n\n    Returns\n    -------\n    tuple\n        A pair of integers: number of copies in the reference, and expected in\n        the sample.\n    \"\"\"\n    chrom = chrom.lower()\n    if chrom in ['chrx', 'x']:\n        ref_copies = ploidy \/\/ 2 if is_reference_male else ploidy\n        exp_copies = ploidy if is_sample_female else ploidy \/\/ 2\n    elif chrom in ['chry', 'y']:\n        ref_copies = ploidy \/\/ 2\n        exp_copies = 0 if is_sample_female else ploidy \/\/ 2\n    else:\n        ref_copies = exp_copies = ploidy\n    return ref_copies, exp_copies\n","101":"def _reference_copies_pure(chrom, ploidy, is_reference_male):\n    \"\"\"Determine the reference number of chromosome copies (pure sample).\n\n    Returns\n    -------\n    int\n        Number of copies in the reference.\n    \"\"\"\n    chrom = chrom.lower()\n    if chrom in ['chry', 'y'] or is_reference_male and chrom in ['chrx', 'x']:\n        ref_copies = ploidy \/\/ 2\n    else:\n        ref_copies = ploidy\n    return ref_copies\n","102":"def _log2_ratio_to_absolute(log2_ratio, ref_copies, expect_copies, purity=None\n    ):\n    \"\"\"Transform a log2 ratio to absolute linear scale (for an impure sample).\n\n    Does not round to an integer absolute value here.\n\n    Math::\n\n        log2_ratio = log2(ncopies \/ ploidy)\n        2^log2_ratio = ncopies \/ ploidy\n        ncopies = ploidy * 2^log2_ratio\n\n    With rescaling for purity::\n\n        let v = log2 ratio value, p = tumor purity,\n            r = reference ploidy, x = expected ploidy,\n            n = tumor ploidy (\"ncopies\" above);\n\n        v = log_2(p*n\/r + (1-p)*x\/r)\n        2^v = p*n\/r + (1-p)*x\/r\n        n*p\/r = 2^v - (1-p)*x\/r\n        n = (r*2^v - x*(1-p)) \/ p\n\n    If purity adjustment is skipped (p=1; e.g. if germline or if scaling for\n    heterogeneity was done beforehand)::\n\n        n = r*2^v\n    \"\"\"\n    if purity and purity < 1.0:\n        ncopies = (ref_copies * 2 ** log2_ratio - expect_copies * (1 - purity)\n            ) \/ purity\n    else:\n        ncopies = _log2_ratio_to_absolute_pure(log2_ratio, ref_copies)\n    return ncopies\n","103":"def rescale_baf(purity, observed_baf, normal_baf=0.5):\n    \"\"\"Adjust B-allele frequencies for sample purity.\n\n    Math::\n\n        t_baf*purity + n_baf*(1-purity) = obs_baf\n        obs_baf - n_baf * (1-purity) = t_baf * purity\n        t_baf = (obs_baf - n_baf * (1-purity))\/purity\n    \"\"\"\n    tumor_baf = (observed_baf - normal_baf * (1 - purity)) \/ purity\n    return tumor_baf\n","104":"def z_prob(cnarr):\n    \"\"\"Calculate z-test p-value at each bin.\"\"\"\n    sd = np.sqrt(1 - cnarr['weight'])\n    z = cnarr['log2'] \/ sd\n    p = 2.0 * norm.cdf(-np.abs(z))\n    return p_adjust_bh(p)\n","105":"def p_adjust_bh(p):\n    \"\"\"Benjamini-Hochberg p-value correction for multiple hypothesis testing.\"\"\"\n    p = np.asfarray(p)\n    by_descend = p.argsort()[::-1]\n    by_orig = by_descend.argsort()\n    steps = float(len(p)) \/ np.arange(len(p), 0, -1)\n    q = np.minimum(1, np.minimum.accumulate(steps * p[by_descend]))\n    return q[by_orig]\n","106":"@property\ndef _chr_x_label(self):\n    if 'chr_x' in self.meta:\n        return self.meta['chr_x']\n    if len(self):\n        chr_x = 'chrX' if self.chromosome.iat[0].startswith('chr') else 'X'\n        self.meta['chr_x'] = chr_x\n        return chr_x\n    return ''\n","107":"def to_fasta(self):\n    \"\"\"Returns a string with the sequence in fasta format\n\n        Returns\n        -------\n        str\n            The FASTA representation of the sequence\n        \"\"\"\n    prefix, suffix = re.split('(?<=size=)\\\\w+', self.label, maxsplit=1)\n    new_count = int(round(self.frequency))\n    new_label = '%s%d%s' % (prefix, new_count, suffix)\n    return '>%s\\n%s\\n' % (new_label, self.sequence)\n","108":"def sample_id_from_read_id(readid):\n    \"\"\"Get SampleID from the split_libraries_fastq.py output\n    fasta file read header\n\n    Parameters\n    ----------\n    readid : str\n        the fasta file read name\n\n    Returns\n    -------\n    sampleid : str\n        the sample id\n    \"\"\"\n    sampleread = readid.split(' ')[0]\n    sampleid = sampleread.rsplit('_', 1)[0]\n    return sampleid\n","109":"def _calc_overlap(x, y, seed):\n    \"\"\"Return an overlapping position between a pair of k-mers.\n\n    \"\"\"\n    if not x or not y:\n        return 0\n    for m in re.finditer(y[:seed], x):\n        overlap = seed\n        p = m.end()\n        if len(x) == p:\n            return overlap\n        tail = re.search(x[p:], y)\n        if not tail:\n            continue\n        if tail.start() == seed:\n            return tail.end()\n    return 0\n","110":"def filter_kmers(kmers, kmer_len, rate):\n    \"\"\"Return a clean set of k-mers in tuple.\n\n       Filter low-complexity and low-frequency kmers.\n    \"\"\"\n    low_comp = [re.compile(base * (kmer_len \/\/ 2)) for base in 'ACGTN']\n    i, x = -1, -1\n    while x != len(low_comp):\n        i += 1\n        x = sum([(not p.findall(kmers[i][0])) for p in low_comp])\n    max_hits = kmers[i][1]\n    clean = []\n    total = 0\n    for s, n in kmers[i:]:\n        if sum([(not p.findall(s)) for p in low_comp]) != len(low_comp):\n            continue\n        if float(max_hits) \/ n > rate:\n            break\n        clean.append((s, n))\n        total += n\n    return [(s, round(float(n) \/ total * 100, 4)) for s, n in clean]\n","111":"def assemble_kmers(kmers, seed):\n    \"\"\"Return assembled k-mers and the frequency in tuple.\n\n       Assemble given k-mers by checking suffix-prefix matches.\n    \"\"\"\n    pre_l, new_l = 0, len(kmers)\n    while pre_l != new_l:\n        pre_l = len(kmers)\n        for i in range(pre_l):\n            kmer, hits = kmers[i]\n            if not hits:\n                continue\n            max_o, max_j = 0, 0\n            for j in range(pre_l):\n                if i == j:\n                    continue\n                if kmers[j][0] in kmer:\n                    hits += kmers[j][1]\n                    kmers[i] = kmer, hits\n                    kmers[j] = '', 0\n                    continue\n                overlap = _calc_overlap(kmer, kmers[j][0], seed)\n                if overlap > max_o:\n                    max_o, max_j = overlap, j\n            if max_o > 0:\n                kmer += kmers[max_j][0][max_o:]\n                hits += kmers[max_j][1]\n                kmers[i] = kmer, hits\n                kmers[max_j] = '', 0\n        kmers = [k for k in kmers if k != ('', 0)]\n        new_l = len(kmers)\n    return kmers\n","112":"def count_mapped_read_sam(samout):\n    \"\"\"Return the number of mapped reads to the genome.\n\n    \"\"\"\n    if not os.path.exists(samout):\n        raise Exception(\"can't open SAM\")\n    mapped = set()\n    for x in fileinput.input(samout):\n        if not x or x.startswith('@'):\n            continue\n        x = x.rstrip().split('\\t')\n        if x[2] != '*':\n            mapped.add(x[0])\n    cnt = sum([int(n.split('_')[1]) for n in mapped])\n    return cnt\n","113":"def execute_subprocess(command, stdin_file_path=None, stdout_file_path=None):\n    stdin_file = None\n    stdin_msg = 'None'\n    if stdin_file_path:\n        stdin_file = open(stdin_file_path, 'r')\n        stdin_msg = stdin_file_path\n    stdout_file = None\n    stdout_msg = 'None'\n    if stdout_file_path:\n        stdout_file = open(stdout_file_path, 'w')\n        stdout_msg = stdout_file_path\n    print('Running command: ' + command + '; STDIN: ' + stdin_msg +\n        '; STDOUT: ' + stdout_msg)\n    return_code = subprocess.call(command, stdin=stdin_file, stdout=\n        stdout_file, stderr=subprocess.STDOUT, shell=True)\n    if stdin_file_path:\n        stdin_file.close()\n    if stdout_file_path:\n        stdout_file.close()\n    return return_code\n","114":"def read_text_lines(text_file_path):\n    text_lines = []\n    ifs = open(text_file_path, 'r')\n    for line in iter(ifs.readline, ''):\n        new_line = standardize_new_line_char(line)\n        text_lines.append(new_line)\n    ifs.close()\n    return text_lines\n","115":"def get_water_models_info(gmx_output_lines):\n    start_line = 0\n    while gmx_output_lines[start_line][0:7] != 'Opening':\n        start_line = start_line + 1\n    start_line = start_line + 1\n    end_line = start_line\n    while gmx_output_lines[end_line][0:7] != 'Opening' and gmx_output_lines[\n        end_line][0] != '\\n':\n        end_line = end_line + 1\n    waters_info = gmx_output_lines[start_line:end_line]\n    waters_info2 = []\n    number = 1\n    for water in waters_info:\n        waters_info2.append([number, water[:-1]])\n        number = number + 1\n    return waters_info2\n","116":"def parse_uniprot_pdbref_chains(chains_span_str):\n    \"\"\"\n    Examples of pdbref chains entries to be parsed:\n    A=65-119             => {'A':[65,119]}\n    A\/C\/E\/G=64-121       => {'A':[64,121], 'B':[64,121], 'C':[64,121], 'D':[64,121]}\n    A=458-778, B=764-778 => {'A':[458,778],'B':[764,778]}\n    \"\"\"\n    comma_sep = chains_span_str.split(',')\n    chains_span = {}\n    for s in comma_sep:\n        span = s.split('=')[1]\n        begin = int(span.split('-')[0])\n        end = int(span.split('-')[1])\n        chainids = s.split('=')[0].strip().split('\/')\n        for c in chainids:\n            chains_span[c] = [begin, end]\n    return chains_span\n","117":"def gen_uniprot_metadata(uniprot_query_string, uniprot_domain_regex):\n    metadata = {'method': 'UniProt', 'gather_from_uniprot': {\n        'uniprot_query_string': uniprot_query_string,\n        'uniprot_domain_regex': uniprot_domain_regex}}\n    return metadata\n","118":"def gen_pdb_metadata(pdbids, uniprot_domain_regex, chainids):\n    metadata = {'method': 'PDB', 'gather_from_pdb': {'pdbids': pdbids,\n        'uniprot_domain_regex': uniprot_domain_regex, 'chainids': chainids}}\n    return metadata\n","119":"def encode_url_query(uniprot_query):\n\n    def replace_all(text, replace_dict):\n        for i, j in replace_dict.iteritems():\n            text = text.replace(i, j)\n        return text\n    encoding_dict = {' ': '+', ':': '%3A', '(': '%28', ')': '%29', '\"':\n        '%22', '=': '%3D'}\n    return replace_all(uniprot_query, encoding_dict)\n","120":"def seqwrap(sequence, add_star=False):\n    \"\"\"\n    Wraps a sequence string to a width of 60.\n    If add_star is set to true, an asterisk will be added\n    to the end of the sequence, for compatibility with\n    Modeller.\n    \"\"\"\n    if add_star:\n        sequence += '*'\n    wrapped = ''\n    for i in range(0, len(sequence), 60):\n        wrapped += sequence[i:i + 60] + '\\n'\n    return wrapped\n","121":"def find_loopmodel_executable():\n    for path in os.environ['PATH'].split(os.pathsep):\n        if not os.path.exists(path):\n            continue\n        path = path.strip('\"')\n        for filename in os.listdir(path):\n            if len(filename) >= 10 and filename[0:10] == 'loopmodel.':\n                if filename[-5:] == 'debug':\n                    warnings.warn(\n                        'loopmodel debug version ({0}) will be ignored, as it runs extremely slowly'\n                        .format(filename))\n                    continue\n                return os.path.join(path, filename)\n    raise Exception('Loopmodel executable not found in PATH')\n","122":"def select_templates_by_validation_score(targetid, validation_score_cutoff=\n    None, validation_score_percentile=None):\n    \"\"\"\n    Parameters\n    ----------\n    targetid: str\n    validation_score_cutoff: float\n    validation_score_percentile: float\n\n    Returns\n    -------\n    selected_templateids: list of str\n    \"\"\"\n    validation_score_filenames = ['validation_scores_sorted-molprobity-{}'.\n        format(stagename) for stagename in modeling_stages]\n    for validation_score_filename in validation_score_filenames[::-1]:\n        validation_score_filepath = os.path.join(default_project_dirnames.\n            models, targetid, validation_score_filename)\n        if os.path.exists(validation_score_filepath):\n            break\n    with open(validation_score_filepath) as validation_score_file:\n        validation_score_lines_split = [line.split() for line in\n            validation_score_file.read().splitlines()]\n    templateids = np.array([i[0] for i in validation_score_lines_split])\n    validation_scores = np.array([float(i[1]) for i in\n        validation_score_lines_split])\n    if validation_score_cutoff:\n        selected_templateids = [str(x) for x in templateids[\n            validation_scores < validation_score_cutoff]]\n    elif validation_score_percentile:\n        percentile_index = len(templateids) - 1 - int((len(templateids) - 1\n            ) * (float(validation_score_percentile) \/ 100.0))\n        selected_templateids = [str(x) for x in templateids[:percentile_index]]\n    else:\n        selected_templateids = templateids\n    return selected_templateids\n","123":"def readFileContents(filename):\n    import os.path\n    if os.path.exists(filename):\n        infile = open(filename, 'r')\n    elif os.path.exists(filename + '.gz'):\n        import gzip\n        infile = gzip.open(filename + '.gz', 'r')\n    else:\n        raise IOError('File %s not found' % filename)\n    contents = infile.read()\n    infile.close()\n    return contents\n","124":"def Hmap_omm2gmx(resname, omm_atomname):\n    \"\"\"Converts OpenMM style amber99sb-ildn Hydrogen names to Gromacs style.\"\"\"\n    if omm_atomname in gmx_omm_Hname_mapping['openmm'][resname]:\n        atom_index = gmx_omm_Hname_mapping['openmm'][resname].index(\n            omm_atomname)\n        gmx_atomname = gmx_omm_Hname_mapping['gmx'][resname][atom_index]\n        return gmx_atomname\n    else:\n        return omm_atomname\n","125":"def Hmap_gmx2omm(resname, gmx_atomname):\n    \"\"\"Converts Gromacs style amber99sb-ildn Hydrogen names to OpenMM style.\"\"\"\n    if gmx_atomname in gmx_omm_Hname_mapping['gmx'][resname]:\n        atom_index = gmx_omm_Hname_mapping['gmx'][resname].index(gmx_atomname)\n        omm_atomname = gmx_omm_Hname_mapping['openmm'][resname][atom_index]\n        return omm_atomname\n    else:\n        return gmx_atomname\n","126":"def calculate_seq_identity(aln):\n    len_shorter_seq = min([len(aln[0][0].replace('-', '')), len(aln[0][1].\n        replace('-', ''))])\n    seq_id = 0\n    for r in range(len(aln[0][0])):\n        if aln[0][0][r] == aln[0][1][r]:\n            seq_id += 1\n    seq_id = 100 * float(seq_id) \/ float(len_shorter_seq)\n    return seq_id\n","127":"def parse_molprobity_oneline_analysis_output(output_text):\n    results_lines = []\n    for line in output_text.splitlines():\n        if len(line) == 0 or line[0] == '#':\n            continue\n        ncolons = line.count(':')\n        if ncolons == 32:\n            results_lines.append(line)\n    molprobity_results = {}\n    for results_line in results_lines:\n        results_line_split = results_line.split(':')\n        filename = results_line_split[0]\n        targetid = filename[:filename.find('.pdb')]\n        target_results = {}\n        for c, coltuple in enumerate(molprobity_oneline_analysis_colnames):\n            colname, coltype = coltuple\n            value = results_line_split[c]\n            try:\n                if coltype is not None:\n                    value = coltype(value)\n            except (ValueError, TypeError):\n                pass\n            target_results[colname] = value\n        molprobity_results[targetid] = target_results\n    return molprobity_results\n","128":"def load_read_counts(readCounts):\n    data = {}\n    indexes = {}\n    titles = []\n    for line in open(readCounts):\n        if line.startswith('Regions'):\n            idx = 0\n            for datafile in line.split('\\t')[1:]:\n                if datafile.strip():\n                    titles.append(datafile.strip())\n                    data[datafile.strip()] = {}\n                    indexes[idx] = datafile.strip()\n                    idx += 1\n        else:\n            for idx, binsline in enumerate(line.split('\\t')[1:]):\n                if binsline.strip():\n                    data[indexes[idx]][line.split('\\t')[0]] = [float(x) for\n                        x in binsline.split(';')]\n    return titles, data\n","129":"def count(self):\n    \"\"\"\n        Count total number of reads in file\n\n        Returns\n        -------\n        int\n            Number of reads\n        \"\"\"\n    if not self.rmdup and not self.rmrepeats:\n        try:\n            return self.track.mapped\n        except Exception:\n            pass\n    c = 0\n    for read in self.track:\n        if not self.rmdup or not read.flag & 1024:\n            if not self.rmrepeats or read.mapq > 0:\n                c += 1\n    return c\n","130":"def process_groups(groups):\n    if not groups:\n        return None\n    pg = []\n    for group in groups.split(','):\n        ids = [int(x) for x in group.split(':')]\n        if len(ids) == 2:\n            pg.append(list(range(ids[0], ids[1] + 1)))\n        else:\n            pg.append(ids)\n    return pg\n","131":"def normalize_data(data, percentile=75):\n    norm_data = {}\n    for track, ar in list(data.items()):\n        flat = ar.flatten()\n        s = scoreatpercentile(flat[~np.isnan(flat)], percentile)\n        if s == 0:\n            sys.stderr.write(\n                \"\"\"Error normalizing track {0} as score at percentile {1} is 0, normalizing to maximum value instead\n\"\"\"\n                .format(track, percentile))\n            x = ar \/ max(flat)\n        else:\n            x = ar \/ s\n            x[x >= 1.0] = 1\n        norm_data[track] = x\n    return norm_data\n","132":"def get_absolute_scale(scale, data, per_track=False):\n    try:\n        scale = float(scale)\n        return scale\n    except:\n        if type(scale) == type('') and scale.endswith('%'):\n            rel_scale = float(scale[:-1])\n            if per_track:\n                print('Hoe')\n                s = [scoreatpercentile(d, rel_scale) for d in data]\n                print(s)\n                return s\n            else:\n                d = np.array(data).flatten()\n                s = scoreatpercentile(d, rel_scale)\n                if s == 0:\n                    try:\n                        s = min(d[d > 0])\n                    except:\n                        s = 1.0\n                return s\n","133":"def mirror_clusters(data, labels, cutoff=0.01):\n    \"\"\"\n    Merge mirrored profiles based on a chi2 test of the mean profiles \n    Only if the profile is mirrored over all data tracks\n    Returns the labels of the two matched mirrored tracks, if there is at least one match with a p-value\n    greater than the cutoff.\n    If not, return (None, None)\n    \"\"\"\n    from functools import cmp_to_key\n    n = len(set(labels))\n    if n == 1:\n        return None, None\n    mirror = dict([(i, {}) for i in range(n)])\n    for track in list(data.keys()):\n        profiles = []\n        for i in range(n):\n            profiles.append(np.mean(data[track][labels == i], 0) + 1e-10)\n        for i in range(n - 1):\n            for j in range(i + 1, n):\n                p = chisquare(profiles[i], profiles[j][::-1])[1]\n                mirror[i].setdefault(j, []).append(p)\n    result = []\n    for i in list(mirror.keys()):\n        for j in list(mirror[i].keys()):\n            result.append([(i, j), mirror[i][j]])\n    key = cmp_to_key(lambda a, b: mycmp(np.mean(a[1]), np.mean(b[1])))\n    for (i, j), ps in sorted(result, key=key)[::-1]:\n        if (np.array(ps) >= cutoff).all():\n            return i, j\n    return None, None\n","134":"def cluster_profile(cluster_data, cluster_type='k', numclusters=3, dist=\n    'euclidean', random_state=None):\n    \"\"\"Cluster profiles for heatmap\n\n    Takes a matrix and clusters either with kmeans or hierarchical clustering.\n    Distance can be either euclidean or pearson. \n\n    Parameters\n    ----------\n    cluster_data :  array_like\n        Data to cluster.\n\n    cluster_type : str, optional\n        Either 'k' for kmeans, 'h' for hierarchical or 'n' for no clustering.\n        If cluster_type equals None, data is also not clustered.\n\n    numclusters : int, optional\n        Number of clusters.\n\n    dist : str, optional\n        Distance metric, either 'euclidean' or 'pearson'.\n\n    Returns\n    -------\n\n    ind : array\n        Indices of sorted input.\n\n    labels : array \n        Cluster labels.\n    \"\"\"\n    if dist not in ['euclidean', 'pearson']:\n        raise ValueError(\"distance can be either 'euclidean' or 'pearson'\")\n    if dist == 'pearson':\n        cluster_data = np.apply_along_axis(scale, 1, cluster_data)\n    if cluster_type == 'k':\n        print('K-means clustering')\n        k = KMeans(n_clusters=numclusters, random_state=random_state)\n        labels = k.fit(cluster_data).labels_\n        ind = labels.argsort()\n    elif cluster_type == 'h':\n        print('Hierarchical clustering')\n        a = AgglomerativeClustering(n_clusters=numclusters, linkage='complete')\n        a.fit(cluster_data)\n        labels = a.labels_\n        c = a.n_leaves_\n        t = {x: [x] for x in range(a.n_leaves_)}\n        for x in a.children_:\n            t[c] = t[x[0]] + t[x[1]]\n            c += 1\n        ind = t[c - 1]\n    elif cluster_type in ['n', None]:\n        ind = np.arange(len(cluster_data))\n        labels = np.zeros(len(cluster_data))\n    else:\n        raise ValueError('Invalid value for cluster_type')\n    return ind, labels\n","135":"def file_sorter(filename):\n    \"\"\"Helps in filename sorting.\n\n    Args:\n        filename (str): the name of the file to compare while sorting\n\n    Returns:\n        tuple: a tuple containing three elements: chromosome (int), start (int)\n               and end (int)\n\n    Using a regular expression, finds the chromosome along with the starting\n    and ending position of an imputed segment. The file\n    ``chr22.1_50000.impute2`` should return ``(22, 1, 50000)``.\n\n    \"\"\"\n    r = re.search('chr(\\\\d+)(_[12])?\\\\.(\\\\d+)_(\\\\d+)\\\\.impute2', filename)\n    return int(r.group(1)), int(r.group(3)), int(r.group(4))\n","136":"def read_bim(bim_fn, chromosomes=tuple()):\n    \"\"\"Reads a BIM file and extracts chromosomes.\n\n    Args:\n        bim_fn (str): the name of the BIM file\n        chromosomes (list): the list of chromosome to extract\n\n    Returns:\n        pandas.DataFrame: the BIM file content\n\n    \"\"\"\n    data = pd.read_csv(bim_fn, delim_whitespace=True, names=['chrom',\n        'name', 'cm', 'pos', 'a1', 'a2'])\n    if len(chromosomes) == 0:\n        return data\n    to_extract = data.chrom == chromosomes[0]\n    for chrom in chromosomes[1:]:\n        to_extract |= data.chrom == chrom\n    return data.loc[to_extract, :]\n","137":"def _format_time_columns(table, first_col):\n    \"\"\"Colorize the time in the table (columns 2 and up).\n\n    Args:\n        table (list): the data for the tabular\n        first_col (int): the first column containing time\n\n    Returns:\n        list: the same data, but with time column colorized\n\n    \"\"\"\n    for i in range(len(table)):\n        for j in range(first_col, len(table[i])):\n            table[i][j] = utils.colorize_time(table[i][j])\n    return table\n","138":"def sanitize_tex(original_text):\n    \"\"\"Sanitize TeX text.\n\n    Args:\n        original_text (str): the text to sanitize for LaTeX\n\n    Text is sanitized by following these steps:\n\n    1. Replaces ``\\\\\\\\`` by ``\\\\textbackslash``\n    2. Escapes certain characters (such as ``$``, ``%``, ``_``, ``}``, ``{``,\n       ``&`` and ``#``) by adding a backslash (*e.g.* from ``&`` to ``\\\\&``).\n    3. Replaces special characters such as ``~`` by the LaTeX equivalent\n       (*e.g.* from ``~`` to ``$\\\\sim$``).\n\n    \"\"\"\n    sanitized_tex = original_text.replace('\\\\', '\\\\textbackslash ')\n    sanitized_tex = re.sub('([{}])'.format(''.join(_escaped_char)),\n        '\\\\\\\\\\\\g<1>', sanitized_tex)\n    for character, mod in _char_mod.items():\n        sanitized_tex = sanitized_tex.replace(character, mod)\n    return sanitized_tex\n","139":"def wrap_tex(original_text):\n    \"\"\"Wraps the text.\n\n    Args:\n        original_text (str): the text to wrap\n\n    Returns:\n        str: a string where the original text was wrapped\n\n    Wraps the text so that lines are no longer than 80 characters. Uses the\n    :py:func:`str.join` function on the results of the :py:func:`wrap`\n    function, so that a single string is returned.\n\n    \"\"\"\n    return '\\n'.join(wrap(original_text))\n","140":"def format_tex(text, tex_format):\n    \"\"\"Change the TeX text format.\n\n    Args:\n        text (str): the text for which the format needs to be specified\n        tex_format (str): the format of the text to return\n\n    Returns:\n        str: the formatted text\n\n    This will change the format by adding the LaTeX format command (*e.g.* from\n    ``text`` to ``\\\\texttt{text}``).\n\n    Note\n    ----\n        Only the following format are available:\n\n        * ``texttt``\n        * ``emph``\n        * ``textbf``\n        * ``textit``\n\n    \"\"\"\n    assert tex_format in _valid_tex_formats, 'invalid format'\n    assert _is_sanitized(text), 'text not sanitized'\n    return '\\\\%s{%s}' % (tex_format, text)\n","141":"def tex_inline_math(content):\n    \"\"\"Creates an inline mathematical formula in TeX.\n\n    Args:\n        content (str): the content of the mathematical formula\n\n    Returns:\n        str: the formatted mathematical formula\n\n    The function only adds ``$`` symbols before and after the content (*e.g.*\n    from ``\\\\pi`` to ``$\\\\pi$``).\n\n    \"\"\"\n    return '${}$'.format(content)\n","142":"def _is_sanitized(text):\n    \"\"\"Check if text is sanitized.\n\n    Args:\n        text (str): the text to check\n\n    Returns:\n        bool: ``True`` if the text is sanitized, ``False`` otherwise\n\n    \"\"\"\n    sanitized = re.search('[^\\\\\\\\][{}]'.format(''.join(_escaped_char)), text)\n    sanitized = sanitized is None\n    for character in _char_mod.keys():\n        sanitized = sanitized and character not in text\n    return sanitized\n","143":"def read_samples(i_filename):\n    \"\"\"Reads the sample file (produced by SHAPEIT).\n\n    Args:\n        i_filename (str): the name of the input file\n\n    Returns:\n        pandas.DataFrame: the list of samples\n\n    This file contains the list of samples that are contained in the\n    ``impute2`` file (with same order). The expected format for this file is a\n    tab separated file with a first row containing the following columns: ::\n\n        ID_1\tID_2\tmissing\tfather\tmother\tsex\tplink_pheno\n\n    The subsequent row will be discarded and should contain: ::\n\n        0\t0\t0 D\tD\tD\tB\n\n    Notes\n    -----\n        We are mostly interested in the sample IDs corresponding to the\n        ``ID_2`` column. Their uniqueness is verified by pandas.\n\n    \"\"\"\n    samples = pd.read_csv(i_filename, sep=' ', usecols=[0, 1])\n    samples = samples.drop(samples.index[0], axis=0)\n    samples['ID_2'] = samples['ID_2'].astype(str)\n    return samples.set_index('ID_2', verify_integrity=True)\n","144":"def skat_read_snp_set(i_filename):\n    \"\"\"Reads the SKAT SNP set file.\n\n    Args:\n        i_filename (str): the name of the input file\n\n    Returns:\n        pandas.DataFrame: the SNP set for the SKAT analysis\n\n    This file has to be supplied by the user. The recognized columns are:\n    ``variant``, ``snp_set`` and ``weight``. The ``weight`` column is optional\n    and can be used to specify a custom weighting scheme for SKAT. If nothing\n    is specified, the default Beta weights are used.\n\n    The file has to be tab delimited.\n\n    \"\"\"\n    skat_info = pd.read_csv(i_filename, sep='\\t', header=0)\n    if 'variant' not in skat_info.columns:\n        raise GenipeError(\n            \"The SKAT SNP set file needs to have a 'variant' column containing the ID of every variant of interest.\"\n            )\n    if 'snp_set' not in skat_info.columns:\n        raise GenipeError(\n            \"The SKAT SNP set file needs to have a 'snp_set' column containing the SNP set ID for every variant. The user is free to choose the SNP ID\"\n            )\n    return skat_info\n","145":"def read_sites_to_extract(i_filename):\n    \"\"\"Reads the list of sites to extract.\n\n    Args:\n        i_filename (str): The input filename containing the IDs of the variants\n                          to consider for the analysis.\n\n    Returns:\n        set: A set containing the variants.\n\n    The expected file format is simply a list of variants. Every row should\n    correspond to a single variant identifier. ::\n\n        3:60069:t\n        rs123456:A\n        3:60322:A\n\n    Typically, this is used to analyze only variants that passed some QC\n    threshold. The :py:mod:`genipe` pipeline generates this file at the\n    'merge_impute2' step.\n\n    \"\"\"\n    markers_to_extract = None\n    with open(i_filename, 'r') as i_file:\n        markers_to_extract = set(i_file.read().splitlines())\n    return markers_to_extract\n","146":"def get_formula(phenotype, covars, interaction, gender_c, categorical):\n    \"\"\"Creates the linear\/logistic regression formula (for statsmodel).\n\n    Args:\n        phenotype (str): the phenotype column\n        covars (list): the list of co variable columns\n        interaction (str): the interaction column\n\n    Returns:\n        str: the formula for the statistical analysis\n\n    Note\n    ----\n        The phenotype column needs to be specified. The list of co variables\n        might be empty (if no co variables are necessary). The interaction\n        column can be set to ``None`` if there is no interaction.\n\n    Note\n    ----\n        The gender column should be categorical (hence, the formula requires\n        the gender to be included into ``C()``, *e.g.* ``C(Gender)``).\n\n    \"\"\"\n    formula = '{} ~ _GenoD'.format(phenotype)\n    for covar in covars:\n        if covar == gender_c or covar in categorical:\n            covar = 'C({})'.format(covar)\n        formula += ' + ' + covar\n    if interaction is not None:\n        if interaction == gender_c or interaction in categorical:\n            interaction = 'C({})'.format(interaction)\n        formula += ' + _GenoD*{}'.format(interaction)\n    return formula\n","147":"def get_samples(fn):\n    \"\"\"Reads the sample files, and extract the information.\n\n    Args:\n        fn (str): the name of the sample file\n\n    Returns:\n        pandas.DataFrame: the sample information\n\n    \"\"\"\n    sample = pd.read_csv(fn, sep=' ')\n    sample = sample.iloc[1:,].reset_index(drop=True)\n    return sample\n","148":"def get_file_prefix(fn):\n    \"\"\"Gets the filename prefix.\n\n    Args:\n        fn (str): the name of the file from which the prefix is required\n\n    Returns:\n        str: the prefix of the file\n\n    This function removes the extension from the file name, and return its\n    prefix (*e.g.* ``test.impute2`` returns ``test``, and\n    ``..\/test.impute2.gz`` returns ``..\/test``).\n\n    \"\"\"\n    prefix = os.path.splitext(fn)[0]\n    if prefix.endswith('impute2'):\n        prefix = os.path.splitext(prefix)[0]\n    return prefix\n","149":"def maf_dosage_from_probs(prob_matrix, a1, a2, scale=2, gender=None,\n    site_name=None):\n    \"\"\"Computes MAF and dosage vector from probs matrix.\n\n    Args:\n        prob_matrix (numpy.array): the probability matrix\n        a1 (str): the first allele\n        a2 (str): the second allele\n        scale (int): the scale value\n        gender (numpy.array): the gender of the samples\n        site_name (str): the name for this site\n\n    Returns:\n        tuple: a tuple containing four values: the dosage vector, the minor\n               allele frequency, the minor and the major allele.\n\n    When 'gender' is not None, we assume that the MAF on chromosome X is\n    required (hence, males count as 1, and females as 2 alleles). There is also\n    an Exception raised if there are any heterozygous males.\n\n    \"\"\"\n    maf = 'NA'\n    major, minor = a1, a2\n    if prob_matrix.shape[0] == 0:\n        return np.array([], dtype=float), maf, minor, major\n    dosage = dosage_from_probs(homo_probs=prob_matrix[:, 2], hetero_probs=\n        prob_matrix[:, 1], scale=scale)\n    set_no_maf = False\n    if gender is None:\n        maf = dosage.sum() \/ (len(dosage) * 2)\n    else:\n        m = gender == 1\n        f = gender == 2\n        males_nb_geno = np.bincount(np.argmax(prob_matrix[m], axis=1),\n            minlength=3)\n        if males_nb_geno[1] > 0:\n            raise GenipeError('{}: heterozygous male present'.format(site_name)\n                )\n        nb_alleles = m.sum() + f.sum() * 2\n        if nb_alleles == 0:\n            logging.warning('All samples have unknown gender, MAF will be NA')\n            maf = dosage.sum() \/ (len(dosage) * 2)\n            set_no_maf = True\n        else:\n            maf = (dosage[f].sum() + dosage[m].sum() \/ 2) \/ nb_alleles\n    if maf != 'NA' and maf > 0.5:\n        minor, major = a1, a2\n        maf = 1 - maf\n        dosage = 2 - dosage\n    return dosage, maf if not set_no_maf else 'NA', minor, major\n","150":"def hard_calls_from_probs(a1, a2, probs):\n    \"\"\"Computes hard calls from probability matrix.\n\n    Args:\n        a1 (str): the first allele\n        a2 (str): the second allele\n        probs (numpy.array): the probability matrix\n\n    Returns:\n        numpy.array: the hard calls computed from the probabilities\n\n    \"\"\"\n    possible_geno = np.array([' '.join([a1] * 2), ' '.join([a1, a2]), ' '.\n        join([a2] * 2)])\n    return possible_geno[np.argmax(probs, axis=1)]\n","151":"def additive_from_probs(a1, a2, probs):\n    \"\"\"Compute additive format from probability matrix.\n\n    Args:\n        a1 (str): the a1 allele\n        a2 (str): the a2 allele\n        probs (numpy.array): the probability matrix\n\n    Returns:\n        tuple: the additive format computed from the probabilities, the minor\n               and major allele.\n\n    The encoding is as follow: 0 when homozygous major allele, 1 when\n    heterozygous and 2 when homozygous minor allele.\n\n    The minor and major alleles are inferred by looking at the MAF. By default,\n    we think a2 is the minor allele, but flip if required.\n\n    \"\"\"\n    calls = np.argmax(probs, axis=1)\n    minor = a2\n    major = a1\n    if np.sum(calls) \/ (len(calls) * 2) > 0.5:\n        calls = 2 - calls\n        minor = a1\n        major = a2\n    return calls, minor, major\n","152":"def _check_shapeit_failed_rc(fn, task=None):\n    \"\"\"Checks the log to explain a failure return code.\n\n    Args:\n        fn (str): the name of the file to check\n        task (str): the name of the task\n\n    Returns:\n        bool: ``True`` if everything is norma, ``False`` otherwise\n\n    This function looks for a known error message in the log file. If the\n    message ``ERROR: Reference and Main panels are not well aligned:`` appears\n    in the log file, then it's normal that the job failed.\n\n    \"\"\"\n    log_fn = fn.replace('.snp.strand', '') + '.log'\n    if not os.path.isfile(log_fn):\n        return False\n    log = None\n    with open(log_fn, 'r') as i_file:\n        log = i_file.read()\n    match = re.search(\n        '\\\\sERROR: Reference and Main panels are not well aligned:\\\\n', log)\n    if match is None:\n        return False\n    return True\n","153":"def _check_shapeit_align_file(fn, task=None):\n    \"\"\"Checks the log to explain the absence of an .snp.strand file.\n\n    Args:\n        fn (str): the name of the file to check\n        task (str): the name of the task\n\n    Returns:\n        bool: ``True`` if everything is normal, ``False`` otherwise.\n\n    This function looks for known message in the log file. If the SNPs were\n    read from the legend file and the haplotypes were read from the hap file,\n    then there were no SNPs flip issue.\n\n    \"\"\"\n    log_fn = fn.replace('.snp.strand', '') + '.log'\n    if not os.path.isfile(log_fn):\n        return False\n    log = None\n    with open(log_fn, 'r') as i_file:\n        log = i_file.read()\n    match = re.search('\\\\sReading SNPs in \\\\[.+\\\\]\\\\n', log)\n    if match is None:\n        return False\n    match = re.search('\\\\sReading reference haplotypes in \\\\[.+\\\\]\\\\n', log)\n    if match is None:\n        return False\n    if task:\n        logging.info('{}: there are no flip issue'.format(task))\n    return True\n","154":"def parse_si_bp(option):\n    SI_STEPS = {'K': 1000, 'M': 1000000, 'G': 1000000000, 'T': 1000000000000}\n    option = str(option).upper().strip().replace(' ', '').replace('BP', '')\n    try:\n        bases = re.findall('-?\\\\d+', option)[0]\n        option = option.replace(bases, '')\n    except IndexError:\n        raise ValueError()\n    bases = int(bases)\n    for char in option:\n        if char in SI_STEPS:\n            bases *= SI_STEPS[char]\n    return bases\n","155":"def __bucketize(self, scores_to_bucket):\n    buckets = {}\n    for region_id, total in enumerate(scores_to_bucket):\n        if total not in buckets:\n            buckets[total] = []\n        buckets[total].append(region_id)\n    return buckets\n","156":"@property\ndef candidates(self):\n    \"\"\"Retrieve candidate region metadata.\n\n        Returns\n        -------\n        Candidate List : list{dict{str, dict{str, int}}}\n            If a query has been performed with :func:`goldilocks.goldilocks.Goldilocks.query`,\n            returns a list of candidates as found in `regions`, sorted by the `func`\n            used by that `query`, sorted and filtered by order and presence in\n            `selected_regions`.\n\n            Otherwise, returns `regions` as a list.\n        \"\"\"\n    if not (len(self.regions) > 0 or self.selected_count == 0):\n        sys.stderr.write('[WARN] No candidates found.\\n')\n    to_iter = sorted(self.regions.keys())\n    if self.selected_count > -1:\n        to_iter = self.selected_regions\n    return [self.regions[i] for i in to_iter]\n"},"comment_lines":{"0":0,"1":0,"2":0,"3":0,"4":0,"5":0,"6":0,"7":0,"8":0,"9":0,"10":0,"11":1,"12":0,"13":0,"14":0,"15":0,"16":0,"17":0,"18":0,"19":0,"20":0,"21":0,"22":3,"23":0,"24":0,"25":6,"26":0,"27":4,"28":6,"29":0,"30":0,"31":0,"32":0,"33":3,"34":2,"35":5,"36":8,"37":16,"38":4,"39":0,"40":0,"41":0,"42":0,"43":0,"44":0,"45":5,"46":0,"47":20,"48":0,"49":0,"50":6,"51":0,"52":0,"53":0,"54":38,"55":0,"56":0,"57":2,"58":4,"59":0,"60":0,"61":0,"62":0,"63":0,"64":3,"65":13,"66":21,"67":0,"68":3,"69":8,"70":0,"71":8,"72":9,"73":13,"74":13,"75":15,"76":10,"77":0,"78":0,"79":0,"80":5,"81":10,"82":2,"83":0,"84":19,"85":18,"86":7,"87":5,"88":11,"89":2,"90":12,"91":20,"92":2,"93":4,"94":3,"95":24,"96":14,"97":8,"98":0,"99":3,"100":13,"101":5,"102":24,"103":6,"104":0,"105":0,"106":0,"107":5,"108":11,"109":2,"110":2,"111":2,"112":2,"113":0,"114":0,"115":0,"116":3,"117":0,"118":0,"119":0,"120":3,"121":0,"122":8,"123":0,"124":0,"125":0,"126":0,"127":0,"128":0,"129":5,"130":0,"131":0,"132":0,"133":4,"134":27,"135":11,"136":7,"137":7,"138":11,"139":10,"140":19,"141":9,"142":6,"143":21,"144":13,"145":18,"146":19,"147":6,"148":10,"149":16,"150":8,"151":15,"152":11,"153":11,"154":0,"155":0,"156":10},"tokens":{"0":103,"1":98,"2":133,"3":68,"4":223,"5":183,"6":78,"7":105,"8":119,"9":85,"10":223,"11":504,"12":229,"13":131,"14":134,"15":71,"16":127,"17":106,"18":160,"19":162,"20":167,"21":168,"22":114,"23":76,"24":65,"25":200,"26":217,"27":283,"28":249,"29":128,"30":690,"31":90,"32":213,"33":613,"34":87,"35":213,"36":243,"37":339,"38":340,"39":57,"40":245,"41":58,"42":53,"43":37,"44":363,"45":248,"46":58,"47":268,"48":103,"49":113,"50":166,"51":131,"52":107,"53":160,"54":683,"55":290,"56":67,"57":111,"58":89,"59":83,"60":74,"61":181,"62":258,"63":187,"64":128,"65":296,"66":575,"67":84,"68":215,"69":120,"70":145,"71":269,"72":184,"73":511,"74":360,"75":307,"76":389,"77":64,"78":68,"79":132,"80":226,"81":271,"82":326,"83":144,"84":281,"85":263,"86":77,"87":243,"88":181,"89":176,"90":391,"91":398,"92":194,"93":406,"94":117,"95":252,"96":239,"97":148,"98":125,"99":160,"100":299,"101":111,"102":329,"103":128,"104":65,"105":97,"106":76,"107":105,"108":100,"109":111,"110":215,"111":289,"112":118,"113":177,"114":62,"115":162,"116":197,"117":68,"118":66,"119":89,"120":104,"121":130,"122":329,"123":83,"124":125,"125":122,"126":112,"127":213,"128":147,"129":115,"130":78,"131":141,"132":147,"133":323,"134":442,"135":166,"136":158,"137":112,"138":217,"139":107,"140":174,"141":85,"142":101,"143":245,"144":260,"145":189,"146":283,"147":75,"148":128,"149":547,"150":128,"151":225,"152":215,"153":263,"154":131,"155":53,"156":184},"lines":{"0":10,"1":10,"2":15,"3":9,"4":19,"5":23,"6":10,"7":11,"8":15,"9":18,"10":21,"11":54,"12":30,"13":12,"14":12,"15":10,"16":19,"17":7,"18":22,"19":22,"20":24,"21":23,"22":15,"23":8,"24":9,"25":23,"26":16,"27":27,"28":43,"29":29,"30":82,"31":10,"32":34,"33":69,"34":12,"35":22,"36":24,"37":44,"38":72,"39":8,"40":33,"41":8,"42":10,"43":8,"44":44,"45":29,"46":9,"47":44,"48":15,"49":11,"50":13,"51":18,"52":14,"53":22,"54":99,"55":36,"56":10,"57":12,"58":13,"59":12,"60":16,"61":11,"62":37,"63":27,"64":11,"65":49,"66":88,"67":13,"68":29,"69":16,"70":18,"71":29,"72":24,"73":74,"74":52,"75":54,"76":48,"77":10,"78":7,"79":23,"80":19,"81":25,"82":48,"83":22,"84":38,"85":22,"86":13,"87":22,"88":20,"89":25,"90":51,"91":52,"92":18,"93":64,"94":10,"95":33,"96":23,"97":17,"98":12,"99":13,"100":26,"101":14,"102":32,"103":13,"104":16,"105":8,"106":8,"107":12,"108":21,"109":17,"110":22,"111":32,"112":15,"113":23,"114":10,"115":19,"116":17,"117":9,"118":10,"119":15,"120":13,"121":14,"122":40,"123":14,"124":8,"125":8,"126":8,"127":28,"128":18,"129":24,"130":11,"131":16,"132":24,"133":33,"134":67,"135":17,"136":25,"137":15,"138":27,"139":15,"140":27,"141":14,"142":19,"143":29,"144":29,"145":26,"146":39,"147":17,"148":18,"149":73,"150":19,"151":27,"152":36,"153":42,"154":22,"155":7,"156":21},"parameters":{"0":1,"1":2,"2":2,"3":1,"4":3,"5":4,"6":2,"7":2,"8":1,"9":2,"10":2,"11":3,"12":3,"13":1,"14":2,"15":5,"16":4,"17":4,"18":2,"19":2,"20":2,"21":2,"22":3,"23":1,"24":3,"25":4,"26":1,"27":2,"28":2,"29":3,"30":3,"31":3,"32":3,"33":4,"34":3,"35":2,"36":2,"37":3,"38":2,"39":1,"40":4,"41":2,"42":4,"43":2,"44":5,"45":3,"46":2,"47":1,"48":1,"49":2,"50":6,"51":1,"52":2,"53":1,"54":4,"55":1,"56":1,"57":3,"58":1,"59":5,"60":1,"61":8,"62":4,"63":4,"64":2,"65":7,"66":14,"67":1,"68":5,"69":1,"70":2,"71":4,"72":3,"73":3,"74":1,"75":3,"76":2,"77":1,"78":2,"79":1,"80":1,"81":1,"82":1,"83":2,"84":4,"85":2,"86":2,"87":1,"88":2,"89":2,"90":3,"91":1,"92":4,"93":6,"94":2,"95":1,"96":4,"97":3,"98":5,"99":3,"100":4,"101":3,"102":4,"103":3,"104":1,"105":1,"106":1,"107":1,"108":1,"109":3,"110":3,"111":2,"112":1,"113":3,"114":1,"115":1,"116":1,"117":2,"118":3,"119":1,"120":2,"121":0,"122":3,"123":1,"124":2,"125":2,"126":1,"127":1,"128":1,"129":1,"130":1,"131":2,"132":3,"133":3,"134":5,"135":1,"136":2,"137":2,"138":1,"139":1,"140":2,"141":1,"142":1,"143":1,"144":1,"145":1,"146":5,"147":1,"148":1,"149":6,"150":3,"151":3,"152":2,"153":2,"154":1,"155":2,"156":1},"functions":{"0":7,"1":13,"2":13,"3":3,"4":13,"5":13,"6":13,"7":18,"8":18,"9":14,"10":14,"11":14,"12":14,"13":14,"14":17,"15":17,"16":17,"17":1,"18":4,"19":4,"20":4,"21":4,"22":27,"23":42,"24":49,"25":23,"26":87,"27":12,"28":8,"29":14,"30":10,"31":38,"32":38,"33":21,"34":21,"35":22,"36":22,"37":22,"38":22,"39":33,"40":6,"41":5,"42":9,"43":8,"44":22,"45":22,"46":5,"47":15,"48":10,"49":17,"50":15,"51":31,"52":5,"53":44,"54":19,"55":18,"56":92,"57":92,"58":92,"59":6,"60":32,"61":62,"62":62,"63":14,"64":14,"65":53,"66":53,"67":53,"68":13,"69":13,"70":13,"71":13,"72":12,"73":12,"74":12,"75":12,"76":12,"77":12,"78":10,"79":6,"80":10,"81":4,"82":4,"83":12,"84":12,"85":12,"86":12,"87":9,"88":9,"89":15,"90":15,"91":4,"92":18,"93":18,"94":18,"95":18,"96":18,"97":18,"98":13,"99":13,"100":13,"101":13,"102":13,"103":13,"104":4,"105":4,"106":16,"107":4,"108":19,"109":4,"110":4,"111":4,"112":7,"113":105,"114":105,"115":105,"116":5,"117":45,"118":45,"119":45,"120":45,"121":45,"122":45,"123":9,"124":5,"125":5,"126":34,"127":8,"128":9,"129":37,"130":10,"131":10,"132":10,"133":10,"134":10,"135":25,"136":25,"137":8,"138":10,"139":10,"140":10,"141":10,"142":10,"143":24,"144":24,"145":24,"146":24,"147":9,"148":9,"149":7,"150":7,"151":7,"152":7,"153":7,"154":1,"155":14,"156":14},"globals":{"0":0.0,"1":0.0,"2":0.0,"3":0.0,"4":1.0,"5":1.0,"6":1.0,"7":0.0,"8":0.0,"9":0.0,"10":0.0,"11":0.0,"12":0.0,"13":0.0,"14":0.0,"15":0.0,"16":0.0,"17":0.0,"18":0.0,"19":0.0,"20":0.0,"21":0.0,"22":0.0,"23":2.0,"24":0.0,"25":4.0,"26":4.0,"27":0.0,"28":0.0,"29":5.0,"30":0.0,"31":6.0,"32":6.0,"33":4.0,"34":4.0,"35":0.0,"36":0.0,"37":0.0,"38":0.0,"39":7.0,"40":1.0,"41":0.0,"42":0.0,"43":0.0,"44":18.0,"45":18.0,"46":0.0,"47":0.0,"48":8.0,"49":4.0,"50":1.0,"51":3.0,"52":0.0,"53":9.0,"54":5.0,"55":2.0,"56":21.0,"57":21.0,"58":21.0,"59":0.0,"60":5.0,"61":20.0,"62":20.0,"63":2.0,"64":2.0,"65":13.0,"66":13.0,"67":13.0,"68":0.0,"69":0.0,"70":0.0,"71":0.0,"72":0.0,"73":0.0,"74":0.0,"75":0.0,"76":0.0,"77":0.0,"78":0.0,"79":0.0,"80":0.0,"81":0.0,"82":0.0,"83":0.0,"84":0.0,"85":0.0,"86":0.0,"87":0.0,"88":0.0,"89":0.0,"90":0.0,"91":0.0,"92":0.0,"93":0.0,"94":0.0,"95":0.0,"96":0.0,"97":0.0,"98":0.0,"99":0.0,"100":0.0,"101":0.0,"102":0.0,"103":0.0,"104":0.0,"105":0.0,"106":0.0,"107":0.0,"108":0.0,"109":0.0,"110":0.0,"111":0.0,"112":0.0,"113":1.0,"114":1.0,"115":1.0,"116":0.0,"117":0.0,"118":0.0,"119":4.0,"120":4.0,"121":4.0,"122":4.0,"123":0.0,"124":1.0,"125":1.0,"126":0.0,"127":0.0,"128":0.0,"129":0.0,"130":0.0,"131":0.0,"132":0.0,"133":0.0,"134":0.0,"135":0.0,"136":0.0,"137":0.0,"138":1.0,"139":1.0,"140":1.0,"141":1.0,"142":1.0,"143":2.0,"144":2.0,"145":2.0,"146":2.0,"147":0.0,"148":0.0,"149":0.0,"150":0.0,"151":0.0,"152":0.0,"153":0.0,"154":0.0,"155":0.0,"156":0.0},"imports":{"0":7,"1":13,"2":13,"3":2,"4":8,"5":8,"6":8,"7":8,"8":8,"9":4,"10":4,"11":4,"12":4,"13":4,"14":7,"15":7,"16":7,"17":3,"18":3,"19":3,"20":3,"21":3,"22":7,"23":10,"24":95,"25":17,"26":13,"27":13,"28":11,"29":11,"30":13,"31":19,"32":19,"33":12,"34":12,"35":24,"36":24,"37":24,"38":24,"39":23,"40":6,"41":4,"42":19,"43":9,"44":19,"45":19,"46":4,"47":6,"48":15,"49":23,"50":15,"51":31,"52":4,"53":21,"54":16,"55":16,"56":63,"57":63,"58":63,"59":5,"60":18,"61":24,"62":24,"63":13,"64":13,"65":41,"66":41,"67":41,"68":4,"69":4,"70":4,"71":4,"72":5,"73":5,"74":5,"75":5,"76":5,"77":5,"78":4,"79":5,"80":14,"81":4,"82":4,"83":2,"84":2,"85":2,"86":2,"87":4,"88":4,"89":9,"90":9,"91":3,"92":6,"93":6,"94":6,"95":6,"96":6,"97":6,"98":4,"99":4,"100":4,"101":4,"102":4,"103":4,"104":5,"105":5,"106":7,"107":3,"108":18,"109":3,"110":3,"111":3,"112":7,"113":14,"114":14,"115":14,"116":3,"117":18,"118":18,"119":15,"120":15,"121":15,"122":15,"123":21,"124":1,"125":1,"126":28,"127":10,"128":6,"129":11,"130":7,"131":7,"132":7,"133":7,"134":7,"135":19,"136":19,"137":7,"138":3,"139":3,"140":3,"141":3,"142":3,"143":24,"144":24,"145":24,"146":24,"147":15,"148":15,"149":3,"150":3,"151":3,"152":15,"153":15,"154":1,"155":12,"156":12}}