{"golden":{"0":"# 2SUM\n\nfrom .helpers import ints\n\n\ndef two_sum(n, a):\n    h = {}\n    for i in range(n):\n        if a[i] in h:\n            return [h[a[i]] + 1, i + 1]\n        h[-a[i]] = i\n    return [-1]\n\n\ndef main(file):\n    info, *arrays = open(file).read().splitlines()\n    k, n = ints(info)\n    for arr in arrays:\n        print(*two_sum(int(n), ints(arr)))\n","1":"# Wobble Bonding and RNA Secondary Structures\n\nfrom functools import cache\nfrom .helpers import Parser\n\n\ndef wobble_pair(x, y):\n    pair = {\"A\": \"U\", \"U\": \"AG\", \"C\": \"G\", \"G\": \"CU\"}\n    return x in pair[y]\n\n\n@cache\ndef rnas(seq):\n    \"\"\"Wobble Bonding and RNA Secondary Structures\"\"\"\n    if len(seq) in range(1):\n        return 1\n    else:\n        return rnas(seq[1:]) + sum(\n            rnas(seq[1:m]) * rnas(seq[m + 1 :])\n            for m in range(4, len(seq))\n            if wobble_pair(seq[0], seq[m])\n        )\n\n\ndef main(file):\n    print(rnas(Parser(file).line()))\n","2":"# Counting Subsets\n\nfrom .helpers import Parser\n\n\ndef main(file):\n    n = Parser(file).ints()[0]\n    print(2**n % 1000000)\n","3":"# Fixing an Inconsistent Character Set\n\n# We can find conflicts if there's an intersection in all implied sets\n# between two character rows\n\nfrom collections import defaultdict\nfrom itertools import product\n\n\ndef conflict(c1, c2):\n    for a, b in product([1, 0], repeat=2):\n        s1 = set(i for i, c in enumerate(c1) if c == a)\n        s2 = set(i for i, c in enumerate(c2) if c == b)\n        if len(s1.intersection(s2)) == 0:\n            return False\n    return True\n\n\ndef conflicts(characters):\n    count = defaultdict(int)\n    for i in range(len(characters)):\n        for j in range(i + 1, len(characters)):\n            if conflict(characters[i], characters[j]):\n                count[i] += 1\n                count[j] += 1\n    return count\n\n\ndef main(file):\n    lines = open(file).read().splitlines()\n    characters = [[int(x) for x in list(ch)] for ch in lines]\n    count = conflicts(characters)\n    rm = [k for k, v in count.items() if v == max(count.values())][0]\n    print(*(lines[:rm] + lines[rm + 1 :]), sep=\"\\n\")\n","4":"# Implement the Neighbor Joining Algorithm\n\nfrom collections import defaultdict\nimport numpy as np\nfrom .ba7b import parse_mat\nfrom .ba7d import as_edges, closest\n\n\ndef nj_matrix(D, n):\n    ND = np.copy(D)\n    for i in range(len(D)):\n        for j in range(len(D)):\n            if i != j:\n                ND[i, j] = (n - 2) * D[i, j] - sum(D[i, :]) - sum(D[j, :])\n    return ND\n\n\ndef neighbor_joining(D, n, labels=None):\n    if not labels:\n        labels = list(range(n))\n\n    if n == 2:\n        T = defaultdict(list)\n        T[labels[0]].append({\"n\": labels[1], \"w\": D[0][1]})\n        return T\n\n    ND = nj_matrix(D, n)\n    i, j = closest(ND)\n    delta = (sum(D[i, :]) - sum(D[j, :])) \/ (n - 2)\n    limb_i = (D[i, j] + delta) \/ 2\n    limb_j = (D[i, j] - delta) \/ 2\n\n    li = labels[i]\n    lj = labels[j]\n\n    D = np.append(D, np.zeros((1, len(D))), axis=0)\n    D = np.append(D, np.zeros((len(D), 1)), axis=1)\n    labels = labels + [max(labels) + 1]\n\n    for k in range(n):\n        D[k, n] = (D[k, i] + D[k, j] - D[i, j]) \/ 2\n        D[n, k] = (D[k, i] + D[k, j] - D[i, j]) \/ 2\n    for x in [j, i]:\n        D = np.delete(D, x, 0)\n        D = np.delete(D, x, 1)\n        del labels[x]\n\n    T = neighbor_joining(D, n - 1, labels)\n\n    T[labels[-1]].append({\"n\": li, \"w\": limb_i})\n    T[labels[-1]].append({\"n\": lj, \"w\": limb_j})\n    return T\n\n\ndef main(file):\n    n, *D = open(file).read().splitlines()\n    D = np.array(parse_mat(D), float)\n    graph = neighbor_joining(D, int(n))\n    for edge in as_edges(graph):\n        print(edge)\n","5":"# Breadth-First Search\n\nfrom .helpers import parse_graph\n\n\ndef bfs(graph, start=1):\n    n = len(graph)\n    d = [-1 for i in range(n + 1)]\n    d[start] = 0\n    q = [start]\n    while q:\n        u = q.pop(0)\n        for v in graph[u]:\n            if d[v] == -1:\n                q.append(v)\n                d[v] = d[u] + 1\n    return d[1:]\n\n\ndef main(file):\n    graph = parse_graph(open(file), directed=True)\n    print(*bfs(graph))\n","6":"# Independent Alleles\n\nfrom .helpers import Parser, pbinom\n\n\ndef lia(k, n):\n    return 1 - pbinom(n - 1, 2**k, 0.25)\n\n\ndef main(file):\n    print(round(lia(*Parser(file).ints()), 3))\n","7":"# GenBank Introduction\n\nfrom Bio import Entrez\nimport os\n\n\ndef entrez_email():\n    try:\n        email = os.environ[\"ENTREZ_EMAIL\"]\n    except KeyError:\n        print(\"Please add your email as the environment variable 'ENTREZ_EMAIL'\")\n        exit()\n    return email\n\n\ndef gbk(orgm, start, end):\n    Entrez.email = entrez_email()\n    handle = Entrez.esearch(\n        db=\"nucleotide\",\n        term=f\"{orgm}[Organism]\",\n        datetype=\"pdat\",\n        mindate=start,\n        maxdate=end,\n    )\n    return Entrez.read(handle)\n\n\ndef main(file):\n    orgm, start, end = open(file).read().splitlines()\n    print(gbk(orgm, start, end)[\"Count\"])\n","8":"# Strings and Lists\n\n\ndef main(file):\n    str, slices = open(file).read().splitlines()\n    a, b, c, d = map(int, slices.split())\n    print(str[a : b + 1], str[c : d + 1])\n","9":"# Generate the Convolution of a Spectrum\n\nfrom collections import defaultdict, Counter\n\n\ndef spectrum_convolution(spec):\n    dict = defaultdict(int)\n    for i in range(len(spec)):\n        for j in range(i + 1, len(spec)):\n            m = abs(spec[i] - spec[j])\n            if m > 0:\n                dict[abs(spec[i] - spec[j])] += 1\n    return Counter(dict).most_common()\n\n\ndef main(file):\n    mass = open(file).read().split()\n    mass = list(map(int, mass))\n    res = spectrum_convolution(mass)\n    print(*sum([[k] * v for k, v in res], []))\n","10":"# Degree Array\n\nfrom .helpers import parse_graph\n\n\ndef main(file):\n    graph = parse_graph(open(file))\n    print(*[len(graph[node]) for node in graph.keys()])\n","11":"# Find the Most Frequent Words with Mismatches in a String\n\nfrom .ba1g import hamming\nfrom .ba1b import count_kmers, most_frequent\nfrom itertools import product\n\n# Note, the best kmer might not be observed in our sequence. The simplistic\n# method here simply checks all possible kmers (which is ~17M for k = 12)\n\n\ndef generate_kmers(k):\n    return (\"\".join(x) for x in product([\"A\", \"C\", \"G\", \"T\"], repeat=k))\n\n\ndef count_hamming_kmers(kmers, d, k):\n    for x in generate_kmers(k):\n        count = sum(kmers[y] for y in kmers if hamming(x, y) <= d)\n        if count > 0:\n            yield [x, count]\n\n\ndef main(file):\n    seq, ints = open(file).read().splitlines()\n    k, d = list(map(int, ints.split()))\n    kmers = count_kmers(seq, k)\n    hkmers = dict(count_hamming_kmers(kmers, d, k))\n    print(*most_frequent(hkmers))\n","12":"# Implement GreedyMotifSearch with Pseudocounts\n\nfrom .ba2d import greedy_motif_search\n\n\ndef main(file):\n    ints, *dna = open(file).read().splitlines()\n    k, t = map(int, ints.split())\n    print(*greedy_motif_search(dna, k, pc=1), sep=\"\\n\")\n","13":"# Alignment-Based Phylogeny\n\n# We can solve this with the \"Small Parsimony\" algorithm\n\nfrom .helpers import Parser\nfrom .nwck import parse_newick\nfrom math import inf\n\n\ndef nodes(graph):\n    s = list(graph.keys())\n    e = [y for v in graph.values() for y in v]\n    return set(s) | set(e)\n\n\n# return all leaves of a simple graph\ndef leaves(graph):\n    return nodes(graph) - set(graph.keys())\n\n\ndef extract_position(graph, seqs, pos):\n    chars = {}\n    for n in nodes(graph) - leaves(graph):\n        chars[n] = \"\"\n    for leaf in leaves(graph):\n        chars[leaf] = seqs[leaf][pos]\n    return chars\n\n\ndef traceback(skp, node, ind):\n    bases = [\"A\", \"C\", \"T\", \"G\", \"-\"]\n    chars = {}\n    chars[node] = bases[ind]\n    for k, v in skp[node][ind].items():\n        if k in skp:\n            chars = chars | traceback(skp, k, v)\n    return chars\n\n\ndef small_parsimony(graph, chars):\n    bases = [\"A\", \"C\", \"T\", \"G\", \"-\"]\n    sk = {}  # minimum parsimony score of the subtree over possible labels\n    skp = {}  # pointer to selected base for each child over possible labels\n    to_process = nodes(graph)\n\n    # # initialise leaves\n    for leaf in leaves(graph):\n        sk[leaf] = [0 if chars[leaf] == c else inf for c in bases]\n        to_process.remove(leaf)\n\n    # iterate over available nodes till all are processed\n    while to_process:\n        for n in list(to_process):\n            if all(v in sk for v in graph[n]):\n                sk[n], skp[n] = [], []\n                for k in bases:\n                    tot = 0\n                    ptr = {}\n                    for d, sk_child in [(d, sk[d]) for d in graph[n]]:\n                        score = []\n                        for i, c in enumerate(bases):\n                            score += [sk_child[i] + (0 if c == k else 1)]\n                        tot += min(score)\n                        ptr[d] = score.index(min(score))\n                    skp[n] += [ptr]\n                    sk[n] += [tot]\n                to_process.remove(n)\n\n    # Recover sequence\n    node = \"0\"\n    score = min(sk[node])\n    return score, traceback(skp, node, sk[node].index(score))\n\n\ndef alph(tree, seqs, i):\n    # initialise sequences\n    for n in nodes(tree) - leaves(tree):\n        seqs[n] = \"\"\n\n    n = len(seqs[list(leaves(tree))[0]])\n    total_score = 0\n    for pos in range(n):\n        chars = extract_position(tree, seqs, pos)\n        score, tbchars = small_parsimony(tree, chars)\n        total_score += score\n        for k, v in tbchars.items():\n            seqs[k] += v\n\n    return total_score, seqs\n\n\ndef simplify_tree(graph):\n    return {k: [x[\"n\"] for x in v] for k, v in graph.items()}\n\n\ndef main(file):\n    handle = open(file)\n    tree = parse_newick(next(handle))\n    tree = simplify_tree(tree)\n    seqs = Parser(handle).fastas()\n    seqs = {x.id: x.seq for x in seqs}\n    total_score, seqs = alph(tree, seqs, 1)\n    print(total_score)\n    for node in tree.keys():\n        if node != \"0\":\n            print(f\">{node}\")\n            print(seqs[node])\n","14":"# Counting Rooted Binary Trees\nfrom functools import reduce\n\n\n# The answer here is just (2n -3)!!\n# We can help the calculation by taking the modulo at each step\ndef root(n, mod=10**6):\n    return reduce(lambda a, b: a * b % mod, range(2 * n - 3, 1, -2))\n\n\ndef main(file):\n    print(root(int(open(file).read())))\n","15":"# Inferring Genotype from a Pedigree\n\nfrom .nwck import parse_newick\nimport numpy as np\n\n\n# For two vectors of parent genotype probabilities, compute child vector\ndef child_gt(a, b):\n    cross = np.array(\n        [\n            [[1, 0, 0], [0.5, 0.5, 0], [0, 1, 0]],\n            [[0.5, 0.5, 0], [0.25, 0.5, 0.25], [0, 0.5, 0.5]],\n            [[0, 1, 0], [0, 0.5, 0.5], [0, 0, 1]],\n        ]\n    )\n    return np.dot(a, np.dot(b, cross))\n\n\ndef main(file):\n    tree = parse_newick(open(file).read().rstrip())\n\n    # Initial probability vectors based on known genotypes\n    gts = {\n        \"AA\": np.array([1, 0, 0]),\n        \"Aa\": np.array([0, 1, 0]),\n        \"aa\": np.array([0, 0, 1]),\n    }\n\n    # Compute vector for nodes not in `gts` dict till we've done all\n    q = list(tree.keys())\n    while q:\n        for n in q:\n            if all(c[\"n\"] in gts for c in tree[n]):\n                q.remove(n)\n                gts[n] = child_gt(*[gts[c[\"n\"]] for c in tree[n]])\n\n    print(*[round(x, 3) for x in gts[\"0\"]])\n","16":"# Base Filtration by Quality\n\nfrom Bio import SeqIO\nimport sys\n\n\ndef main(file):\n    handle = open(file)\n    q = int(next(handle))\n    for x in SeqIO.parse(handle, \"fastq\"):\n        quals = x.letter_annotations[\"phred_quality\"]\n        for start in range(len(quals)):\n            if quals[start] >= q:\n                break\n        for end in range(len(quals) - 1, 0, -1):\n            if quals[end] >= q:\n                break\n        SeqIO.write(x[start : end + 1], sys.stdout, \"fastq\")\n","17":"# Calculating Protein Mass\n\nfrom .helpers import Parser\nfrom rosalind.helpers import aa_mass\n\n\ndef protein_mass(prot):\n    aam = aa_mass()\n    return sum([aam[x] for x in prot])\n\n\ndef main(file):\n    m = protein_mass(Parser(file).line())\n    print(round(m, 3))\n","18":"# Find a Position in a Genome Minimizing the Skew\n\n\ndef find_minima(seq):\n    skew = [0]\n    delta = {\"G\": 1, \"C\": -1, \"A\": 0, \"T\": 0}\n    for i in range(len(seq)):\n        skew.append(skew[i] + delta[seq[i]])\n    m = min(skew)\n    return (i for i, x in enumerate(skew) if x == m)\n\n\ndef main(file):\n    seq = open(file).read().splitlines()[0]\n    print(*find_minima(seq))\n","19":"# Semi-Connected Graph\n\nfrom .helpers import parse_graphs\nfrom .scc import scc\nfrom .hdag import hdag\n\n# The idea here is we take strongly connected components\n# Then we build a new graph with edges between components\n# If there is a hamiltonian path in this new graph, then we are semi connected\n\n\ndef find_comp(n, components):\n    for j, comp in enumerate(components):\n        if n in comp:\n            return j\n\n\ndef condense(graph, components):\n    ngraph = {}\n    for i, comp in enumerate(components):\n        ngraph[i] = set(\n            [\n                find_comp(dest, components)\n                for node in comp\n                for dest in graph[node]\n                if dest not in comp\n            ]\n        )\n    return ngraph\n\n\ndef sc(graph):\n    components = list(scc(graph))\n    ngraph = condense(graph, components)\n    return -1 if hdag(ngraph) == [-1] else 1\n\n\ndef main(file):\n    graphs = parse_graphs(open(file), directed=True)\n    print(*[sc(g) for g in graphs])\n","20":"# Implement the Lloyd Algorithm for k-Means Clustering\n\nimport numpy as np\nfrom .ba8a import read_types, euclidean_distance\n\n\ndef ncd_assignment(x, centers):\n    \"\"\"Center index that minimises Euclidean distance to point\"\"\"\n    dists = [euclidean_distance(x, c) for c in centers]\n    return dists.index(min(dists))\n\n\ndef compute_center(points, assigns, i):\n    data = [p for p, a in zip(points, assigns) if a == i]\n    return np.mean(np.array(data), 0)\n\n\n# I'm lazily avoiding checking for convergence here and hoping that 20\n# iterations is enough. This converges quite quickly though...\ndef k_means(points, k, iter=20):\n    centers = points[:k]\n    for _ in range(iter):\n        assigns = [ncd_assignment(point, centers) for point in points]\n        centers = [compute_center(points, assigns, i) for i in range(k)]\n    return centers\n\n\ndef main(file):\n    handle = open(file)\n    k, m = next(read_types(handle, int))\n    points = [np.array(point) for point in read_types(handle, float)]\n    for m in k_means(points, k):\n        print(*[f\"{x:.3f}\" for x in m])\n","21":"# Using the Spectrum Graph to Infer Peptides\n\nfrom .helpers import Parser\nfrom .full import weight_graph\n\n\ndef sgra(ions):\n    \"\"\"Using the Spectrum Graph to Infer Peptides\"\"\"\n\n    def infer_peptide(w, seq):\n        for w2, aa in graph[w]:\n            yield from infer_peptide(w2, seq + aa)\n        yield seq\n\n    graph = weight_graph(ions)\n    w = min(ions)\n    return max(list(infer_peptide(w, \"\")), key=len)\n\n\ndef main(file):\n    weights = [float(x) for x in Parser(file).lines()]\n    print(sgra(weights), sep=\"\\n\")\n","22":"# Complementing a Strand of DNA\n\nfrom .helpers import Parser\n\n\ndef main(file):\n    print(Parser(file).dna().revc())\n","23":"# Counting Point Mutations\n\nfrom .helpers import Parser\n\n\ndef hamm(s1, s2):\n    \"\"\"Calculate Hamming distance\"\"\"\n    return sum(xi != yi for xi, yi in zip(s1, s2))\n\n\ndef main(file):\n    print(hamm(*Parser(file).lines()))\n","24":"# Implement ColoredEdges\n\nimport re\nfrom .ba6f import chromosome2cycle\n\n\ndef ints(x):\n    return list(map(int, x.split()))\n\n\ndef colored_edges(P):\n    g = {}\n    for chromosome in P:\n        nodes = chromosome2cycle(chromosome)\n        for j in range(len(chromosome)):\n            i1 = 2 * j + 1\n            i2 = (2 * j + 2) % len(nodes)\n            g[nodes[i1]] = nodes[i2]\n    return g\n\n\ndef main(file):\n    s = open(file).read().rstrip()\n    P = [ints(x) for x in re.findall(r\"\\((.+?)\\)\", s)]\n    edges = [(k, v) for k, v in colored_edges(P).items()]\n    print(*edges, sep=\", \")\n","25":"# Inferring Peptide from Full Spectrum\n\nfrom math import isclose\nfrom collections import defaultdict\nfrom .helpers import Parser\nfrom .spec import match_mass\n\n\n# Return a dictionary that let's us look up complementary ion pairs\n# e.g. \"PRO\" and \"TEIN\" for full peptide \"PROTEIN\"\n# We can find pairs, because their weights approximately sum to that\n# of the peptide.\ndef find_pairs(peptide, ions):\n    pairs = {}\n    for w in ions:\n        for w2 in ions:\n            if isclose(w2 + w, peptide):\n                pairs[w] = w2\n    return pairs\n\n\n# This builds a graph of possibly adjacent ions.\n# ions can be considered adjacent in a graph if the difference in\n# their masses is close to the mass of a known amino acid.\ndef weight_graph(ions):\n    graph = defaultdict(list)\n    for i in range(len(ions)):\n        for j in range(i + 1, len(ions)):\n            aa = match_mass(ions[j] - ions[i])\n            if aa:\n                graph[ions[i]] += [[ions[j], aa]]\n    return graph\n\n\ndef full(peptide, ions):\n    \"\"\"Inferring Peptide from Full Spectrum\"\"\"\n\n    def infer_peptide(w, seq, rm):\n        for w2, aa in graph[w]:\n            if w2 in rm:\n                continue\n            if len(seq) + 1 == target_len:\n                yield seq + aa\n            else:\n                yield from infer_peptide(w2, seq + aa, rm + [w, pairs[w]])\n\n    graph = weight_graph(ions)\n    pairs = find_pairs(peptide, ions)\n    target_len = int(len(ions) \/ 2 - 1)\n    w = min(ions)\n    return list(infer_peptide(w, \"\", [w, pairs[w]]))\n\n\ndef main(file):\n    weights = [float(x) for x in Parser(file).lines()]\n    print(*full(weights[0], weights[1:]), sep=\"\\n\")\n","26":"# Conditions and Loops\n\n\ndef main(file):\n    a, b = map(int, open(file).read().split())\n    print(sum([x for x in range(a, b) if x % 2 == 1]))\n","27":"# Implement ChromosomeToCycle\n\nfrom .ba6a import read_perm\n\n\ndef chromosome2cycle(perm):\n    nodes = []\n    for i in perm:\n        if i > 0:\n            nodes += [2 * i - 1, 2 * i]\n        else:\n            nodes += [-2 * i, -2 * i - 1]\n    return nodes\n\n\ndef ba6f(s):\n    return chromosome2cycle(read_perm(s))\n\n\ndef format_cycle(cycle):\n    return \"(\" + \" \".join(map(str, cycle)) + \")\"\n\n\ndef main(file):\n    s = open(file).read().rstrip()\n    print(format_cycle(ba6f(s)))\n","28":"# Matching Random Motifs\n\nfrom math import exp\nfrom .helpers import Parser\n\n\ndef main(file):\n    l1, seq = Parser(file).lines()\n    n, x = map(float, l1.split(\" \"))\n    gc = sum([seq.count(x) for x in \"GC\"])\n    lam = ((1 - x) \/ 2) ** (len(seq) - gc) * (x \/ 2) ** gc * n\n    print(round(1 - exp(-lam), 3))\n","29":"# Implement TreeColoring\n\nfrom collections import defaultdict\nfrom .ba9e import color_tree\n\n\ndef parse_input(file):\n    f = open(file)\n    g = defaultdict(list)\n    for line in f:\n        line = line.rstrip()\n        if line == \"-\":\n            break\n        x, nodes = line.split(\" -> \")\n        if nodes == \"{}\":\n            continue\n        for n in nodes.split(\",\"):\n            g[int(x)].append({\"n\": int(n)})\n\n    colors = defaultdict(lambda: None)\n    for line in f:\n        line = line.rstrip()\n        x, c = line.split(\": \")\n        colors[int(x)] = c\n    return g, colors\n\n\ndef main(file):\n    g, colors = parse_input(file)\n    colors = color_tree(g, colors)\n    for k in sorted(colors.keys()):\n        print(f\"{k}: {colors[k]}\")\n","30":"# Sequence a Peptide\n\nfrom collections import defaultdict\nfrom .ba11d import prefixes2peptide\nfrom .ba11c import masses\nfrom .ba7a import nodes\nfrom math import inf\n\n\n# Bellman-Ford Algorithm (required for the negative weights)\n# This version finds longest path in a DAG from source to sink and returns path\ndef bf(start, graph):\n    d = [-inf for _ in range(len(nodes(graph)))]\n    d[start] = 0\n    p = {start: []}\n    n = nodes(graph)\n    for _ in range(len(n) - 1):\n        for u, x in graph.items():\n            for v in x:\n                if d[u] + v[\"w\"] > d[v[\"n\"]]:\n                    d[v[\"n\"]] = d[u] + v[\"w\"]\n                    p[v[\"n\"]] = p[u] + [v[\"n\"]]\n    return d, p\n\n\ndef build_dag(v, masses):\n    v = [0] + v\n    dag = defaultdict(list)\n    m = {v: k for k, v in masses.items()}\n    for i in range(len(v)):\n        for j in range(i + 1, len(v)):\n            if j - i in m:\n                dag[i].append({\"n\": j, \"w\": v[j]})\n    return dag\n\n\ndef main(file):\n    m = masses()\n    vector = list(map(int, open(file).read().split()))\n    dag = build_dag(vector, m)\n    _, path = bf(0, dag)\n    print(prefixes2peptide(path[len(vector)], m))\n","31":"# Motzkin Numbers and RNA Secondary Structures\n\nfrom functools import cache\nfrom .helpers import Parser\nfrom .cat import valid_pair\n\n\n@cache\ndef motz(seq, mod=10**6):\n    \"\"\"Motzkin Numbers and RNA Secondary Structures\"\"\"\n    if len(seq) in range(1):\n        return 1\n    else:\n        return (\n            motz(seq[1:])\n            + sum(\n                motz(seq[1:m]) * motz(seq[m + 1 :])\n                for m in range(1, len(seq))\n                if valid_pair(seq[0], seq[m])\n            )\n            % mod\n        )\n\n\ndef main(file):\n    print(motz(Parser(file).seqs()[0]))\n","32":"# Generate the k-mer Composition of a String\n\nfrom .ba1a import substrings\n\n\ndef main(file):\n    k, seq = open(file).read().splitlines()\n    print(*substrings(seq, int(k)), sep=\"\\n\")\n","33":"# Construct a Profile HMM\n\n# from book:\n# ignore columns for which the fraction of space symbols is greater than or\n# equal to a column removal threshold q\n\nimport numpy as np\n\n\ndef parse_input(handle):\n    \u03b8 = float(next(handle).rstrip())\n    next(handle)\n    alphabet = next(handle).split()\n    next(handle)\n    alignment = np.array([list(x) for x in handle.read().splitlines()])\n    return \u03b8, alphabet, alignment\n\n\n# compute column or row in emission probability matrix\ndef index(i, type):\n    if type == \"ins\":\n        return (i + 1) * 3 + 1\n    else:\n        return {\"match\": 0, \"del\": 1}[type] + 3 * i + 2\n\n\n# There must be an easier way, but this divides rows by rowMax\n# ignore zeros in the row, or when the rowsum is zero.\ndef rownorm(x, inc_zeros=False, min_val=0.0):\n    if inc_zeros and sum(x) == 0:\n        x[:] = 1\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n        new = x \/ sum(x)\n        new[x == 0.0] = min_val\n        return new\n\n\ndef normalise(x, inc_zeros=False, min_val=0.0):\n    return np.array([rownorm(r, inc_zeros=inc_zeros, min_val=min_val) for r in x])\n\n\n# This problem is very fussy about formatting.\n# I think we need to ensure correct rounding and certainly tab-delimiting\ndef print_mat(mat, rl, cl):\n    print(*cl, sep=\"\\t\")\n    for i, row in enumerate(mat):\n        r = [rl[i]] + [round(x, 3) if x > 0.0 else \"0\" for x in row]\n        print(*r, sep=\"\\t\")\n\n\ndef print_tprob(x):\n    n = (x.shape[0] - 3) \/\/ 3\n    print_mat(x, state_labels(n), state_labels(n))\n\n\ndef print_eprob(x, alphabet):\n    n = (x.shape[0] - 3) \/\/ 3\n    print_mat(x, state_labels(n), alphabet)\n\n\ndef state_labels(n):\n    x = [\"S\", \"I0\"]\n    for i in range(1, n + 1):\n        x += [f\"M{i}\", f\"D{i}\", f\"I{i}\"]\n    x += \"E\"\n    return x\n\n\ndef transition_mat(n):\n    x = np.zeros((n * 3 + 3, n * 3 + 3), dtype=float)\n    return x\n\n\n# Initialise a emission probability matrix\ndef emission_mat(n, m):\n    return np.zeros((n * 3 + 3, m), dtype=float)\n\n\ndef profile_hmm(\u03b8, alphabet, alignment):\n    valid_col = np.mean(alignment == \"-\", axis=0) < \u03b8\n    valid_len = sum(valid_col)\n    end = valid_len * 3 + 2\n    tprob = transition_mat(valid_len)\n    eprob = emission_mat(valid_len, len(alphabet))\n\n    for seq in alignment:\n        pind = 0\n        j = -1\n        for i, char in enumerate(seq):\n            if valid_col[i]:\n                j += 1\n                if char == \"-\":\n                    ind = index(j, \"del\")\n                else:\n                    ind = index(j, \"match\")\n                tprob[pind, ind] += 1\n                pind = ind\n            else:\n                if char != \"-\":\n                    ind = index(j, \"ins\")\n                    tprob[pind, ind] += 1\n                    pind = ind\n            if char != \"-\":\n                eprob[ind, alphabet.index(char)] += 1\n        tprob[pind, end] += 1\n\n    tprob = normalise(tprob)\n    eprob = normalise(eprob)\n\n    return tprob, eprob\n\n\ndef main(file):\n    \u03b8, alphabet, alignment = parse_input(open(file))\n    tprob, eprob = profile_hmm(\u03b8, alphabet, alignment)\n    print_tprob(tprob)\n    print(\"--------\")\n    print_eprob(eprob, alphabet)\n","34":"# Find All Approximate Occurrences of a Pattern in a String\n\nfrom .ba1a import substrings\nfrom .ba1g import hamming\n\n\ndef find_approx_matches(pattern, text, d):\n    for i, x in enumerate(substrings(text, len(pattern))):\n        if hamming(x, pattern) <= d:\n            yield i\n\n\ndef main(file):\n    pattern, text, d = open(file).read().splitlines()\n    print(*find_approx_matches(pattern, text, int(d)))\n","35":"# Construct the De Bruijn Graph of a String\n\nfrom .ba3d import dbru\n\n\ndef main(file):\n    seqs = open(file).read().splitlines()\n    for k, v in dbru(seqs).items():\n        print(k + \" -> \" + \",\".join(v))\n","36":"# Quick Sort\n\nfrom .helpers import ints\nfrom .par3 import par3\n\n\n# Quick Sort is very similar to our median algorithm: recursive partitioning.\ndef qs(A):\n    def qsr(A, low, high):\n        s, e = par3(A, low, high)\n        if low < s:\n            qsr(A, low, s)\n        if high > e:\n            qsr(A, e + 1, high)\n\n    qsr(A, 0, len(A) - 1)\n\n\ndef main(file):\n    handle = open(file)\n    _ = next(handle)\n    A = ints(next(handle))\n    qs(A)\n    print(*A)\n","37":"# FASTQ format introduction\n\nfrom Bio import SeqIO\nimport sys\n\n\ndef main(file):\n    for rcrd in SeqIO.parse(file, \"fastq\"):\n        SeqIO.write(rcrd, sys.stdout, \"fasta\")\n","38":"# Perform a Multiple Sequence Alignment with a Profile HMM\n\n# I'm not 100% sure about the final alignment score calculated here\n# but the optimum path appears correct.\n\nfrom collections import defaultdict\nimport numpy as np\nfrom .ba10e import state_labels\nfrom .ba10f import pseudocount_profile_hmm\nfrom math import inf, log\n\n\ndef parse_input(handle):\n    seq = next(handle).rstrip()\n    next(handle)\n    \u03b8, \u03c3 = map(float, next(handle).rstrip().split())\n    next(handle)\n    alphabet = next(handle).split()\n    next(handle)\n    alignment = np.array([list(x) for x in handle.read().splitlines()])\n    return seq, \u03b8, \u03c3, alphabet, alignment\n\n\ndef tprob_as_dict(x):\n    g = defaultdict(float)\n    n = (x.shape[0] - 3) \/\/ 3\n    lab = state_labels(n)\n    for i in range(x.shape[0]):\n        for j in range(x.shape[0]):\n            g[lab[i], lab[j]] = x[i][j]\n    return g\n\n\ndef eprob_as_dict(x, alphabet):\n    g = defaultdict(float)\n    n = (x.shape[0] - 3) \/\/ 3\n    lab = state_labels(n)\n    for i in range(x.shape[0]):\n        for j, c in enumerate(alphabet):\n            g[lab[i], c] = x[i][j]\n    return g\n\n\n# Build the weighted HMM graph based on transition matrix\ndef hmm_graph(tm, n):\n    def add_node(a, b):\n        g[a].append({\"n\": b, \"w\": tm[a, b]})\n\n    g = defaultdict(list)\n    for b in [\"I0\", \"M1\", \"D1\"]:\n        add_node(\"S\", b)\n    for i in range(n):\n        a = f\"I{i}\"\n        for b in [a, f\"M{i+1}\", f\"D{i+1}\"]:\n            add_node(a, b)\n    for i in range(1, n):\n        for a in [f\"M{i}\", f\"D{i}\"]:\n            for b in [f\"M{i+1}\", f\"I{i}\", f\"D{i+1}\"]:\n                add_node(a, b)\n    for a in [f\"I{n}\", f\"M{n}\", f\"D{n}\"]:\n        for b in [f\"I{n}\", \"E\"]:\n            add_node(a, b)\n\n    return g\n\n\n# Topological order (based on Figure 10.22)\n# yields node and column as a tuple\n# n: number of valid columns in alignment\n# m: sequence length\ndef topological_order(n, m):\n    yield (\"S\", 0)\n    for j in range(n):\n        yield (f\"D{j+1}\", 0)\n    for i in range(m):\n        yield (\"I0\", i + 1)\n        for j in range(n):\n            for c in [\"M\", \"D\", \"I\"]:\n                yield (f\"{c}{j+1}\", i + 1)\n    yield (\"E\", i + 2)\n\n\n# Function to find previous node and column possibilities based on current node\n# and column (see figure 10.21)\ndef prev_nodes(node, col, n, m):\n    if node[0] == \"E\":\n        return [(f\"D{n}\", m), (f\"M{n}\", m), (f\"I{n}\", m)]\n    i = int(node[1:])\n    if col == 0:\n        if i == 1:\n            return [(\"S\", 0)]\n        else:\n            return [(f\"D{i-1}\", 0)]\n    elif node == \"I0\":\n        if col == 1:\n            return [(\"S\", 0)]\n        else:\n            return [(\"I0\", col - 1)]\n    elif node == \"M1\":\n        if col == 1:\n            return [(\"S\", 0)]\n        else:\n            return [(\"I0\", col - 1)]\n    elif node[0] == \"I\":\n        if col == 1:\n            return [(f\"D{i}\", 0)]\n        else:\n            return [(f\"D{i}\", col - 1), (f\"M{i}\", col - 1), (f\"I{i}\", col - 1)]\n    elif node[0] == \"M\":\n        if col == 1:\n            return [(f\"D{i-1}\", 0)]\n        else:\n            return [(f\"D{i-1}\", col - 1), (f\"M{i-1}\", col - 1), (f\"I{i-1}\", col - 1)]\n    elif node[0] == \"D\":\n        if i == 1:\n            return [(\"I0\", col)]\n        else:\n            return [(f\"D{i-1}\", col), (f\"M{i-1}\", col), (f\"I{i-1}\", col)]\n    else:\n        print(f\"{node} not handled!\")\n\n\n# Convert structure like {n: [{'n': n2, 'w': w}]} to {n: {n2: w}}\ndef process_graph(graph):\n    return {k: {x[\"n\"]: x[\"w\"] for x in v} for k, v in graph.items()}\n\n\ndef main(file):\n    seq, \u03b8, \u03c3, alphabet, alignment = parse_input(open(file))\n    tprob, eprob = pseudocount_profile_hmm(\u03b8, \u03c3, alphabet, alignment)\n    n = (tprob.shape[0] - 3) \/\/ 3\n    tprob = tprob_as_dict(tprob)\n    eprob = eprob_as_dict(eprob, alphabet)\n\n    graph = hmm_graph(tprob, n)\n    order = topological_order(n, len(seq))\n    graph = process_graph(graph)\n\n    # results will be a dict indexed by tuple (node and column) pointing to score\n    # ptr will be indexed by same tuple and point to previous node\n    prev = next(order)\n    results = {prev: 0}\n    ptr = {prev: (None, None)}\n    for node, col in order:\n        ptr[(node, col)] = 0\n        results[(node, col)] = -inf\n        for pnode, pcol in prev_nodes(node, col, n, len(seq)):\n            if pcol < col and node != \"E\":\n                emission = eprob[node, seq[col - 1]]\n            else:\n                emission = 1\n            prob = log(graph[pnode][node]) + log(emission) + results[(pnode, pcol)]\n            if prob > results[(node, col)]:\n                results[(node, col)] = prob\n                ptr[(node, col)] = (pnode, pcol)\n\n    # traceback\n    path = []\n    pos = (\"E\", len(seq) + 1)\n    while pos[0]:\n        path += [ptr[pos][0]]\n        pos = ptr[pos]\n\n    print(*path[::-1][2:])\n","39":"# Creating a Distance Matrix\n\nfrom .helpers import Parser\nfrom .hamm import hamm\n\n\ndef pdst(seqs):\n    \"\"\"Creating a Distance Matrix\"\"\"\n    n = len(seqs)\n    d = [[0.0 for x in range(n)] for y in range(n)]\n    for i in range(n):\n        for j in range(i + 1, n):\n            d[i][j] = d[j][i] = hamm(seqs[i], seqs[j]) \/ len(seqs[i])\n    return d\n\n\ndef main(file):\n    for r in pdst(Parser(file).seqs()):\n        print(*[f\"{x:.5f}\" for x in r])\n","40":"# Introduction to Pattern Matching\n\nfrom .helpers import Parser\nfrom itertools import groupby\n\n\ndef trie(seqs):\n    graph = {}\n    if len(seqs):\n        for base, nseqs in groupby(sorted(seqs), key=lambda s: s[0]):\n            graph[base] = trie([seq[1:] for seq in nseqs if len(seq) > 1])\n    return graph\n\n\ndef as_adjacency(graph, nodes=[]):\n    node = len(nodes) + 1\n    for edge in sorted(graph):\n        i = len(nodes) + 1\n        nodes.append((node, i + 1, edge))\n        nodes + as_adjacency(graph[edge], nodes)\n    return nodes\n\n\ndef main(file):\n    seqs = Parser(file).lines()\n    for edge in as_adjacency(trie(seqs)):\n        print(*edge)\n","41":"# Implement DistanceBetweenPatternAndStrings\n\nfrom .ba2b import distance_between_pattern_and_strings\n\n\ndef main(file):\n    pattern, dna = open(file).read().splitlines()\n    print(distance_between_pattern_and_strings(pattern, dna.split()))\n","42":"# Implement 2-BreakOnGenome\n\nfrom .ba6a import read_perm, format_perm\nfrom .ba6h import colored_edges\nfrom .ba6j import two_break_on_genome_graph\nfrom .ba6i import graph2genome\n\n\ndef ints(x):\n    return list(map(int, x.split(\", \")))\n\n\n# we don't exactly follow the pseudocode from rosalind here...\ndef two_break_on_genome(P, i, ip, j, jp):\n    genome_graph = colored_edges([P])\n    genome_graph = two_break_on_genome_graph(genome_graph, i, ip, j, jp)\n    return graph2genome(genome_graph)\n\n\ndef main(file):\n    genome, ind = open(file).read().splitlines()\n    genome = read_perm(genome)\n    new = two_break_on_genome(genome, *ints(ind))\n    print(*[format_perm(x) for x in new])\n","43":"# Find a Profile-most Probable k-mer in a String\n\nfrom .ba1a import substrings\nimport math\n\n\ndef pmpkmer(text, k, profile):\n    base = {\"A\": 0, \"C\": 1, \"G\": 2, \"T\": 3}\n    prob = -1\n    for x in substrings(text, k):\n        p = math.prod(profile[base[x[j]]][j] for j in range(k))\n        if p > prob:\n            prob, best = p, x\n    return best\n\n\ndef main(file):\n    text, k, *profile = open(file).read().splitlines()\n    profile = [list(map(float, x.split())) for x in profile]\n    print(pmpkmer(text, int(k), profile))\n","44":"# Enumerating Gene Orders\n\nfrom itertools import permutations\nfrom .helpers import Parser\n\n\ndef main(file):\n    n = Parser(file).ints()[0]\n    perm = list(permutations(range(1, n + 1)))\n    print(len(perm))\n    for i in perm:\n        print(*i)\n","45":"# Wright-Fisher's Expected Behavior\n\nfrom .helpers import Parser\n\n\ndef main(file):\n    l1, l2 = Parser(file).lines()\n    n = int(l1)\n    s2 = list(map(float, l2.split()))\n    print(*[round(x * n, 3) for x in s2])\n","46":"# Find the Shortest Non-Shared Substring of Two Strings\n\n# We have to be careful here as the title is misleading. We actually need\n# The shortest substring of **Text1** that does not appear in **Text2**\n#\n# The logic here is that we can find the shortest a non-shared sequence by\n# finding the sequence from the root up to a non-purple node.\n# For the shortest non-shared sequence, we don't need to consider nodes\n# deeper in the tree and we also only need the first character of the final\n# edge to the non-purple node (since as soon as we add this character, we\n# have a subsequence unique to one of the sequences).\n#\n# Since we're only interested in subsequences present in Text1, we condition\n# on our final node being red.\n\n\nfrom rosalind.bioinformatics_stronghold.suff import suff\nfrom .ba9e import as_graph, color_tree, leaf_colors\n\n\ndef non_purple_edges(tree, colors):\n    def _non_purple_edges(node, seq, path):\n        if colors[node] == \"purple\":\n            for child in tree[node]:\n                yield from _non_purple_edges(\n                    child[\"n\"], seq + [child[\"l\"]], path + [node]\n                )\n        else:\n            if colors[node] == \"red\":\n                seq[-1] = seq[-1][0]\n                seq = \"\".join(seq)\n                if \"#\" not in seq and \"$\" not in seq:\n                    yield seq\n\n    yield from _non_purple_edges(0, [], [])\n\n\ndef shortest_nonshared_substring(seq1, seq2):\n    tree = suff(seq1 + \"$\" + seq2 + \"#\")\n    tree = as_graph(tree)\n    colors = color_tree(tree, leaf_colors(tree))\n    return min(non_purple_edges(tree, colors), key=lambda x: len(x))\n\n\ndef main(file):\n    seq1, seq2 = open(file).read().splitlines()\n    print(shortest_nonshared_substring(seq1, seq2))\n","47":"# Building a Heap\n\nfrom .helpers import ints\n\n\ndef heapify(heap, i):\n    if i == 0:\n        return\n    parent = (i - 1) \/\/ 2\n    child = i\n    if heap[parent] > heap[child]:\n        heapify(heap, parent)\n    else:\n        heap[parent], heap[child] = heap[child], heap[parent]\n        heapify(heap, parent)\n\n\ndef heap(arr):\n    heap = []\n    for i, x in enumerate(arr):\n        heap.append(x)\n        heapify(heap, i)\n    return heap\n\n\ndef main(file):\n    _, arr = open(file).read().splitlines()\n    print(*heap(ints(arr)))\n","48":"# Find a Highest-Scoring Peptide in a Proteome against a Spectrum\n\nfrom .ba11c import masses\nfrom math import inf\nfrom .ba11c import peptide2vector\n\n\n# This is a brute force scan through all peptides in the proteome\n# We can move to the next start position once our peptide candidate mass\n# exceeds that implied by the spectral vector (i.e. its length)\ndef peptide_identification(sv, proteome):\n    m = masses()\n    best_score = -inf\n    best_peptide = \"\"\n    for i in range(len(proteome)):\n        for j in range(i + 1, len(proteome)):\n            pv = peptide2vector(proteome[i:j], m)\n            if len(pv) > len(sv):\n                break\n            if len(pv) == len(sv):\n                score = sum(a * b for a, b in zip(pv, sv))\n                if score > best_score:\n                    best_score = score\n                    best_peptide = proteome[i:j]\n    return best_peptide, best_score\n\n\ndef main(file):\n    sv, seq = open(file).read().splitlines()\n    sv = list(map(int, sv.split()))\n    peptide, _ = peptide_identification(sv, seq)\n    print(peptide)\n","49":"# Read Filtration by Quality\n\nfrom Bio import SeqIO\n\n\ndef ints(x):\n    return list(map(int, x.split()))\n\n\ndef good_quality(q, p, seq):\n    quals = seq.letter_annotations[\"phred_quality\"]\n    return sum(x >= q for x in quals) \/ len(quals) >= p \/ 100\n\n\ndef main(file):\n    handle = open(file)\n    q, p = ints(next(handle))\n    print(sum(good_quality(q, p, x) for x in SeqIO.parse(handle, \"fastq\")))\n","50":"# Construct the Overlap Graph of a Collection of k-mers\n\nfrom itertools import permutations\n\n\ndef overlap_graph(dna):\n    \"\"\"\n    Naive algorithm that finds overlaps by considering all pairs\n    \"\"\"\n    for pair in permutations(dna, 2):\n        if pair[0].endswith(pair[1][:-1]):\n            yield (pair[0], pair[1])\n\n\ndef main(file):\n    dna = open(file).read().splitlines()\n    for a, b in overlap_graph(dna):\n        print(a + \" -> \" + b)\n","51":"# Expected Number of Restriction Sites\n\nfrom .helpers import Parser\n\n\ndef eval(n, s, x):\n    \"\"\"Expected Number of Restriction Sites\"\"\"\n    gc = sum([s.count(x) for x in \"GC\"])\n    p = ((1 - x) \/ 2) ** (len(s) - gc) * (x \/ 2) ** gc\n    return p * (n - len(s) + 1)\n\n\ndef main(file):\n    n, s, a = Parser(file).lines()\n    n = int(n)\n    a = map(float, a.split())\n    print(*[round(eval(n, s, x), 3) for x in a])\n","52":"# Compute the Probability of a Hidden Path\n\nfrom math import prod\n\n\ndef parse_input(handle):\n    seq = next(handle).rstrip()\n    next(handle)\n    states = next(handle).split()\n    next(handle)\n    tmat = dict(\n        ((states[i], states[j]), float(v))\n        for i, x in enumerate(handle.read().splitlines()[1:])\n        for j, v in enumerate(x.split()[1:])\n    )\n    return seq, tmat\n\n\ndef ba10a(seq, tmat):\n    return 0.5 * prod(tmat[x] for x in zip(seq, seq[1:]))\n\n\ndef main(file):\n    pr = ba10a(*parse_input(open(file)))\n    print(f\"{pr:.11e}\")\n","53":"# Bellman-Ford Algorithm\n\nfrom .helpers import parse_graph\nfrom math import inf, isinf\n\n\ndef nedges(graph):\n    count = 0\n    for _, outs in graph.items():\n        for _ in outs:\n            count += 1\n    return count\n\n\ndef nodes(graph):\n    s = list(graph.keys())\n    e = [y[\"n\"] for v in graph.values() for y in v]\n    return set(s) | set(e)\n\n\ndef bf(graph, start=1):\n    d = {n: inf for n in nodes(graph)}\n    d[start] = 0\n    for _ in range(nedges(graph) - 1):\n        for u, x in graph.items():\n            for v in x:\n                if d[u] + v[\"w\"] < d[v[\"n\"]]:\n                    d[v[\"n\"]] = d[u] + v[\"w\"]\n    return [\"x\" if isinf(v) else v for k, v in d.items()]\n\n\ndef main(file):\n    print(*bf(parse_graph(open(file), directed=True, weighted=True)))\n","54":"# Overlap Graphs\n\nfrom .helpers import Parser\nfrom itertools import permutations\n\n\ndef overlap_graph(seqs, n=3):\n    for pair in permutations(seqs, 2):\n        if pair[0].seq.endswith(pair[1].seq[:n]):\n            yield (pair[0].id, pair[1].id)\n\n\ndef main(file):\n    fa = Parser(file).fastas()\n    for i in overlap_graph(fa):\n        print(*i)\n","55":"# Compute the Number of Peptides of Given Total Mass\n\nfrom functools import cache\nfrom .ba4c import mass\n\n# The total number of ways to create a peptide of mass e.g. 1024 can be\n# calculated as the sum over ways to create a peptide of mass (1024-aa) for each\n# aa in the list of amino acid masses. These sub masses then be calculated\n# recursively...\n\n# Note that for the possible amino acid matches, we use the *set* of masses (not\n# all masses, which differs because some amino acids have the same mass). This\n# effectively means we do not count multiple solutions with different amino\n# acids of the same weight! This is IMO a bug in rosalind as these should be\n# counted.\n\n\ndef n_peptides(m):\n    @cache\n    def nways(m):\n        if m < 0:\n            return 0\n        if m == 0:\n            return 1\n        return sum(nways(m - k) for k in masses)\n\n    masses = set(mass().values())\n    return nways(m)\n\n\ndef main(file):\n    print(n_peptides(int(open(file).read())))\n","56":"# Negative Weight Cycle\n\nfrom .bf import nodes, nedges\nfrom .helpers import parse_graphs\n\n\ndef nwc(graph):\n    dist = {n: 10**20 for n in nodes(graph)}\n    dist[1] = 0\n    for _ in range(nedges(graph) - 1):\n        for u, x in graph.items():\n            for v in x:\n                dist[v[\"n\"]] = min(dist[u] + v[\"w\"], dist[v[\"n\"]])\n    for u, x in graph.items():\n        for v in x:\n            if dist[u] + v[\"w\"] < dist[v[\"n\"]]:\n                return 1\n    return -1\n\n\ndef main(file):\n    graphs = parse_graphs(open(file), directed=True, weighted=True)\n    print(*[nwc(g) for g in graphs])\n","57":"# Comparing Spectra with the Spectral Convolution\n\nfrom collections import Counter\nfrom .helpers import Parser\n\n\ndef conv(s1, s2):\n    \"\"\"Comparing Spectra with the Spectral Convolution\"\"\"\n    x = sorted([round(i - j, 5) for i in s1 for j in s2])\n    return Counter(x).most_common()[0]\n\n\ndef main(file):\n    l1, l2 = Parser(file).lines()\n    s1 = list(map(float, l1.split()))\n    s2 = list(map(float, l2.split()))\n    res = conv(s1, s2)\n    print(res[1], res[0], sep=\"\\n\")\n","58":"# Find Patterns Forming Clumps in a String\n\nfrom collections import defaultdict\nfrom .ba1a import substrings\n\n\n# find (0-based) positions of k-length kmers within text\ndef find_kmers(text, k):\n    x = defaultdict(list)\n    for i, t in enumerate(substrings(text, k)):\n        x[t] += [i]\n    return x\n\n\n# Do a given array of kmers at positions p form a clump of t within L\n# given that each is k-long.\ndef has_clump(p, L, t, k):\n    for i in range(len(p) - t + 1):\n        if (p[i + t - 1] + k - p[i]) <= L:\n            return True\n    return False\n\n\ndef main(file):\n    seq, ints = open(file).read().splitlines()\n    k, L, t = list(map(int, ints.split()))\n    kmers = find_kmers(seq, k)\n    print(*[x for x in kmers if has_clump(kmers[x], L, t, k)])\n","59":"# Inferring mRNA from Protein\n\nfrom functools import reduce\nfrom .helpers import Parser\nfrom rosalind.helpers import codons\n\n\ndef mrna(seq, mod=1000000):\n    cod = codons()\n    seq = seq + \"*\"\n    return reduce(lambda p, c: p * cod[c] % mod, seq, 1)\n\n\ndef main(file):\n    print(mrna(Parser(file).line()))\n","60":"# Merge Sort\n\nfrom .mer import mer\nfrom .helpers import ints\n\n\ndef ms(a):\n    if len(a) > 1:\n        mid = len(a) \/\/ 2\n        return mer(ms(a[:mid]), ms(a[mid:]))\n    else:\n        return a\n\n\ndef main(file):\n    _, a = open(file).read().splitlines()\n    print(*ms(ints(a)))\n","61":"# Construct the Graph of a Spectrum\n\nfrom collections import defaultdict\nfrom .ba4c import mass\n\n\ndef spectrum_graph(x):\n    m = {v: k for k, v in mass().items()}\n    g = defaultdict(list)\n    for i in range(len(x)):\n        for j in range(i + 1, len(x)):\n            d = x[j] - x[i]\n            if d in m:\n                g[x[i]].append({\"n\": x[j], \"l\": m[d]})\n    return g\n\n\ndef main(file):\n    x = [0] + list(map(int, open(file).read().split()))\n    for k, v in spectrum_graph(x).items():\n        for v2 in v:\n            print(f\"{k}->{v2['n']}:{v2['l']}\")\n","62":"# Implement ChromosomeToCycle\n\nfrom .ba6a import read_perm, format_perm\n\n\ndef cycle2chromosome(cycle):\n    nodes = []\n    for j1, j2 in zip(cycle[::2], cycle[1::2]):\n        if j1 < j2:\n            nodes += [j2 \/\/ 2]\n        else:\n            nodes += [-j1 \/\/ 2]\n    return nodes\n\n\ndef main(file):\n    s = open(file).read().rstrip()\n    chrom = cycle2chromosome(read_perm(s))\n    print(format_perm(chrom))\n","63":"# Creating a Restriction Map\n\nfrom .helpers import Parser\n\n# The logic here is that, if we start at zero, the maximum of the differences\n# observed in a difference multiset (ms), must exist as a number in the set x\n# if we are starting at 0. Equally, all numbers of x must exist somewhere in the\n# multiset (as differences from 0). So we can iterate through the multiset to\n# find them.\n#\n# Since difference between all pairs of values in x must exist in the multiset,\n# we can find a new value for x (from the multiset) by looking to see whether\n# all differences between the new value (n) and all other known x values exist\n# in the multiset.\n\n\ndef pdpl(ms):\n    \"\"\"Creating a Restriction Map\"\"\"\n    x = [0, max(ms)]\n    for n in set(ms):\n        diffs = [abs(n - member) for member in x]\n        if set(diffs).issubset(set(ms)):\n            x.append(n)\n            for d in diffs:\n                ms.remove(d)\n\n    return sorted(x)\n\n\ndef main(file):\n    print(*pdpl(Parser(file).ints()))\n","64":"# Genome Assembly with Perfect Coverage and Repeats\n\nfrom functools import cache\nfrom .helpers import Parser\nfrom .dbru import dbru\n\n\ndef drop_edge(edges, edge):\n    g = list(edges)\n    g.remove(edge)\n    return tuple(g)\n\n\n@cache\ndef find_paths(edges, key, assembly):\n    if len(edges) == 0:\n        yield assembly[: -(len(key) + 1)]\n    else:\n        opts = [b for a, b in edges if a == key]\n        for nkey in opts:\n            new = drop_edge(edges, (key, nkey))\n            yield from find_paths(new, nkey, assembly + nkey[-1])\n\n\ndef grep(seqs):\n    \"\"\"Genome Assembly with Perfect Coverage and Repeats\"\"\"\n    db = tuple(dbru(seqs, rev=False))\n    res = set(find_paths(db, seqs[0][1:], seqs[0]))\n    return sorted(res)\n\n\ndef main(file):\n    seqs = Parser(file).lines()\n    res = sorted(grep(seqs))\n    print(\"\\n\".join(res))\n","65":"# Counting Quartets\n\nfrom math import comb\n\n\ndef main(file):\n    n = int(open(file).readline())\n    print(comb(n, 4) % 1000000)\n","66":"# Mendel's First Law\n\nfrom .helpers import Parser\nfrom math import comb\n\n\ndef iprb(k, m, n):\n    tot = comb(k + m + n, 2)\n    poss = comb(k, 2) + k * m + k * n + m * n \/ 2 + comb(m, 2) * 3 \/ 4\n    return poss \/ tot\n\n\ndef main(file):\n    print(round(iprb(*Parser(file).ints()), 5))\n","67":"# Finding a Motif in DNA\n\nfrom .helpers import Parser\n\n\ndef find_motif(seq1, seq2):\n    size = len(seq2)\n    for i in range(len(seq1) - size + 1):\n        if seq1[i : (i + size)] == seq2:\n            yield i + 1\n\n\ndef main(file):\n    s1, s2 = Parser(file).lines()\n    print(*list(find_motif(s1, s2)))\n","68":"# Reconstruct a String from its Genome Path\n\n\ndef genome_path(dna):\n    return dna[0] + \"\".join(x[-1] for x in dna[1:])\n\n\ndef main(file):\n    dna = open(file).read().splitlines()\n    print(genome_path(dna))\n","69":"# Distances in Trees\n\nimport re\nfrom collections import defaultdict\nfrom math import inf\nfrom heapq import heappop, heappush\nfrom .helpers import nodes\n\n\n# we will push and pop from a stack of nodes as we work through the\n# newick tree backwards (backwards because we can read node names before we\n# need to create them!).\n#\n# \"(\" corresponds to ending a group of nodes (when working backwards)\n# - create an edge from parent to node (move out)\n# - move current node to parent and parent to parent's parent\n#\n# \")\" corresponds to starting a group of nodes:\n# - set parent to be current node (move in)\n# - read node name or take next available integer\n# - create a new node with no children\n#\n# A non-comma token corresponds to a node name\n# If it doesn't directly follow a \")\" it is a terminal node\n# - append to current nodes children\n#\n# We will create an undirected graph (connecting children to parents)\n# and remove the root when we're done (to facilitate a search for shortest\n# path...)\ndef parse_newick(newick, directed=True):\n    newick = re.sub(\",,\", \",.,\", newick)\n    newick = re.sub(r\"\\(,\", \"(.,\", newick)\n    newick = re.sub(r\",\\)\", \",.)\", newick)\n    newick = re.sub(r\"\\(\\)\", \"(.)\", newick)\n    newick = re.sub(r\"^\\((.+)\\);\", r\"\\1\", newick)\n    m = re.finditer(r\"(\\(|[A-z_.]+|,|\\))\", newick)\n    tokens = [x.group() for x in m]\n\n    count = 0\n    node_stack = [\"0\"]\n    g = defaultdict(list)\n    i = len(tokens) - 1\n    while i >= 0:\n        if tokens[i] == \"(\":\n            node_stack = node_stack[:-1]\n        elif tokens[i] == \")\":\n            if i + 1 < len(tokens) and tokens[i + 1] not in \",)\":\n                node = tokens[i + 1]\n            else:\n                count += 1\n                node = str(count)\n            g[node_stack[-1]].append({\"n\": node, \"w\": 1})\n            if not directed:\n                g[node].append({\"n\": node_stack[-1], \"w\": 1})\n            node_stack += [node]\n        elif tokens[i] != \",\" and (i == 0 or tokens[i - 1] != \")\"):\n            if tokens[i] == \".\":\n                count += 1\n                tokens[i] = str(count)\n            g[node_stack[-1]].append({\"n\": tokens[i], \"w\": 1})\n            if not directed:\n                g[tokens[i]].append({\"n\": node_stack[-1], \"w\": 1})\n        i -= 1\n    return g\n\n\ndef dij(graph, start=1):\n    dist = {n: inf for n in nodes(graph)}\n    dist[start] = 0\n    q = []\n    heappush(q, (0, start))\n    processed = set()\n\n    while q:\n        u = heappop(q)[1]\n        processed.add(u)\n        for v in graph[u]:\n            if v[\"n\"] not in processed:\n                dist[v[\"n\"]] = min(dist[u] + v[\"w\"], dist[v[\"n\"]])\n                heappush(q, (dist[v[\"n\"]], v[\"n\"]))\n\n    return dist\n\n\ndef newick_dist(tree, nodes):\n    return dij(parse_newick(tree, directed=False), nodes[0])[nodes[1]]\n\n\ndef main(file):\n    contents = open(file).read().split(\"\\n\\n\")\n    if contents[-1] == \"\":\n        contents = contents[:-1]\n    trees = [x.split(\"\\n\") for x in contents]\n    print(*[newick_dist(tree[0], tree[1].split()) for tree in trees])\n","70":"# Implement NumberToPattern\n\n\ndef number_to_symbol(x):\n    return [\"A\", \"C\", \"G\", \"T\"][x]\n\n\ndef number_to_pattern(i, k):\n    if k == 1:\n        return number_to_symbol(i)\n    else:\n        pi, r = divmod(i, 4)\n        return number_to_pattern(pi, k - 1) + number_to_symbol(r)\n\n\ndef main(file):\n    index, k = open(file).read().splitlines()\n    print(number_to_pattern(int(index), int(k)))\n","71":"# Sorting by Reversals\n\nfrom .helpers import Parser\nfrom .rear import sort\n\n\ndef main(file):\n    s, t = Parser(file).lines()\n    s = list(map(int, s.split()))\n    t = list(map(int, t.split()))\n    nr, c = sort(s, t)\n    print(nr)\n    for r in c[0][\"p\"]:\n        print(*r)\n","72":"# Implement ConvolutionCyclopeptideSequencing\n\nfrom .ba4h import spectrum_convolution\nfrom .ba4g import leaderboard_cyclopeptide_sequencing\n\n\ndef convolution_cyclopeptide_sequencing(m, n, spectrum):\n    conv = spectrum_convolution(spectrum)\n    conv = [(k, v) for k, v in conv if 57 <= k <= 200]\n    masses = [k for k, v in conv if v >= conv[m - 1][1]]\n    return leaderboard_cyclopeptide_sequencing(spectrum, n, masses)\n\n\ndef main(file):\n    m, n, spectrum = open(file).read().splitlines()\n    spectrum = list(map(int, spectrum.split()))\n    p = convolution_cyclopeptide_sequencing(int(m), int(n), spectrum)\n    print(*p, sep=\"-\")\n","73":"# Implement GraphToGenome\n\nimport re\nfrom .ba6g import cycle2chromosome\nfrom .ba6a import format_perm\nfrom copy import copy\n\n\ndef ints(x):\n    return list(map(int, x.split(\", \")))\n\n\ndef first_key(g):\n    return list(g.keys())[0]\n\n\n# Find a single cycle from colored edges\ndef find_node_cycle(graph):\n    start = first_key(graph)\n    a = start\n    component = []\n    while graph:\n        b = graph.pop(a)\n        graph.pop(b)\n        n = b + 1 if b % 2 else b - 1\n        if n == start:\n            return [b] + component + [a]\n        component += [a, b]\n        a = n\n\n\n# find cycles in colored edges\n# to do this, we first make each edge \"undirected\"\ndef find_node_cycles(graph):\n    graph = copy(graph)\n    for k, v in list(graph.items()):\n        graph[v] = k\n    while graph:\n        yield find_node_cycle(graph)\n\n\ndef graph2genome(genome_graph):\n    graph = []\n    for nodes in find_node_cycles(genome_graph):\n        graph += [cycle2chromosome(nodes)]\n    return graph\n\n\ndef parse_edge_string(s):\n    g = {}\n    for x in re.findall(r\"\\((.+?)\\)\", s):\n        a, b = ints(x)\n        g[a] = b\n        # g[b] = a\n    return g\n\n\ndef main(file):\n    s = open(file).read().rstrip()\n    genome_graph = parse_edge_string(s)\n    print(*[format_perm(x) for x in graph2genome(genome_graph)], sep=\"\")\n","74":"# Find a Cyclic Peptide with Theoretical Spectrum Matching an Ideal Spectrum\n\nfrom .ba4c import mass, cyclo_spectrum\nfrom .ba1a import substrings\n\n\ndef expand(peptides, masses):\n    return [p + [x] for x in masses for p in list(peptides)]\n\n\ndef is_consistent(peptide, target):\n    m = [sum(x) for i in range(1, len(peptide) + 1) for x in substrings(peptide, i)]\n    return all(x in target for x in m)\n\n\ndef cyclopeptide_sequencing(target):\n    masses = set(mass().values())\n    peptides = [[]]\n    while len(peptides):\n        peptides = expand(peptides, masses)\n        for peptide in list(peptides):\n            if sum(peptide) == target[-1]:\n                if list(cyclo_spectrum(peptide)) == target:\n                    yield peptide\n                peptides.remove(peptide)\n            elif not is_consistent(peptide, target):\n                peptides.remove(peptide)\n\n\ndef main(file):\n    mass = open(file).read().split()\n    target = list(map(int, mass))\n    res = [\"-\".join(str(x) for x in p) for p in cyclopeptide_sequencing(target)]\n    print(*res)\n","75":"# Finding a Spliced Motif\n\nfrom .helpers import Parser\n\n\ndef matches(s1, s2):\n    i, j = 0, 0\n    while j < len(s2):\n        if s2[j] == s1[i]:\n            yield i + 1\n            j += 1\n        i += 1\n\n\ndef main(file):\n    s1, s2 = [x.seq for x in Parser(file).fastas()]\n    print(*list(matches(s1, s2)))\n","76":"# Reconstruct a String from its Burrows-Wheeler Transform\n\nfrom collections import defaultdict\n\n\ndef index_seq(seq):\n    d = defaultdict(int)\n    for c in seq:\n        yield c, d[c]\n        d[c] += 1\n\n\ndef bwtj(seq):\n    first = list(index_seq(sorted(seq)))\n    last = list(index_seq(seq))\n    curr = (\"$\", 0)\n    res = \"\"\n    for i in range(len(seq)):\n        curr = first[last.index(curr)]\n        res += curr[0]\n    return res\n\n\ndef main(file):\n    print(bwtj(open(file).read().rstrip()))\n","77":"# Rabbits and Recurrence Relations\n\nfrom .helpers import Parser\n\n\ndef fib(n, k):\n    a, b = 1, 1\n    for _ in range(2, n):\n        a, b = b, k * a + b\n    return b\n\n\ndef main(file):\n    print(fib(*Parser(file).ints()))\n","78":"# Compute the Hamming Distance Between Two Strings\n\n\ndef hamming(s1, s2):\n    return sum(s1[i] != s2[i] for i in range(len(s1)))\n\n\ndef main(file):\n    s1, s2 = open(file).read().splitlines()\n    print(hamming(s1, s2))\n","79":"# Compute the 2-Break Distance Between a Pair of Genomes\n\nimport re\nfrom collections import defaultdict\n\n\ndef find_component(node, graph):\n    q = [node]\n    visited = set()\n    while q:\n        node = q.pop(0)\n        visited.add(node)\n        for n in graph[node]:\n            if n not in visited:\n                q += [n]\n    return visited\n\n\n# In the format chosen here, I take a directed edge +x (e.g. \"+1\")\n# and assign the node at the \u201chead\u201d of this edge as -1 and the\n# node at the \u201ctail\u201d as +1.\ndef parse_genome_graph(s):\n    g = defaultdict(list)\n    for component in re.findall(r\"\\((.+?)\\)\", s):\n        component = list(map(int, component.split()))\n        for i in range(len(component) - 1):\n            g[component[i]] += [-component[i + 1]]\n            g[-component[i + 1]] += [component[i]]\n        g[component[-1]] += [-component[0]]\n        g[-component[0]] += [component[-1]]\n    return g\n\n\n# merge the graphs (assumes gp and gp have the same nodes)\ndef breakpoint_graph(p, q):\n    bg = {}\n    for node in p.keys():\n        bg[node] = p[node] + q[node]\n    return bg\n\n\ndef ba6c(genomes):\n    bg = breakpoint_graph(*genomes)\n    nodes = set(bg.keys())\n    n_blocks = len(nodes) \/\/ 2\n    n_components = 0\n    while len(nodes):\n        res = find_component(next(iter(nodes)), bg)\n        nodes = nodes - res\n        n_components += 1\n\n    return n_blocks - n_components\n\n\ndef main(file):\n    genomes = [parse_genome_graph(s) for s in open(file).read().splitlines()]\n    print(ba6c(genomes))\n","80":"# Generate the Theoretical Spectrum of a Cyclic Peptide\n\nimport yaml\nfrom importlib import resources\n\n\ndef mass():\n    path = resources.files(\"rosalind.resources\").joinpath(\"mass.yaml\")\n    with open(path) as stream:\n        return yaml.safe_load(stream)\n\n\ndef cyclo_spectrum(peptide):\n    spec = [0, sum(peptide)]\n    for i in range(1, len(peptide)):\n        for j in range(len(peptide)):\n            spec += [sum((peptide[j:] + peptide[:j])[0:i])]\n    return sorted(spec)\n\n\ndef main(file):\n    peptide = open(file).read().splitlines()[0]\n    m = mass()\n    print(*cyclo_spectrum([m[x] for x in peptide]))\n","81":"# Estimate the Parameters of an HMM\n\nfrom collections import defaultdict\nimport numpy as np\nfrom .ba10e import normalise, print_mat\n\n\ndef parse_input(handle):\n    seq = next(handle).rstrip()\n    next(handle)\n    alphabet = next(handle).split()\n    next(handle)\n    path = next(handle).rstrip()\n    next(handle)\n    states = next(handle).split()\n    return seq, alphabet, path, states\n\n\ndef as_dict(x, r, c):\n    g = defaultdict(float)\n    for i in range(x.shape[0]):\n        for j in range(x.shape[1]):\n            g[r[i], c[j]] = x[i][j]\n    return g\n\n\ndef estimate_tmat(path, states, to_dict=False):\n    tmat = np.zeros((len(states), len(states)), dtype=float)\n    for a, b in zip(path, path[1:]):\n        tmat[states.index(a)][states.index(b)] += 1\n    tmat = normalise(tmat, inc_zeros=True, min_val=1e-16)\n    if to_dict:\n        return as_dict(tmat, states, states)\n    else:\n        return tmat\n\n\ndef estimate_emat(seq, alphabet, path, states, to_dict=False):\n    emat = np.zeros((len(states), len(alphabet)), dtype=float)\n    for a, b in zip(path, seq):\n        emat[states.index(a)][alphabet.index(b)] += 1\n    emat = normalise(emat, inc_zeros=True, min_val=1e-16)\n    if to_dict:\n        return as_dict(emat, states, alphabet)\n    else:\n        return emat\n\n\ndef main(file):\n    seq, alphabet, path, states = parse_input(open(file))\n    tm = estimate_tmat(path, states)\n    em = estimate_emat(seq, alphabet, path, states)\n    print_mat(tm, states, states)\n    print(\"--------\")\n    print_mat(em, states, alphabet)\n","82":"# Transcribing DNA into RNA\n\nfrom .helpers import Parser\n\n\ndef main(file):\n    print(Parser(file).dna().rna())\n","83":"# Convert a Peptide into a Peptide Vector\n\nimport os\nfrom .ba4c import mass\n\n\n# masses with fake masses for tests if we're in test mode\ndef masses():\n    if \"ROSALIND_TEST\" in os.environ:\n        return {\"X\": 4, \"Z\": 5}\n    else:\n        return mass()\n\n\ndef peptide_mass(peptide, masses):\n    return sum(masses[x] for x in peptide)\n\n\ndef peptide2vector(peptide, masses):\n    vec = [0] * (peptide_mass(peptide, masses) + 1)\n    for i in range(len(peptide) + 1):\n        vec[peptide_mass(peptide[:i], masses)] = 1\n    return vec[1:]\n\n\ndef main(file):\n    peptide = open(file).read().rstrip()\n    print(*peptide2vector(peptide, masses()))\n","84":"# Connected Components\n\nfrom .helpers import parse_graph\n\n\ndef find_component(node, graph):\n    def visit(node, visited):\n        visited.add(node)\n        for n in list(set(graph[node]) - visited):\n            visit(n, visited)\n        return visited\n\n    return visit(node, set())\n\n\ndef find_components(graph):\n    nodes = set(graph.keys())\n    components = list()\n    while len(nodes):\n        res = find_component(next(iter(nodes)), graph)\n        nodes = nodes - res\n        components.append(res)\n    return components\n\n\ndef main(file):\n    graph = parse_graph(open(file))\n    print(len(find_components(graph)))\n","85":"# Implement the Viterbi Algorithm\n\nfrom math import log\nimport numpy as np\n\n\ndef parse_input(handle):\n    seq = next(handle).rstrip()\n    next(handle)\n    alphabet = next(handle).split()\n    next(handle)\n    states = next(handle).split()\n    next(handle)\n    lines = [next(handle) for _ in range(len(states) + 1)]\n    tmat = {\n        (states[i], states[j]): float(v)\n        for i, x in enumerate(lines[1:])\n        for j, v in enumerate(x.split()[1:])\n    }\n    next(handle)\n    lines = [next(handle) for i in range(len(states) + 1)]\n    emat = {\n        (states[i], alphabet[j]): float(v)\n        for i, x in enumerate(lines[1:])\n        for j, v in enumerate(x.split()[1:])\n    }\n    return seq, states, tmat, emat\n\n\ndef viterbi(seq, states, tmat, emat):\n    mat = np.zeros((len(seq), len(states)))\n    ptr = np.zeros((len(seq), len(states)), dtype=int)\n\n    # we assume starting in any state is equally likely\n    for i, state in enumerate(states):\n        mat[0, i] = log(emat[state, seq[0]] \/ len(states))\n\n    for i, emission in enumerate(seq[1:], start=1):\n        for j, state in enumerate(states):\n            opt = [\n                log(tmat[prev, state]) + log(emat[state, emission]) + mat[i - 1, k]\n                for k, prev in enumerate(states)\n            ]\n            p = opt.index(max(opt))\n            ptr[i, j] = p\n            mat[i, j] = max(opt)\n    ind = np.argmax(mat[i, :])\n\n    # traceback\n    state_seq = states[ind]\n    while i > 0:\n        state_seq = states[ptr[i, ind]] + state_seq\n        ind = ptr[i, ind]\n        i -= 1\n    return state_seq\n\n\ndef main(file):\n    seq, states, tmat, emat = parse_input(open(file))\n    print(viterbi(seq, states, tmat, emat))\n","86":"# Find a Middle Edge in an Alignment Graph in Linear Space\n\nfrom math import floor\nfrom .ba5e import blosum62\n\n\n# Calculate scores in linear space\n# This keeps a vector of scores scores for the current and previous alignemnt\n# columns (sc and psc). It also computes a single backtrack vector (bt)\n# corresponding to cell that each value of sc was computed from.\ndef calculate_scores(s1, s2, scores, penalty):\n    sc = list(range(0, (len(s1) + 1) * penalty, penalty))\n    bt = [0] * (len(s1) + 1)\n    for j in range(1, len(s2) + 1):\n        psc = sc[:]\n        sc[0] = psc[0] + penalty\n        for i in range(1, len(s1) + 1):\n            opt = [\n                psc[i] + penalty,\n                sc[i - 1] + penalty,\n                psc[i - 1] + scores[s1[i - 1]][s2[j - 1]],\n            ]\n            sc[i] = max(opt)\n            bt[i] = opt.index(sc[i])\n    return sc, bt\n\n\n# To find, the middle edge, we first find the highest scoring value of the\n# middle column, then find the edge from this node to the next.\ndef middle_edge(s1, s2, scores, penalty):\n    mid = floor((len(s2)) \/ 2)\n    sc1, _ = calculate_scores(s1, s2[:mid], scores, penalty)\n    sc2, bt2 = calculate_scores(s1[::-1], s2[mid::][::-1], scores, penalty)\n    total = [a + b for (a, b) in zip(sc1, sc2[::-1])]\n    best = total.index(max(total))\n    n1 = (best, mid)\n    moves = [(n1[0], n1[1] + 1), (n1[0] + 1, n1[1]), (n1[0] + 1, n1[1] + 1)]\n    n2 = moves[bt2[::-1][best]]\n    return (n1, n2)\n\n\ndef main(file):\n    s1, s2 = open(file).read().splitlines()\n    print(middle_edge(s1, s2, blosum62(), -5))\n","87":"# Find the Reverse Complement of a String\n\n\ndef revcomp(seq):\n    return seq[::-1].translate(str.maketrans(\"ACGT\", \"TGCA\"))\n\n\ndef main(file):\n    seq = open(file).read().splitlines()[0]\n    print(revcomp(seq))\n","88":"# Counting Disease Carriers\n\nfrom math import sqrt\nfrom .helpers import Parser\n\n\ndef afrq(a):\n    \"\"\"Counting Disease Carriers\"\"\"\n    return [2 * sqrt(x) - x for x in a]\n\n\ndef main(file):\n    b = afrq(Parser(file).floats())\n    print(*[round(x, 3) for x in b])\n","89":"# Find a Shortest Transformation of One Genome into Another by 2-Breaks\n\nfrom .ba6c import find_component, parse_genome_graph, breakpoint_graph\nfrom .ba6a import format_perm\n\n\ndef find_components(graph):\n    nodes = set(graph.keys())\n    while nodes:\n        res = find_component(next(iter(nodes)), graph)\n        nodes = nodes - res\n        yield res\n\n\ndef non_trivial_cycle_nodes(graph):\n    for c in find_components(graph):\n        if len(c) > 2:\n            return list(c)\n    return None\n\n\ndef find_genome_component(node, graph):\n    q = [node]\n    visited = list()\n    while q:\n        node = q.pop(0)\n        visited.append(node)\n        for n in graph[node]:\n            if -n not in visited:\n                q += [-n]\n    return visited\n\n\ndef format_genome_graph(g):\n    nodes = set(g.keys())\n    components = []\n    while nodes:\n        comp = find_genome_component(next(iter(nodes)), g)\n        nodes = nodes - set(comp)\n        nodes = nodes - set(-x for x in comp)\n        components += [comp]\n    x = [format_perm(c) for c in components]\n    return \"\".join(x)\n\n\ndef add_edge(g, i, j):\n    g[i] += [j]\n    g[j] += [i]\n\n\ndef del_edge(g, i, j):\n    g[i].remove(j)\n    g[j].remove(i)\n\n\ndef ba6d(P, Q):\n    bg = breakpoint_graph(P, Q)\n    nodes = non_trivial_cycle_nodes(bg)\n    yield format_genome_graph(P)\n    while nodes:\n        # arbitrary (first) edge from blue (q) edges in non-trivial cycle\n        j = nodes[0]\n        i2 = Q[nodes[0]][0]\n\n        # edge from ref edges linking to j\n        i = P[j][0]\n\n        # edge from ref edges linking to i2\n        j2 = P[i2][0]\n\n        # red (p) edges with (i,j) and (i2, j2) removed\n        del_edge(P, i, j)\n        del_edge(P, i2, j2)\n\n        # red (p) edges with (j, i2) and (j2, i) added\n        add_edge(P, j, i2)\n        add_edge(P, j2, i)\n\n        yield format_genome_graph(P)\n        bg = breakpoint_graph(P, Q)\n        nodes = non_trivial_cycle_nodes(bg)\n\n\ndef main(file):\n    P, Q = [parse_genome_graph(s) for s in open(file).read().splitlines()]\n    for g in ba6d(P, Q):\n        print(g)\n","90":"# Construct the Suffix Array of a String\n\n\ndef suffix_array(text):\n    seqs = dict((i, text[i:]) for i in range(len(text)))\n    return sorted(seqs.keys(), key=lambda x: seqs[x])\n\n\ndef main(file):\n    print(*suffix_array(open(file).read().rstrip()), sep=\", \")\n","91":"# Pairwise Global Alignment\n\nfrom Bio import SeqIO\nfrom .frmt import get_records\nimport sys\n\n\n# Here we just write fasta format to STDOUT, paste these into the Needle\n# web interface.\ndef main(file):\n    ids = open(file).read().split()\n    records = get_records(ids)\n    for rcrd in records:\n        SeqIO.write(rcrd, sys.stdout, \"fasta\")\n","92":"# Compute the Number of Breakpoints in a Permutation\n\nfrom .ba6a import read_perm\n\n\ndef nbreakpoints(perm):\n    perm = [0] + perm + [len(perm) + 1]\n    return sum(perm[i] + 1 != perm[i + 1] for i in range(len(perm) - 1))\n\n\ndef main(file):\n    s = open(file).read().rstrip()\n    print(nbreakpoints(read_perm(s)))\n","93":"# Local Alignment with Scoring Matrix\n\nfrom .helpers import Parser\nfrom rosalind.helpers import pam250\n\n\ndef loca(s1, s2, score, penalty):\n    m, p = {}, {}\n    for j in range(len(s2) + 1):\n        m[j, 0] = 0\n        p[j, 0] = \"\u2191\"\n    for i in range(len(s1) + 1):\n        m[0, i] = 0\n        p[0, i] = \"\u2190\"\n\n    m[0, 0] = 0\n    for j in range(len(s2)):\n        for i in range(len(s1)):\n            new = (j + 1, i + 1)\n            opt = [\n                m[j, i] + score[s1[i]][s2[j]],\n                m[j, i + 1] + penalty,\n                m[j + 1, i] + penalty,\n                0,\n            ]\n            m[new] = max(opt)\n            p[new] = [\"\u2196\", \"\u2191\", \"\u2190\", \"\u2196\"][opt.index(max(opt))]\n\n    max_score = max(x for x in m.values())\n    j, i = [k for k, v in m.items() if v == max_score][0]\n    a1, a2 = \"\", \"\"\n    while i > 0 or j > 0:\n        if m[j, i] == 0:\n            break\n        if p[j, i] == \"\u2196\":\n            a1 += s1[i - 1]\n            a2 += s2[j - 1]\n            j, i = j - 1, i - 1\n        elif p[j, i] == \"\u2190\":\n            a1 += s1[i - 1]\n            i = i - 1\n        elif p[j, i] == \"\u2191\":\n            a2 += s2[j - 1]\n            j = j - 1\n\n    return {\"dist\": max_score, \"a1\": a1[::-1], \"a2\": a2[::-1]}\n\n\ndef main(file):\n    seqs = Parser(file).seqs()\n    res = loca(seqs[0], seqs[1], pam250(), -5)\n    print(res[\"dist\"])\n    print(res[\"a1\"])\n    print(res[\"a2\"])\n","94":"# Compute the Size of a Spectral Dictionary\n\n\nfrom functools import cache\nfrom .ba11c import masses\n\n\ndef dict_size(sv, T, max_score):\n    @cache\n    def size(i, t):\n        if i == 0 and t == 0:\n            return 1\n        if t < 0 or i <= 0:\n            return 0\n        return sum(size(i - x, t - sv[i]) for x in mass)\n\n    mass = masses().values()\n    n = len(sv)\n    sv = [0] + sv\n    return sum(size(n, x) for x in range(T, max_score + 1))\n\n\ndef main(file):\n    sv, T, max_score = open(file).read().splitlines()\n    sv = list(map(int, sv.split()))\n    print(dict_size(sv, int(T), int(max_score)))\n","95":"# Dictionaries\n\nfrom collections import Counter\n\n\ndef main(file):\n    for k, v in Counter(open(file).read().split()).items():\n        print(k, v)\n","96":"# Implement GreedyMotifSearch\n\nfrom .ba1a import substrings\nfrom .ba2c import pmpkmer\nfrom collections import Counter\n\n\ndef create_profile(seqs, pc=0):\n    bases = [\"A\", \"C\", \"G\", \"T\"]\n    profile = [[] for b in bases]\n    for i, b in enumerate(bases):\n        profile[i] = [\n            (sum(x[j] == b for x in seqs) + pc) \/ len(seqs) for j in range(len(seqs[0]))\n        ]\n    return profile\n\n\ndef score(motifs):\n    score = 0\n    for i in range(len(motifs[0])):\n        bases = [x[i] for x in motifs]\n        c = Counter(bases).most_common()[0][0]\n        score += sum(x != c for x in bases)\n    return score\n\n\ndef greedy_motif_search(dna, k, pc=0):\n    best_motifs = [x[0:k] for x in dna]\n    for kmer in substrings(dna[0], k):\n        motifs = [kmer]\n        for i in range(1, len(dna)):\n            motifs += [pmpkmer(dna[i], k, create_profile(motifs, pc=pc))]\n        if score(motifs) < score(best_motifs):\n            best_motifs = motifs\n    return best_motifs\n\n\ndef main(file):\n    ints, *dna = open(file).read().splitlines()\n    k, t = map(int, ints.split())\n    print(*greedy_motif_search(dna, k), sep=\"\\n\")\n","97":"# Compute the Probability of an Outcome Given a Hidden Path\n\nfrom math import prod\n\n\ndef parse_input(handle):\n    seq = next(handle).rstrip()\n    next(handle)\n    alphabet = next(handle).split()\n    next(handle)\n    path = next(handle).rstrip()\n    next(handle)\n    states = next(handle).split()\n    next(handle)\n    tmat = {\n        (states[i], alphabet[j]): float(v)\n        for i, x in enumerate(handle.read().splitlines()[1:])\n        for j, v in enumerate(x.split()[1:])\n    }\n    return seq, path, tmat\n\n\ndef ba10b(seq, path, tmat):\n    return prod(tmat[x] for x in zip(path, seq))\n\n\ndef main(file):\n    pr = ba10b(*parse_input(open(file)))\n    print(f\"{pr:.11e}\")\n","98":"# Dijkstra's Algorithm\n\nfrom .helpers import parse_graph\n\nfrom math import inf\nfrom heapq import heappush, heappop\n\n\ndef dij(graph, start=1):\n    d = [inf for i in range(len(graph) + 1)]\n    d[start] = 0\n    q = []\n    heappush(q, (0, start))\n    processed = set()\n\n    while q:\n        u = heappop(q)[1]\n        processed.add(u)\n        for v in graph[u]:\n            if v[\"n\"] not in processed:\n                d[v[\"n\"]] = min(d[u] + v[\"w\"], d[v[\"n\"]])\n                heappush(q, (d[v[\"n\"]], v[\"n\"]))\n\n    return [-1 if x == inf else x for x in d[1:]]\n\n\ndef main(file):\n    graph = parse_graph(open(file), directed=True, weighted=True)\n    print(*dij(graph))\n","99":"# Mortal Fibonacci Rabbits\n\nfrom .helpers import Parser\n\n\ndef fibd(n, m):\n    v = [1] + (m - 1) * [0]\n    for i in range(2, n + 1):\n        v = [sum(v[1:])] + v[:-1]\n    return sum(v)\n\n\ndef main(file):\n    print(fibd(*Parser(file).ints()))\n","100":"# Compute the Number of Times a Pattern Appears in a Text\n\n\ndef substrings(text, size):\n    for i in range(len(text) - size + 1):\n        yield text[i : i + size]\n\n\ndef pattern_count(text, pattern):\n    return sum(pattern == x for x in substrings(text, len(pattern)))\n\n\ndef main(file):\n    text, pattern = open(file).read().splitlines()\n    print(pattern_count(text, pattern))\n","101":"# Finding Genes with ORFs\n\nfrom Bio.Seq import Seq\nimport re\n\n\ndef trim_seq(x):\n    return x[: (len(x) \/\/ 3 * 3)]\n\n\ndef translations(x):\n    for i in range(3):\n        x = trim_seq(x[i:])\n        yield str(x.translate())\n        yield str(x.reverse_complement().translate())\n\n\ndef main(file):\n    best = \"\"\n    seq = str(open(file).read().rstrip())\n    for x in translations(Seq(seq)):\n        matches = re.findall(r\"M[^\\*]+\", x)\n        best = max(matches + [best], key=len)\n    print(best)\n","102":"# Testing Bipartiteness\n\nfrom .helpers import parse_graphs\n\n\n# d stores the coloring of each node as 0 or 1\ndef bip(g):\n    d = {1: 0}\n    q = [1]\n    while q:\n        u = q.pop(0)\n        for v in g[u]:\n            col = (d[u] + 1) % 2\n            if v not in d:\n                q.append(v)\n                d[v] = col\n            elif d[v] != col:\n                return -1\n    return 1\n\n\ndef main(file):\n    print(*[bip(g) for g in parse_graphs(open(file))])\n","103":"# Implement AdditivePhylogeny\n\n# Hands down the worst code I've written in a long time.\n# TODO: refactor \/ rewrite!\n\nfrom collections import defaultdict\nfrom .ba7b import limb, parse_mat\nfrom .ba7a import nodes\n\n\ndef find_path(graph, path, target):\n    \"\"\"Search of tree, returning route to target and cumulative dist\"\"\"\n    if target in [x[0] for x in path]:\n        yield path\n    x = path[-1]\n    if x[0] in graph:\n        for node in graph[x[0]]:\n            if node not in path:\n                yield from find_path(\n                    graph, path + [(node[\"n\"], x[1] + node[\"w\"])], target\n                )\n\n\ndef find_leaves(D, n):\n    for i in range(len(D)):\n        for k in range(i + 1, len(D)):\n            if D[i][k] == D[i][n] + D[n][k]:\n                return i, n, k\n\n\n# There may be multiple nodes between i and k, so we need to find the\n# path from i to k (there's only 1) and break the appropriate edge.\ndef add_node(T, i, k, x, n):\n    \"\"\"Add node in graph D, between i and k, dist x from i, labelled n\"\"\"\n    path = list(find_path(T, [(i, 0)], k))[0]\n    for p, node in enumerate(path):\n        if node[1] > x:\n            break\n    p = p - 1\n    i, d1 = path[p]\n    k, d2 = path[p + 1]\n\n    # delete edge\n    T[i] = list(filter(lambda x: x[\"n\"] != k, T[i]))\n    T[i].append({\"n\": n, \"w\": x - d1})\n    T[n].append({\"n\": k, \"w\": d2 - x})\n    return T\n\n\ndef additive_phylogeny(D, m):\n    n = len(D) - 1\n    if len(D) == 2:\n        g = defaultdict(list)\n        g[0].append({\"n\": 1, \"w\": D[0][1]})\n        return g\n\n    limb_len = limb(D, n)\n    for j in range(len(D)):\n        if j != n:\n            D[j][n] = D[j][n] - limb_len\n            D[n][j] = D[j][n]\n\n    # three leaves such that Di,k = Di,n + Dn,k\n    (i, n, k) = find_leaves(D, n)\n    x = D[i][n]\n\n    # remove row n and column n from D\n    D = [x[:n] + x[n + 1 :] for x in D[:n] + D[n + 1 :]]\n\n    T = additive_phylogeny(D, m)\n\n    # label for new internal node\n    v = max(max(nodes(T)), m - 1) + 1\n\n    # break an internal edge adding a new node (possibly)\n    # and add the new leaf node\n    T = add_node(T, i, k, x, v)\n    T[v].append({\"n\": n, \"w\": limb_len})\n    return T\n\n\ndef as_edges(graph):\n    edges = []\n    for k in sorted(graph):\n        for v in graph[k]:\n            edges += [f\"{k}->{v['n']}:{v['w']}\"]\n            edges += [f\"{v['n']}->{k}:{v['w']}\"]\n    return sorted(edges)\n\n\ndef main(file):\n    n, *D = open(file).read().splitlines()\n    graph = additive_phylogeny(parse_mat(D), int(n))\n    for edge in as_edges(graph):\n        print(edge)\n","104":"# Calculating Expected Offspring\n\nfrom .helpers import Parser\n\n\ndef iev(v):\n    p = [1, 1, 1, 0.75, 0.5, 0]\n    return sum([x[0] * x[1] * 2 for x in zip(v, p)])\n\n\ndef main(file):\n    print(iev(Parser(file).ints()))\n","105":"# Implement the Soft k-Means Clustering Algorithm\n\nimport numpy as np\nfrom .ba8a import read_types, euclidean_distance\n\n\ndef partition_function(x, centers, \u03b2):\n    num = [np.exp(-\u03b2 * euclidean_distance(x, cen)) for cen in centers]\n    return np.array(num) \/ sum(num)\n\n\ndef hidden_matrix(data, centers, \u03b2):\n    return np.array([partition_function(x, centers, \u03b2) for x in data])\n\n\ndef soft_k_means(points, k, \u03b2, iter=20):\n    centers = np.array(points[:k])\n    points = np.array(points)\n    for _ in range(iter):\n        hm = hidden_matrix(points, centers, \u03b2)\n        centers = [np.dot(hm[:, i], points) for i in range(k)]\n        sums = np.sum(hm, 0)\n        centers = np.transpose(np.transpose(centers) \/ sums)\n    return centers\n\n\ndef main(file):\n    handle = open(file)\n    k, m = next(read_types(handle, int))\n    \u03b2 = next(read_types(handle, float))[0]\n    points = [np.array(point) for point in read_types(handle, float)]\n    for m in soft_k_means(points, k, \u03b2):\n        print(*[f\"{x:.3f}\" for x in m])\n","106":"# Solve the Turnpike Problem\n\n# ## PROBLEM\n#\n# We have a set of distances between positions\n# e.g. -10 -8 -7 -6 -5 -4 -3 -3 -2 -2 0 0 0 0 0 2 2 3 3 4 5 6 7 8 10\n\n# We we are trying to infer a set of positions that would be\n# consistent with this, e.g. 0 2 4 7 10\n\n# It is easy to infer distances from positions: all_differences([0, 2, 4, 7, 10])\n# but harder to do the reverse.\n\n# ## SOLUTION OUTLINE\n#\n# Note that:\n# * We can assume that the first position is at 0.\n#\n# * Logically, the maximum position must be the maximum of the distances (10)\n#\n# * We can ignore negative and 0-distances since all positions must be unique\n#   Thus, we will consider only the distances [2, 2, 3, 3, 4, 5, 6, 7, 8]\n#\n# * To find a solution, we can prune distances from this list while adding\n#   positions to our candidate list till there are no more distances.\n#\n# * When adding a candidate position, we must check that all distances between\n#   this position and our current list are within the known distances list. This\n#   can be accomplished by subtracting Counter objects: xdist - dist.\n#\n# * To proceed we note that the next largest value above is 7. So either, 7\n#   exists as a position, or, since we know we have 10 in our list, we may have\n#   1 as a position.\n#\n# * We consider both possibilities, check if the distances implied by adding\n#   this are consistent with the known differences, and if so, recurse.\n#\n# * In our first iteration, a position of 8 is consistent, since we have\n#   differences 8-0 = 8 and 10-8 = 2 in our known list. It could also be 2.\n#\n#   In our next iteration, we cannot have 7 as a position, since this would\n#   imply our difference list would contain 10 - 7 == 3, which it does not...\n\nfrom collections import Counter\nfrom itertools import product\n\n\ndef all_differences(pos):\n    return sorted([a - b for a, b in product(pos, repeat=2)])\n\n\ndef difference(pos, d):\n    return Counter(abs(x - d) for x in pos)\n\n\ndef turnpike(dist, pos):\n    if not dist:\n        yield sorted(pos)\n    else:\n        for x in [max(dist), max(pos) - max(dist)]:\n            xdist = difference(pos, x)\n            if not (xdist - dist):\n                yield from turnpike(dist - xdist, pos + [x])\n\n\ndef main(file):\n    dist = open(file).read().split()\n    dist = list(map(int, dist))\n    pos = [0, max(dist)]\n    dist = Counter(filter(lambda x: x > 0, dist[:-1]))\n    # for solution in turnpike(dist, pos):\n    #     print(*solution)\n    print(*next(turnpike(dist, pos)))\n","107":"# Find All Occurrences of a Pattern in a String\n\nfrom .ba1a import substrings\n\n\ndef pattern_find(text, pattern):\n    for i, x in enumerate(substrings(text, len(pattern))):\n        if x == pattern:\n            yield i\n\n\ndef main(file):\n    pattern, text = open(file).read().splitlines()\n    print(*pattern_find(text, pattern))\n","108":"# k-Mer Composition\n\nfrom .helpers import Parser\nfrom itertools import product\n\n\ndef kmer_perm(k):\n    set = [\"A\", \"C\", \"G\", \"T\"]\n    perm = list(product(set, repeat=k))\n    return [\"\".join(x) for x in perm]\n\n\ndef main(file):\n    \"\"\"k-Mer Composition\"\"\"\n    seq = Parser(file).fastas()[0].seq\n\n    # initialise hash with all possible 4-mers permutations\n    d = {k: 0 for k in kmer_perm(4)}\n\n    # Run through 4-mer slices of sequence and increment dictionary keys\n    for i in range(len(seq) - 3):\n        d[seq[i : (i + 4)]] += 1\n\n    print(*d.values())\n","109":"# Finding a Shared Spliced Motif\n\nfrom .helpers import Parser\n\n\ndef lcsq(s1, s2):\n    \"\"\"Finding a Shared Spliced Motif\"\"\"\n    # initialise\n    m, p = {}, {}\n    for j in range(len(s2) + 1):\n        m[j, 0] = 0\n        p[j, 0] = [j - 1, 0]\n\n    for i in range(len(s1) + 1):\n        m[0, i] = 0\n        p[0, i] = [0, i - 1]\n\n    # fill matrices\n    for j in range(len(s2)):\n        for i in range(len(s1)):\n            if s1[i] == s2[j]:\n                m[j + 1, i + 1] = m[j, i] + 1\n                p[j + 1, i + 1] = [j, i]\n            else:\n                opt = [m[j + 1, i], m[j, i + 1]]\n                m[j + 1, i + 1] = max(opt)\n                p[j + 1, i + 1] = [[j + 1, i], [j, i + 1]][opt.index(max(opt))]\n\n    # traceback\n    subs = \"\"\n    i, j = len(s1), len(s2)\n    while i > 0 and j > 0:\n        if p[j, i] == [j - 1, i - 1]:\n            subs += s1[i - 1]\n        j, i = p[j, i]\n\n    return subs[::-1]\n\n\ndef main(file):\n    print(lcsq(*Parser(file).seqs()))\n","110":"# Counting Inversions\n\nfrom .helpers import ints\n\n\ndef mer2(a1, a2):\n    a = []\n    s = len(a1)\n    count, i = 0, 0\n    while a1 and a2:\n        if a1[0] <= a2[0]:\n            i += 1\n            a += [a1.pop(0)]\n        else:\n            count += s - i\n            a += [a2.pop(0)]\n    return a + a1 + a2, count\n\n\ndef ms2(a):\n    if len(a) > 1:\n        mid = len(a) \/\/ 2\n        a1, c1 = ms2(a[:mid])\n        a2, c2 = ms2(a[mid:])\n        a, c = mer2(a1, a2)\n        return a, c1 + c2 + c\n    else:\n        return a, 0\n\n\ndef main(file):\n    _, array = open(file).read().splitlines()\n    _, count = ms2(ints(array))\n    print(count)\n","111":"# 2-Way Partition\n\nfrom .helpers import ints\n\n\ndef par(x):\n    pivot, pi = x[0], 0\n    for i in range(1, len(x)):\n        if x[i] <= pivot:\n            pi += 1\n            x[i], x[pi] = x[pi], x[i]\n    x[0], x[pi] = x[pi], x[0]\n    return x\n\n\ndef main(file):\n    _, x = open(file).read().splitlines()\n    print(*par(ints(x)))\n","112":"# Transitions and Transversions\n\nfrom .helpers import Parser\n\n\ndef ts(x, y):\n    \"\"\"Transition?\"\"\"\n    ts = {\"A\": \"G\", \"C\": \"T\", \"G\": \"A\", \"T\": \"C\"}\n    return x == ts[y]\n\n\ndef tran(seqs):\n    \"\"\"Transitions and Transversions\"\"\"\n    mm, tr = 0, 0\n    for x, y in zip(seqs[0], seqs[1]):\n        if x != y:\n            mm += 1\n            tr += int(ts(x, y))\n\n    return tr \/ (mm - tr)\n\n\ndef main(file):\n    print(round(tran(Parser(file).seqs()), 11))\n","113":"# Convert a Peptide Vector into a Peptide\n\nfrom .ba11c import masses\n\n\ndef peptide_mass(peptide, masses):\n    return sum(masses[x] for x in peptide)\n\n\ndef prefixes2peptide(prefixes, masses):\n    m = {v: k for k, v in masses.items()}\n    peptide = [m[b - a] for a, b in zip([0] + prefixes, prefixes)]\n    return \"\".join(peptide)\n\n\ndef vector2peptide(vector, masses):\n    prefixes = [i + 1 for i, d in enumerate(vector) if d]\n    return prefixes2peptide(prefixes, masses)\n\n\ndef main(file):\n    vector = list(map(int, open(file).read().split()))\n    print(vector2peptide(vector, masses()))\n","114":"# Find the Most Frequent Words in a String\n\nfrom collections import defaultdict\nfrom .ba1a import substrings\n\n\ndef count_kmers(text, k):\n    d = defaultdict(int)\n    for x in substrings(text, k):\n        d[x] += 1\n    return d\n\n\ndef most_frequent(d):\n    return [k for k in d if d[k] == max(d.values())]\n\n\ndef main(file):\n    text, k = open(file).read().splitlines()\n    print(*most_frequent(count_kmers(text, int(k))))\n","115":"# Implement LeaderboardCyclopeptideSequencing\n\nfrom .ba4c import mass, cyclo_spectrum\nfrom .ba1a import substrings\nfrom .ba4f import score\nfrom .ba4e import expand\n\n\ndef linear_spectrum(peptide):\n    spec = [0]\n    for i in range(1, len(peptide) + 1):\n        for x in substrings(peptide, i):\n            spec.append(sum(x))\n    return spec\n\n\ndef linear_score(peptide, spectrum):\n    return score(linear_spectrum(peptide), spectrum)\n\n\ndef cyclo_score(peptide, spectrum):\n    return score(cyclo_spectrum(peptide), spectrum)\n\n\ndef cut(peptides, spectrum, n):\n    if len(peptides) < n:\n        return peptides\n    sc = [linear_score(p, spectrum) for p in peptides]\n    lim = sorted(sc, reverse=True)[n - 1]\n    return [p for p, sc in zip(peptides, sc) if sc >= lim]\n\n\ndef leaderboard_cyclopeptide_sequencing(spec, n, masses):\n    lb = [[]]\n    leader = []\n    while len(lb):\n        lb = expand(lb, masses)\n        for pep in lb.copy():\n            if sum(pep) == spec[-1]:\n                if cyclo_score(pep, spec) > cyclo_score(leader, spec):\n                    leader = pep\n            elif sum(pep) > spec[-1]:\n                lb.remove(pep)\n        lb = cut(lb, spec, n)\n    return leader\n\n\ndef main(file):\n    n, m = open(file).read().splitlines()\n    spec = list(map(int, m.split()))\n    masses = set(mass().values())\n    p = leaderboard_cyclopeptide_sequencing(spec, int(n), masses)\n    print(*p, sep=\"-\")\n","116":"# Newick Format with Edge Weights\n\nimport re\nfrom collections import defaultdict\nfrom .nwck import dij\n\n\n# A slightly hacky modification of nwck:parse_newick that only works if\n# weights are present.\n# TODO: rewrite to be a single function...\n\n\ndef parse_newick(newick, directed=True):\n    newick = re.sub(\",,\", \",.,\", newick)\n    newick = re.sub(r\"\\(,\", \"(.,\", newick)\n    newick = re.sub(r\",\\)\", \",.)\", newick)\n    newick = re.sub(r\"\\(\\)\", \"(.)\", newick)\n    newick = re.sub(r\"^\\((.+)\\);\", r\"\\1\", newick)\n    m = re.finditer(r\"(\\(|([A-z_.]*:\\d+)|,|\\))\", newick)\n    tokens = [x.groups()[0] for x in m]\n\n    count = 0\n    node_stack = [\"0\"]\n    g = defaultdict(list)\n    i = len(tokens) - 1\n    while i >= 0:\n        if tokens[i] == \"(\":\n            node_stack = node_stack[:-1]\n        elif tokens[i] == \")\":\n            if i + 1 < len(tokens) and tokens[i + 1] not in \",)\":\n                if tokens[i + 1][0] == \":\":\n                    weight = tokens[i + 1][1:]\n                    count += 1\n                    node = str(count)\n                else:\n                    node, weight = tokens[i + 1].split(\":\")\n            g[node_stack[-1]].append({\"n\": node, \"w\": int(weight)})\n            if not directed:\n                g[node].append({\"n\": node_stack[-1], \"w\": int(weight)})\n            node_stack += [node]\n        elif tokens[i] != \",\" and (i == 0 or tokens[i - 1] != \")\"):\n            if tokens[i] == \".\":\n                count += 1\n                tokens[i] = str(count)\n            node, weight = tokens[i].split(\":\")\n            g[node_stack[-1]].append({\"n\": node, \"w\": int(weight)})\n            if not directed:\n                g[node].append({\"n\": node_stack[-1], \"w\": int(weight)})\n        i -= 1\n    return g\n\n\ndef newick_dist(tree, nodes):\n    return dij(parse_newick(tree, directed=False), nodes[0])[nodes[1]]\n\n\ndef main(file):\n    contents = open(file).read().split(\"\\n\\n\")\n    if contents[-1] == \"\":\n        contents = contents[:-1]\n    trees = [x.split(\"\\n\") for x in contents]\n    print(*[newick_dist(tree[0], tree[1].split()) for tree in trees])\n","117":"# Locating Restriction Sites\n\nfrom .helpers import Parser\n\n\ndef reverse_pallindromes(seq):\n    comp = seq.translate(str.maketrans(\"ACGT\", \"TGCA\"))\n    n = len(seq)\n    for i in range(n):\n        for size in range(2, 7):\n            lo = i - size\n            hi = i + size\n            if lo < 0 or hi > n or seq[lo:hi] != comp[lo:hi][::-1]:\n                break\n            else:\n                yield [i - size + 1, size * 2]\n\n\ndef main(file):\n    seq = Parser(file).fastas()[0].seq\n    res = sorted(reverse_pallindromes(seq))\n    for row in res:\n        print(*row)\n","118":"# Read Quality Distribution\n\nfrom Bio import SeqIO\n\n\ndef main(file):\n    handle = open(file)\n    n = int(next(handle))\n    count = 0\n    for rcrd in SeqIO.parse(handle, \"fastq\"):\n        q = rcrd.letter_annotations[\"phred_quality\"]\n        count += sum(q) \/ len(q) < n\n    print(count)\n","119":"# Implement Baum-Welch Learning\n\nimport numpy as np\nfrom .ba10i import parse_input, print_dict\nfrom .ba10j import soft_decode, forward, backward\nfrom .ba10h import as_dict\n\n# To implement this method, we need to calculate two responsibility matrices\n# (PI^* and PI^**, see page 225). PI^* is the same as `soft_decode` calculated\n# in ba10j.\n\n\n# Estimate responsibility matrix (PI^**)\n# implementing equation in purple box on p. 224\n# NB: Weight_i(l,k) = transition_pi_(i-1),pi_i * emission_pi_i(x_i)\ndef estimate_pi2(seq, fwd, bak, tmat, emat, states):\n    rep_mat = np.zeros((fwd.shape[0] - 1, len(states), len(states)), dtype=float)\n    for i in range(0, fwd.shape[0] - 1):\n        for j, s1 in enumerate(states):\n            for k, s2 in enumerate(states):\n                weight = tmat[s1, s2] * emat[s2, seq[i + 1]]\n                rep_mat[i, j, k] = (\n                    fwd[i, j] * bak[i + 1, k] * weight \/ sum(fwd[i, :] * bak[i, :])\n                )\n    return rep_mat\n\n\n# Estimate a transition matrix from the responsibility matrix PI^**\ndef estimate_tmat(seq, st, tmat, emat):\n    fwd = forward(seq, st, tmat, emat)\n    bak = backward(seq, st, tmat, emat)\n    pi2 = estimate_pi2(seq, fwd, bak, tmat, emat, st)\n    tmat = np.sum(pi2, 0)\n    tmat = tmat \/ np.sum(tmat, axis=1, keepdims=True)\n    return as_dict(tmat, st, st)\n\n\n# Estimate a emission matrix from the responsibility matrix PI^*\ndef estimate_emat(seq, al, st, tmat, emat):\n    pi1 = soft_decode(seq, st, tmat, emat)\n    emat = np.zeros((len(st), len(al)), dtype=float)\n    for i, emission in enumerate(al):\n        ind = np.array(list(seq)) == emission\n        emat[:, i] = np.sum(pi1[ind, :], 0)\n    emat = emat \/ np.sum(emat, axis=1, keepdims=True)\n    return as_dict(emat, st, al)\n\n\ndef main(file):\n    niter, seq, st, al, tmat, emat = parse_input(open(file))\n    for _ in range(niter):\n        tmat2 = estimate_tmat(seq, st, tmat, emat)\n        emat2 = estimate_emat(seq, al, st, tmat, emat)\n        emat, tmat = emat2, tmat2\n    print_dict(tmat, st, st)\n    print(\"--------\")\n    print_dict(emat, st, al)\n","120":"# Construct the Suffix Tree of a String\n\nfrom rosalind.bioinformatics_stronghold.suff import suff, get_edges\n\n\ndef main(file):\n    seq = open(file).read().rstrip()\n    tree = suff(seq)\n    print(*list(get_edges(tree)), sep=\"\\n\")\n","121":"# Majority Element\n\n# I would naturally solve this with hashing\n# Here though, I've implemented Moore's voting algorithm\n\nfrom .helpers import ints\n\n\ndef find_candidate(arr):\n    maj_index = 0\n    count = 1\n    for i in range(len(arr)):\n        count += 1 if arr[maj_index] == arr[i] else -1\n        if count == 0:\n            maj_index = i\n            count = 1\n    return arr[maj_index]\n\n\ndef is_majority(arr, cand):\n    count = 0\n    for i in range(len(arr)):\n        if arr[i] == cand:\n            count += 1\n    return count > len(arr) \/ 2\n\n\n# Using Moore's voting algorithm\ndef maj_vote(arr):\n    cand = find_candidate(arr)\n    return cand if is_majority(arr, cand) else -1\n\n\ndef main(file):\n    _, *arrays = open(file).read().splitlines()\n    arrays = [ints(arr) for arr in arrays]\n    print(*[maj_vote(arr) for arr in arrays])\n","122":"# Global Alignment with Scoring Matrix and Affine Gap Penalty\n\nfrom .helpers import Parser\nfrom rosalind.helpers import blosum62\n\n\ndef insert_indel(word, i):\n    return word[:i] + \"-\" + word[i:]\n\n\ndef gaff(v, w, score, sigma, epsilon):\n    \"\"\"Global Alignment with Affine Gap Penalty\"\"\"\n    # See Biological Sequence Analysis page 29\n    # We now have to keep track of three matrices m, x and y\n\n    m = [[0 for j in range(len(w) + 1)] for i in range(len(v) + 1)]\n    x = [[0 for j in range(len(w) + 1)] for i in range(len(v) + 1)]\n    y = [[0 for j in range(len(w) + 1)] for i in range(len(v) + 1)]\n    px = [[0 for j in range(len(w) + 1)] for i in range(len(v) + 1)]\n    pm = [[0 for j in range(len(w) + 1)] for i in range(len(v) + 1)]\n    py = [[0 for j in range(len(w) + 1)] for i in range(len(v) + 1)]\n\n    # Initialize the edges with the given penalties.\n    for i in range(1, len(v) + 1):\n        x[i][0] = sigma + (i - 1) * epsilon\n        m[i][0] = sigma + (i - 1) * epsilon\n        y[i][0] = 10 * sigma\n    for j in range(1, len(w) + 1):\n        y[0][j] = sigma + (j - 1) * epsilon\n        m[0][j] = sigma + (j - 1) * epsilon\n        x[0][j] = 10 * sigma\n\n    # Fill in the scores for the lower, middle, upper, and backtrack matrices.\n    for i in range(1, len(v) + 1):\n        for j in range(1, len(w) + 1):\n            s = [x[i - 1][j] + epsilon, m[i - 1][j] + sigma]\n            x[i][j] = max(s)\n            px[i][j] = s.index(x[i][j])\n\n            s = [y[i][j - 1] + epsilon, m[i][j - 1] + sigma]\n            y[i][j] = max(s)\n            py[i][j] = s.index(y[i][j])\n\n            s = [x[i][j], m[i - 1][j - 1] + score[v[i - 1]][w[j - 1]], y[i][j]]\n            m[i][j] = max(s)\n            pm[i][j] = s.index(m[i][j])\n\n    # Initialize the values of i, j and the aligned sequences.\n    i, j = len(v), len(w)\n    a1, a2 = v, w\n\n    # Get the maximum score, and the corresponding backtrack starting position.\n    scores = [x[i][j], m[i][j], y[i][j]]\n    max_score = max(scores)\n    s = scores.index(max_score)\n\n    # Backtrack to the edge of the matrix starting bottom right.\n    while i * j != 0:\n        if s == 0:\n            if px[i][j] == 1:\n                s = 1\n            i -= 1\n            a2 = insert_indel(a2, j)\n        elif s == 1:\n            if pm[i][j] == 1:\n                i -= 1\n                j -= 1\n            else:\n                s = pm[i][j]\n        else:\n            if py[i][j] == 1:\n                s = 1\n            j -= 1\n            a1 = insert_indel(a1, i)\n\n    # Prepend the necessary preceding indels to get to (0,0).\n    for _ in range(i):\n        a2 = insert_indel(a2, 0)\n    for _ in range(j):\n        a1 = insert_indel(a1, 0)\n\n    return {\"dist\": str(max_score), \"a1\": a1, \"a2\": a2}\n\n\ndef main(file):\n    seqs = Parser(file).seqs()\n    res = gaff(seqs[0], seqs[1], blosum62(), -11, -1)\n    print(res[\"dist\"])\n    print(res[\"a1\"])\n    print(res[\"a2\"])\n","123":"# Edit Distance\n\nfrom .helpers import Parser\n\n\ndef edit(s1, s2):\n    \"\"\"Edit Distance\"\"\"\n    # initialise\n    m = {}\n    for j in range(len(s2) + 1):\n        m[j, 0] = j\n    for i in range(len(s1) + 1):\n        m[0, i] = i\n\n    # fill matrices\n    for j in range(len(s2)):\n        for i in range(len(s1)):\n            if s1[i] == s2[j]:\n                m[j + 1, i + 1] = m[j, i]\n            else:\n                m[j + 1, i + 1] = min([m[j + 1, i], m[j, i], m[j, i + 1]]) + 1\n\n    return m[len(s2), len(s1)]\n\n\ndef main(file):\n    seqs = Parser(file).seqs()\n    print(edit(seqs[0], seqs[1]))\n","124":"# Hamiltonian Path in DAG\n\nfrom .ts import topological_sort\nfrom .helpers import parse_graphs\n\n# A Hamiltonian path exists if and only if there are edge between consecutive\n# topologically sorted vertices\n\n\ndef hdag(graph):\n    x = topological_sort(graph)\n    for a, b in zip(x, x[1:]):\n        if b not in graph[a]:\n            return [-1]\n    return [1] + x\n\n\ndef main(file):\n    for graph in parse_graphs(open(file), directed=True):\n        print(*hdag(graph))\n","125":"# Adapt SmallParsimony to Unrooted Trees\n\nfrom collections import defaultdict\nfrom .ba7f import ba6f, root, print_edges\n\n\ndef root_tree(graph, node):\n    for child in graph[node]:\n        if node in graph[child]:\n            graph[child].remove(node)\n        root_tree(graph, child)\n\n\n# parse input (and convert to a rooted tree, setting root to first non-leaf node)\ndef parse_input(handle):\n    n = next(handle)\n    n = int(n)\n    seqs = {}\n    graph = defaultdict(list)\n    for i, edge in enumerate(range(n)):\n        next(handle)\n        f, t = next(handle).rstrip().split(\"->\")\n        graph[int(f)].append(i)\n        seqs[i] = t\n\n    lines = handle.readlines()\n    root = int(lines[0].rstrip().split(\"->\")[0])\n    for edge in lines:\n        f, t = edge.rstrip().split(\"->\")\n        graph[int(f)].append(int(t))\n\n    root_tree(graph, root)\n    return seqs, graph\n\n\ndef main(file):\n    seqs, graph = parse_input(open(file))\n    total_score, seqs = ba6f(graph, seqs)\n    print(total_score)\n    print_edges(graph, seqs, root(graph))\n","126":"# The Wright-Fisher Model of Genetic Drift\n\nfrom .helpers import Parser, dbinom\nimport numpy as np\n\n\ndef wf_model(n, m, g):\n    # transition matrix\n    tmat = [[dbinom(x, n, m \/ n) for x in range(n + 1)] for m in range(n + 1)]\n    tmat = np.array(tmat)\n    v = np.array([0] * (n + 1))\n    v[m] = 1\n    for i in range(g):\n        v = np.dot(v, tmat)\n    return v\n\n\ndef wfmd(n, m, g, k):\n    p = wf_model(2 * n, m, g)\n    return sum(p[:-k])\n\n\ndef main(file):\n    print(round(wfmd(*Parser(file).ints()), 3))\n","127":"# Data Formats\n\nfrom .gbk import entrez_email\nfrom Bio import Entrez, SeqIO\nimport sys\n\n\ndef get_records(ids):\n    Entrez.email = entrez_email()\n    handle = Entrez.efetch(db=\"nucleotide\", id=ids, rettype=\"fasta\")\n    return list(SeqIO.parse(handle, \"fasta\"))\n\n\ndef frmt(ids):\n    records = get_records(ids)\n    lengths = [len(x) for x in records]\n    return records[lengths.index(min(lengths))]\n\n\ndef main(file):\n    ids = open(file).read().split()\n    rcrd = frmt(ids)\n    SeqIO.write(rcrd, sys.stdout, \"fasta\")\n","128":"# Introduction to Set Operations\n\nfrom builtins import eval\nfrom .helpers import Parser\n\n\ndef main(file):\n    n, s1, s2 = Parser(file).lines()\n    n = int(n)\n    s1, s2 = eval(s1), eval(s2)\n    s3 = set(range(1, n + 1))\n    print(*[s1 | s2, s1 & s2, s1 - s2, s2 - s1, s3 - s1, s3 - s2], sep=\"\\n\")\n","129":"# Find a Median String\n\nfrom .ba1a import substrings\nfrom .ba1i import generate_kmers\nfrom .ba1g import hamming\nimport math\n\n\ndef minimum_distance(x, text):\n    return min(hamming(w, x) for w in substrings(text, len(x)))\n\n\ndef distance_between_pattern_and_strings(pattern, dna):\n    return sum(minimum_distance(pattern, x) for x in dna)\n\n\ndef median_string(dna, k):\n    dist = math.inf\n    for kmer in generate_kmers(k):\n        x = distance_between_pattern_and_strings(kmer, dna)\n        if dist > x:\n            dist, median = x, kmer\n    return median\n\n\ndef main(file):\n    k, *dna = open(file).read().splitlines()\n    print(median_string(dna, int(k)))\n","130":"# Compute Limb Lengths in a Tree\n\n\ndef parse_mat(lines):\n    \"\"\"Parse integer matrix from set of lines\"\"\"\n    return [[int(x) for x in y.split()] for y in lines]\n\n\ndef limb(D, j):\n    \"\"\"Calculate limb length j for distance matrix d\"\"\"\n    limb_len = 100000\n    n = len(D)\n    for k in range(int(n)):\n        for i in range(int(n)):\n            if j != k and i != j:\n                limb_len = min((D[i][j] + D[j][k] - D[i][k]) \/\/ 2, limb_len)\n    return limb_len\n\n\ndef main(file):\n    _, j, *arr = open(file).read().splitlines()\n    print(limb(parse_mat(arr), int(j)))\n","131":"# Find the Longest Substring Shared by Two Strings\n\nfrom collections import defaultdict\nfrom rosalind.bioinformatics_stronghold.suff import suff\nimport re\n\n\ndef as_graph(suff):\n    def build_graph(suff, T, n1):\n        n2 = n1\n        for edge in sorted(suff):\n            n2 += 1\n            T[n1].append({\"n\": n2, \"l\": edge})\n            n2 = build_graph(suff[edge], T, n2)\n        return n2\n\n    T = defaultdict(list)\n    build_graph(suff, T, 0)\n    return T\n\n\ndef leaves(graph):\n    tails = list(n[\"n\"] for nodes in graph.values() for n in nodes)\n    heads = set(graph.keys())\n    return set(tails) - heads\n\n\n# reverse a (child points to parent), retaining edge labels\ndef reverse_graph(graph):\n    rev = {}\n    for node in graph.keys():\n        for child in graph[node]:\n            rev[child[\"n\"]] = {\"n\": node, \"l\": child[\"l\"]}\n    return rev\n\n\ndef purple_edges(tree, colors):\n    def _purple_edges(node, seq):\n        if colors[node] == \"purple\":\n            anyp = any(colors[n[\"n\"]] == \"purple\" for n in tree[node])\n            if anyp:\n                for child in tree[node]:\n                    yield from _purple_edges(child[\"n\"], seq + child[\"l\"])\n            else:\n                yield seq\n\n    yield from _purple_edges(0, \"\")\n\n\n# initialise leaves. The final nodes in our graph have no contents, but\n# here we'll set its color based on the label from the previous nodes\n# edges\ndef leaf_colors(tree):\n    rev = reverse_graph(tree)\n    colors = defaultdict(lambda: None)\n    for leaf in leaves(tree):\n        edge = rev[leaf][\"l\"]\n        m = re.search(r\"([$#])\", edge)\n        colors[leaf] = \"red\" if m.group() == \"$\" else \"blue\"\n    return colors\n\n\n# Add \"colours\" to a paired suffix tree.\n# \"red\" == sequence 1 (only), ending \"$\"\n# \"blue\" == sequence 2 (only), ending \"#\"\n# \"purple == shared.\ndef color_tree(tree, colors):\n    q = list(tree.keys())\n    while q:\n        for node in q:\n            cols = [colors[n[\"n\"]] for n in tree[node]]\n            if all(cols):\n                cols = set(cols)\n                if cols == set([\"red\"]):\n                    colors[node] = \"red\"\n                elif cols == set([\"blue\"]):\n                    colors[node] = \"blue\"\n                else:\n                    colors[node] = \"purple\"\n                q.remove(node)\n\n    return colors\n\n\ndef longest_shared_substring(seq1, seq2):\n    tree = suff(seq1 + \"$\" + seq2 + \"#\")\n    tree = as_graph(tree)\n    colors = color_tree(tree, leaf_colors(tree))\n    return max(purple_edges(tree, colors), key=lambda x: len(x))\n\n\ndef main(file):\n    seq1, seq2 = open(file).read().splitlines()\n    print(longest_shared_substring(seq1, seq2))\n","132":"# The Founder Effect and Genetic Drift\n\nfrom math import log10\nfrom .helpers import Parser\nfrom .wfmd import wf_model\n\n\ndef foun(n, m, a):\n    \"\"\"The Founder Effect and Genetic Drift\"\"\"\n    for g in range(1, m + 1):\n        yield [log10(wf_model(2 * n, i, g)[0]) for i in a]\n\n\ndef main(file):\n    \"\"\"The Founder Effect and Genetic Drift\"\"\"\n    l1, l2 = Parser(file).lines()\n    n, m = [int(x) for x in l1.split()]\n    a = [int(x) for x in l2.split()]\n    for x in foun(n, m, a):\n        print(*[f\"{f:.12f}\" for f in x])\n","133":"# Compute the Probability of a String Emitted by an HMM\n\nimport numpy as np\nfrom .ba10c import parse_input\n\n\ndef likelihood(seq, states, tmat, emat):\n    mat = np.ones((len(seq) + 1, len(states)))\n\n    for i, state in enumerate(states):\n        mat[0, i] = emat[state, seq[0]] \/ len(states)\n\n    for i, emission in enumerate(seq[1:], start=1):\n        for j, state in enumerate(states):\n            mat[i, j] = sum(\n                tmat[prev, state] * emat[state, emission] * mat[i - 1, k]\n                for k, prev in enumerate(states)\n            )\n\n    return sum(mat[i, :])\n\n\ndef main(file):\n    seq, states, tmat, emat = parse_input(open(file))\n    print(likelihood(seq, states, tmat, emat))\n","134":"# Implement PatternToNumber\n\n\ndef symbol_to_number(x):\n    return [\"A\", \"C\", \"G\", \"T\"].index(x)\n\n\ndef pattern_to_number(seq):\n    if len(seq) == 0:\n        return 0\n    else:\n        return 4 * pattern_to_number(seq[:-1]) + symbol_to_number(seq[-1])\n\n\ndef main(file):\n    seq = open(file).read().splitlines()[0]\n    print(pattern_to_number(seq))\n","135":"# Sex-Linked Inheritance\n\nfrom .helpers import Parser\n\n\ndef main(file):\n    arr = Parser(file).floats()\n    print(*[round(2 * v * (1 - v), 3) for v in arr])\n","136":"# Construct the De Bruijn Graph of a String\n\nfrom collections import defaultdict\nfrom .ba1a import substrings\n\n\ndef dbru(seqs):\n    g = defaultdict(list)\n    for x in seqs:\n        g[x[:-1]].append(x[1:])\n    return g\n\n\ndef main(file):\n    k, seq = open(file).read().splitlines()\n    for k, v in dbru(substrings(seq, int(k))).items():\n        print(k + \" -> \" + \",\".join(v))\n","137":"# Find a Highest-Scoring Overlap Alignment of Two Strings\n\n\ndef overlap_alignment(s1, s2, penalty=-2):\n    m, p = {}, {}\n    for j in range(len(s2) + 1):\n        m[j, 0] = j * penalty\n        p[j, 0] = \"\u2191\"\n    for i in range(len(s1) + 1):\n        m[0, i] = 0\n        p[0, i] = \"\u2190\"\n\n    m[0, 0] = 0\n    for j in range(len(s2)):\n        for i in range(len(s1)):\n            new = (j + 1, i + 1)\n            match = 1 if s1[i] == s2[j] else penalty\n            opt = [\n                m[j, i] + match,\n                m[j, i + 1] + penalty,\n                m[j + 1, i] + penalty,\n            ]\n            m[new] = max(opt)\n            p[new] = [\"\u2196\", \"\u2191\", \"\u2190\"][opt.index(max(opt))]\n\n    sc = [m[j, len(s1)] for j in range(len(s2) + 1)]\n    max_score = max(sc)\n    j = sc.index(max_score)\n    i = len(s1)\n\n    a1, a2 = \"\", \"\"\n    while i > 0 and j > 0:\n        if p[j, i] == \"\u2196\":\n            a1 += s1[i - 1]\n            a2 += s2[j - 1]\n            j, i = j - 1, i - 1\n        elif p[j, i] == \"\u2190\":\n            a1 += s1[i - 1]\n            a2 += \"-\"\n            i = i - 1\n        elif p[j, i] == \"\u2191\":\n            a1 += \"-\"\n            a2 += s2[j - 1]\n            j = j - 1\n\n    return max_score, a1[::-1], a2[::-1]\n\n\ndef main(file):\n    s1, s2 = open(file).read().splitlines()\n    print(*overlap_alignment(s1, s2), sep=\"\\n\")\n","138":"# Align Two Strings Using Affine Gap Penalties\n\nfrom .ba5e import blosum62\n\n\ndef insert_indel(word, i):\n    return word[:i] + \"-\" + word[i:]\n\n\ndef global_affine(v, w, sigma=-11, epsilon=-1):\n    score = blosum62()\n    m, x, y = {}, {}, {}\n    px, pm, py = {}, {}, {}\n\n    x[0, 0], m[0, 0], y[0, 0] = 0, 0, 0\n    for i in range(1, len(v) + 1):\n        x[i, 0] = sigma + (i - 1) * epsilon\n        m[i, 0] = sigma + (i - 1) * epsilon\n        y[i, 0] = 10 * sigma\n    for j in range(1, len(w) + 1):\n        y[0, j] = sigma + (j - 1) * epsilon\n        m[0, j] = sigma + (j - 1) * epsilon\n        x[0, j] = 10 * sigma\n\n    for i in range(1, len(v) + 1):\n        for j in range(1, len(w) + 1):\n            s = [x[i - 1, j] + epsilon, m[i - 1, j] + sigma]\n            x[i, j] = max(s)\n            px[i, j] = s.index(x[i, j])\n\n            s = [y[i, j - 1] + epsilon, m[i, j - 1] + sigma]\n            y[i, j] = max(s)\n            py[i, j] = s.index(y[i, j])\n\n            s = [x[i, j], m[i - 1, j - 1] + score[v[i - 1]][w[j - 1]], y[i, j]]\n            m[i, j] = max(s)\n            pm[i, j] = s.index(m[i, j])\n\n    i, j = len(v), len(w)\n    a1, a2 = v, w\n\n    scores = [x[i, j], m[i, j], y[i, j]]\n    max_score = max(scores)\n    s = scores.index(max_score)\n\n    while i * j != 0:\n        if s == 0:\n            if px[i, j] == 1:\n                s = 1\n            i -= 1\n            a2 = insert_indel(a2, j)\n        elif s == 1:\n            if pm[i, j] == 1:\n                i -= 1\n                j -= 1\n            else:\n                s = pm[i, j]\n        else:\n            if py[i, j] == 1:\n                s = 1\n            j -= 1\n            a1 = insert_indel(a1, i)\n\n    for _ in range(i):\n        a2 = insert_indel(a2, 0)\n    for _ in range(j):\n        a1 = insert_indel(a1, 0)\n\n    return max_score, a1, a2\n\n\ndef main(file):\n    s1, s2 = open(file).read().splitlines()\n    print(*global_affine(s1, s2), sep=\"\\n\")\n","139":"# Working with Files\n\n\ndef main(file):\n    for i, line in enumerate(open(file).readlines()):\n        if i % 2 == 1:\n            print(line, end=\"\")\n","140":"# Variables and Some Arithmetic\n\n\ndef main(file):\n    print(sum([int(x) ** 2 for x in open(file).read().split()]))\n","141":"# Reversal Distance\n\nfrom itertools import combinations\n\n\ndef flip(x, i, j):\n    \"\"\"Flip a section of a sequence\"\"\"\n    rev = list.copy(x)\n    rev[i:j] = rev[i:j][::-1]\n    return rev\n\n\ndef breaks(s, t):\n    \"\"\"Identify breaks between a sequence and target\"\"\"\n    return [\n        i + 1 for i in range(len(s) - 1) if abs(t.index(s[i]) - t.index(s[i + 1])) != 1\n    ]\n\n\ndef min_breaks(seqs, t):\n    rev = []\n    for s in seqs:\n        for i, j in combinations(breaks(s[\"s\"], t), 2):\n            rev.append({\"s\": flip(s[\"s\"], i, j), \"p\": s[\"p\"] + [[i, j - 1]]})\n    min_b = len(t)\n    mr = []\n    for r in rev:\n        n = len(breaks(r[\"s\"], t))\n        if n < min_b:\n            min_b = n\n            mr = [r]\n        elif n == min_b:\n            mr.append(r)\n    return mr\n\n\n# based on https:\/\/medium.com\/@matthewwestmk\/87c62d690eef\ndef sort(s, t):\n    \"\"\"Sorting by Reversals\"\"\"\n    s = [\"-\"] + s + [\"+\"]\n    t = [\"-\"] + t + [\"+\"]\n    nr = 0\n    c = [{\"s\": s, \"p\": []}]\n    seqs = [s]\n    while t not in seqs:\n        c = min_breaks(c, t)\n        nr += 1\n        seqs = [x[\"s\"] for x in c]\n    return nr, c\n\n\ndef main(file):\n    data = open(file).read().strip().split(\"\\n\\n\")\n    data = [tuple([list(map(int, y.split())) for y in x.split(\"\\n\")]) for x in data]\n    print(*[sort(s, t)[0] for s, t in data])\n","142":"# Median\n\nfrom .helpers import ints\nfrom .par3 import par3\n\n\n# s and e are the indices of the start and end of the middle partition\ndef med(A, k):\n    def find_med(A, k, low, high):\n        s, e = par3(A, low, high)\n        if k < s:\n            return find_med(A, k, low, s)\n        elif k > e:\n            return find_med(A, k, e + 1, high)\n        else:\n            return A[s]\n\n    return find_med(A, k, 0, len(A) - 1)\n\n\ndef main(file):\n    handle = open(file)\n    _ = next(handle)\n    A = ints(next(handle))\n    k = int(next(handle))\n    print(med(A, k - 1))\n","143":"# Complementing a Strand of DNA\n\nfrom Bio import SeqIO\n\n\ndef main(file):\n    count = 0\n    for rcrd in SeqIO.parse(file, \"fasta\"):\n        count += rcrd.seq == rcrd.reverse_complement().seq\n    print(count)\n","144":"# Ordering Strings of Varying Length Lexicographically\n\nimport re\nfrom .helpers import Parser\nfrom itertools import product\n\n\ndef lexv(s, n):\n    \"\"\"Ordering Strings of Varying Length Lexicographically\"\"\"\n    s = [\"_\"] + s\n    perm = list(product(s, repeat=n))\n    perm = [\"\".join(x) for x in perm]\n    perm = [re.sub(\"_+$\", \"\", x) for x in perm]\n    return list(filter(lambda x: \"_\" not in x, perm[1:]))\n\n\ndef main(file):\n    l1, l2 = Parser(file).lines()\n    print(*lexv(l1.split(), int(l2)), sep=\"\\n\")\n","145":"# Find the Longest Repeat in a String\n\nfrom rosalind.bioinformatics_stronghold.suff import suff\n\n\n# We'll do DFS over tree, ending iteration in the node above a leaf\n# By doing this, edges returned are shared and therefore repeats\ndef internal_edges(tree):\n    for n1 in tree.keys():\n        if not len(tree[n1]):\n            yield \"\"\n        for n2 in internal_edges(tree[n1]):\n            yield n1 + n2\n\n\ndef longest_shared_substring(tree):\n    return max(internal_edges(tree), key=lambda x: len(x))\n\n\ndef main(file):\n    seq = open(file).read().rstrip()\n    tree = suff(seq)\n    print(longest_shared_substring(tree))\n","146":"# Assessing Assembly Quality with N50 and N75\n\nfrom .helpers import Parser\n\n\ndef asmq(seqs, n):\n    \"\"\"Assessing Assembly Quality with N50 and N75\"\"\"\n    lens = sorted([len(x) for x in seqs], reverse=True)\n    tot = sum(lens)\n    cumsum = 0\n    for x in lens:\n        cumsum += x\n        if cumsum \/ tot > n \/ 100:\n            return x\n\n\ndef main(file):\n    seqs = Parser(file).lines()\n    print(asmq(seqs, 50), asmq(seqs, 75))\n","147":"# Finding a Shared Motif\n\nfrom .helpers import Parser\n\n\ndef lcsm(seqs):\n    seqs = sorted(seqs, key=len)\n\n    maxsub = \"\"\n    s0 = seqs.pop()\n    s = len(s0)\n\n    for i in range(s):\n        for j in range(s - i + 1):\n            if j > len(maxsub) and all(s0[i : i + j] in x for x in seqs):\n                maxsub = s0[i : i + j]\n\n    return maxsub\n\n\ndef main(file):\n    print(lcsm(Parser(file).seqs()))\n","148":"# Counting Optimal Alignments\n\nfrom .helpers import Parser\n\n\ndef ctea(s1, s2):\n    \"\"\"Counting Optimal Alignments\"\"\"\n\n    score, routes = {}, {}\n    for j in range(len(s2) + 1):\n        score[j, 0] = j\n        routes[j, 0] = 1\n\n    for i in range(len(s1) + 1):\n        score[0, i] = i\n        routes[0, i] = 1\n\n    for j in range(len(s2)):\n        for i in range(len(s1)):\n            pos = [(j + 1, i), (j, i), (j, i + 1)]\n            cost = [1, int(s1[i] != s2[j]), 1]\n            scores = [score[pos[x]] + cost[x] for x in range(3)]\n            best = min(scores)\n            new = (j + 1, i + 1)\n            score[new] = best\n            routes[new] = sum(routes[pos[x]] for x in range(3) if scores[x] == best)\n\n    return routes[len(s2), len(s1)] % 134217727\n\n\ndef main(file):\n    seqs = Parser(file).seqs()\n    print(ctea(seqs[0], seqs[1]))\n","149":"# Reconstruct a String from its Paired Composition\n\nfrom .ba3b import genome_path\nfrom .ba3d import dbru\nfrom .ba3g import count_connections\n\n\ndef one_in_out(c):\n    return c[\"in\"] == 1 and c[\"out\"] == 1\n\n\ndef maximal_nonbranching_paths(graph):\n    con = count_connections(graph)\n    paths = []\n    for n, c in con.items():\n        if not one_in_out(c) and c[\"out\"] > 0:\n            for e in graph[n]:\n                path = [n, e]\n                while one_in_out(con[e]):\n                    path += graph[e]\n                    e = graph[e][0]\n                paths += [path]\n\n    seen = sum(paths, [])\n    todo = [x for x in con.keys() if one_in_out(con[x]) and x not in seen]\n    while len(todo):\n        e = todo.pop(0)\n        path = [e]\n        while one_in_out(con[e]) and graph[e][0] in todo:\n            path += graph[e]\n            e = graph[e][0]\n            todo.remove(e)\n        paths += [path + [path[0]]]\n    return paths\n\n\ndef main(file):\n    dna = open(file).read().splitlines()\n    graph = dbru(dna)\n    for path in sorted(maximal_nonbranching_paths(graph)):\n        print(genome_path(path))\n","150":"# RNA Splicing\n\nfrom functools import reduce\nfrom .helpers import Parser, Dna\n\n\ndef main(file):\n    def trim(gene, intron):\n        s = gene.find(intron)\n        return gene[:s] + gene[s + len(intron) :]\n\n    seqs = Parser(file).seqs()\n    print(Dna(reduce(trim, seqs)).translate())\n","151":"# Genome Assembly as Shortest Superstring\n\nfrom math import floor\nfrom .helpers import Parser\n\n\n# Fast find match using `find`\n# Find overlap of at least min_overlap of prefix of s2 in s1\n# Stores an index to search for next match\ndef find_overlap(s1, s2, min_overlap=None):\n    ix = 1\n    if min_overlap is None:\n        min_overlap = floor(len(s2) \/ 2)\n    while ix < len(s1):\n        ix = s1.find(s2[:min_overlap], ix)\n        if ix == -1:\n            break\n        if s2.startswith(s1[ix:]):\n            return len(s1) - ix\n        ix += 1\n\n\ndef construct_assembly(seqs):\n    # Build a forward and reverse overlap graph of sequences\n    fmap, rmap, starts, ends = ({}, {}, {}, {})\n    for p1 in seqs.keys():\n        for p2 in seqs.keys():\n            if p1 in starts or p2 in ends or p1 in p2:\n                continue\n            n = find_overlap(seqs[p1], seqs[p2])\n            if n:\n                fmap[p1] = {\"overlap\": n, \"next\": p2}\n                rmap[p2] = p1\n                starts[p1] = True\n                ends[p2] = True\n                break\n\n    # Find starting key using rmap\n    k = list(seqs.keys())[0]\n    while k in rmap:\n        k = rmap[k]\n\n    # Initialise list with sequence 1, then add suffixes of matching sequences\n    seq = [seqs[k]]\n    while k in fmap:\n        seq.append(seqs[fmap[k][\"next\"]][fmap[k][\"overlap\"] :])\n        k = fmap[k][\"next\"]\n\n    # Join all sequences\n    return \"\".join(seq)\n\n\ndef main(file):\n    seqs = Parser(file).fastas()\n    seqs = dict([(x.id, x.seq) for x in seqs])\n    print(construct_assembly(seqs))\n","152":"# Quartets\n\nfrom itertools import combinations, product\n\n\ndef qrt(s):\n    taxa = [[i for i, x in enumerate(s) if x == v] for v in [\"0\", \"1\"]]\n    for x in product(combinations(taxa[0], 2), combinations(taxa[1], 2)):\n        yield tuple(sorted(x))\n\n\ndef format_pair(pair, names):\n    return \"{\" + \", \".join([names[x] for x in pair]) + \"}\"\n\n\ndef main(file):\n    names, *splits = open(file).read().splitlines()\n    names = names.split()\n    for pairs in set().union(*[set(qrt(s)) for s in splits]):\n        print(*[format_pair(pair, names) for pair in pairs])\n","153":"# Implement TrieMatching\n\nfrom .ba9a import trie, edge_match\n\n\ndef prefix_trie_matching(text, graph):\n    v = 0\n    for symbol in text:\n        m = edge_match(graph, v, symbol)\n        if m:\n            if not graph[m]:\n                return True\n            else:\n                v = m\n        else:\n            return False\n\n\ndef trie_matching(text, graph):\n    for i in range(len(text)):\n        if prefix_trie_matching(text[i:], graph):\n            yield i\n\n\ndef main(file):\n    text, *seqs = open(file).read().splitlines()\n    graph = trie(seqs)\n    print(*trie_matching(text, graph))\n","154":"# Introduction to the Bioinformatics Armory\n\nfrom Bio.Seq import Seq\n\n\ndef ini(seq):\n    seq = Seq(seq)\n    return [seq.count(base) for base in (\"A\", \"C\", \"G\", \"T\")]\n\n\ndef main(file):\n    seq = str(open(file).read())\n    print(*ini(seq))\n","155":"# Introduction to Random Strings\n\nfrom math import log10\nfrom .helpers import Parser\n\n\ndef logp(x, gc):\n    \"\"\"log10 probability of observing x given GC content\"\"\"\n    return log10({\"G\": gc, \"C\": gc, \"A\": 1 - gc, \"T\": 1 - gc}[x] \/ 2)\n\n\ndef prob(seq, arr):\n    \"\"\"Introduction to Random Strings\"\"\"\n    return [sum([logp(x, gc) for x in seq]) for gc in arr]\n\n\ndef main(file):\n    seq, arr = Parser(file).lines()\n    arr = [float(x) for x in arr.split()]\n    res = [f\"{x:.3f}\" for x in prob(seq, arr)]\n    print(*res)\n","156":"# Find a Longest Common Subsequence of Two Strings\n\n\ndef longest_common_subsequence(s1, s2):\n    # Initialise\n    m, p = {}, {}\n    for j in range(len(s2) + 1):\n        m[j, 0] = 0\n        p[j, 0] = \"\u2191\"\n\n    for i in range(len(s1) + 1):\n        m[0, i] = 0\n        p[0, i] = \"\u2190\"\n\n    # fill matrices\n    for j in range(len(s2)):\n        for i in range(len(s1)):\n            if s1[i] == s2[j]:\n                opt = [m[j + 1, i], m[j, i + 1], m[j, i] + 1]\n            else:\n                opt = [m[j + 1, i], m[j, i + 1], m[j, i]]\n            m[j + 1, i + 1] = max(opt)\n            p[j + 1, i + 1] = [\"\u2190\", \"\u2191\", \"\u2196\"][opt.index(max(opt))]\n\n    # traceback\n    ss = \"\"\n    i, j = len(s1), len(s2)\n    while i > 0 or j > 0:\n        if p[j, i] == \"\u2196\":\n            ss += s1[i - 1]\n            j, i = j - 1, i - 1\n        elif p[j, i] == \"\u2190\":\n            i = i - 1\n        elif p[j, i] == \"\u2191\":\n            j = j - 1\n\n    return ss[::-1]\n\n\ndef main(file):\n    s1, s2 = open(file).read().splitlines()\n    print(longest_common_subsequence(s1, s2))\n","157":"# Genome Assembly with Perfect Coverage\n\nfrom .helpers import Parser\nfrom .dbru import dbru\n\n\ndef find_cycle(graph):\n    key = sorted(list(graph.keys()))[0]\n    visited = set()\n    cycle = []\n    while key not in visited:\n        cycle += [key]\n        visited.add(key)\n        key = graph[key]\n    return cycle\n\n\ndef join_cycle(chain):\n    return \"\".join(x[0] for x in chain)\n\n\ndef pcov(seqs):\n    \"\"\"Genome Assembly with Perfect Coverage\"\"\"\n    x = find_cycle(dict(dbru(seqs)))\n    return join_cycle(sorted(x))\n\n\ndef main(file):\n    print(pcov(Parser(file).lines()))\n","158":"# Protein Translation\n\nfrom Bio.Seq import Seq\n\n\ndef main(file):\n    dna, prot = open(file).read().splitlines()\n    for i in [1, 2, 3, 4, 5, 6, 9, 10, 11, 12, 13, 14, 15]:\n        if prot == Seq(dna).translate(table=i, to_stop=True):\n            print(i)\n            break\n","159":"# Catalan Numbers and RNA Secondary Structures\n\nfrom functools import cache\nfrom .helpers import Parser\n\n\ndef valid_pair(x, y):\n    pair = {\"A\": \"U\", \"U\": \"A\", \"C\": \"G\", \"G\": \"C\"}\n    return x == pair[y]\n\n\n@cache\ndef cat(seq, mod=10**6):\n    \"\"\"Calculate total number of noncrossing perfect matchings\"\"\"\n    if len(seq) in range(1):\n        return 1\n    else:\n        return (\n            sum(\n                cat(seq[1:m]) * cat(seq[m + 1 :])\n                for m in range(1, len(seq), 2)\n                if valid_pair(seq[0], seq[m])\n            )\n            % mod\n        )\n\n\ndef main(file):\n    print(cat(*Parser(file).seqs()))\n","160":"# Generate the Frequency Array of a String\n\nfrom .ba1a import pattern_count\nfrom .ba1i import generate_kmers\n\n\ndef frequency_array(seq, k):\n    return (pattern_count(seq, x) for x in generate_kmers(k))\n\n\ndef main(file):\n    seq, k = open(file).read().splitlines()\n    print(*frequency_array(seq, int(k)))\n","161":"# Translating RNA into Protein\n\nfrom .helpers import Parser\n\n\ndef main(file):\n    print(Parser(file).rna().translate())\n","162":"# Completing a Tree\n\nfrom .helpers import Parser\n\n\n# Nb. A connected tree of n nodes will always contain n-1 edges\ndef main(file):\n    data = Parser(file).lines()\n    n_nodes = int(data[0])\n    print(n_nodes - len(data[1:]) - 1)\n","163":"# Solve the Soft Decoding Problem\n\nimport numpy as np\nfrom .ba10c import parse_input\n\n\ndef forward(seq, states, tmat, emat):\n    mat = np.ones((len(seq), len(states)))\n\n    for i, state in enumerate(states):\n        mat[0, i] = emat[state, seq[0]]\n    for i, emission in enumerate(seq[1:], start=1):\n        for j, state in enumerate(states):\n            mat[i, j] = sum(\n                tmat[prev, state] * emat[state, emission] * mat[i - 1, k]\n                for k, prev in enumerate(states)\n            )\n\n    return mat\n\n\ndef backward(seq, states, tmat, emat):\n    mat = np.ones((len(seq), len(states)))\n\n    for i, emission in enumerate(seq[::-1][:-1], start=1):\n        for j, state in enumerate(states):\n            mat[len(seq) - i - 1, j] = sum(\n                tmat[state, prev] * emat[prev, emission] * mat[len(seq) - i, k]\n                for k, prev in enumerate(states)\n            )\n    return mat\n\n\ndef soft_decode(seq, states, tmat, emat, normalise=True):\n    tot = forward(seq, states, tmat, emat) * backward(seq, states, tmat, emat)\n    if normalise:\n        tot = tot \/ np.sum(tot, axis=1, keepdims=True)\n    return tot\n\n\ndef main(file):\n    seq, states, tmat, emat = parse_input(open(file))\n    tot = soft_decode(seq, states, tmat, emat)\n    print(*states, sep=\"\\t\")\n    for r in np.round(tot, 4):\n        print(*r, sep=\"\\t\")\n","164":"# Generate the Theoretical Spectrum of a Linear Peptide\n\nfrom .ba4g import linear_spectrum\nfrom .ba4c import mass\n\n\ndef main(file):\n    peptide = open(file).read().rstrip()\n    masses = mass()\n    print(*linear_spectrum([masses[x] for x in peptide]))\n","165":"# Shortest Cycle Through a Given Edge\n\nfrom .helpers import parse_graphs\nfrom .dij import dij\n\n# For this problem, we start Dijaktra's search at the end of the specified\n# edge, and take the distance to the start of the edge (if reachable) and add\n# the weight of the specified edge to get the cycle length.\n\n\ndef first_edges(handle):\n    lines = handle.read().splitlines()\n    ngraphs = int(lines[0])\n    start = 1\n    edges = []\n    for i in range(ngraphs):\n        if lines[start] == \"\":\n            start += 1\n        _, n_edges = lines[start].split()\n        edges += [list(map(int, lines[start + 1].split()))]\n        start += int(n_edges) + 1\n    return edges\n\n\ndef cte(graph, edge):\n    dist = dij(graph, start=edge[1])[edge[0] - 1]\n    return dist if dist == -1 else dist + edge[2]\n\n\ndef main(file):\n    edges = first_edges(open(file))\n    graphs = list(parse_graphs(open(file), directed=True, weighted=True))\n    res = [cte(graphs[i], edges[i]) for i in range(len(graphs))]\n    print(*res)\n","166":"# Identifying Maximal Repeats\n\nfrom collections import defaultdict\nfrom .helpers import Parser\nfrom .suff import suff\n\n\n# This is a bit messy, but the idea here is that we take a suffix tree and\n# for each node with children, we get the full path (concatenated edge\n# labels) from root. This sequence must be a repeat (since the node has\n# children).\n#\n# Out list `edges` in `main()` contains these paths (from each node with\n# children) and the (total) number of descendants that node has.\n# Now for each possible number of descendants (which is related to the number\n# of repeats), we ensure that we have the maximal repeat by excluding any\n# sequences that are fully contained inside others with the same number of\n# children.\n\n\ndef as_edges(graph):\n    for k in sorted(graph):\n        for v in graph[k]:\n            yield k, v[\"n\"], v[\"l\"]\n\n\ndef count_descendants(T):\n    def count(node):\n        if node not in T:\n            d[node] = 0\n            return 0\n        if node not in d:\n            d[node] = len(T[node]) + sum(count(x[\"n\"]) for x in T[node])\n            return d[node]\n\n    d = {}\n    count(0)\n    return d\n\n\ndef reverse_graph(graph):\n    rev = {}\n    for node in graph.keys():\n        for child in graph[node]:\n            rev[child[\"n\"]] = {\"n\": node, \"l\": child[\"l\"]}\n    return rev\n\n\ndef pathtoroot(rev, node):\n    path = \"\"\n    while node in rev:\n        path = rev[node][\"l\"] + path\n        node = rev[node][\"n\"]\n    return path\n\n\ndef internal_edges(graph, node):\n    rev = reverse_graph(graph)\n    desc = count_descendants(graph)\n    if desc[node] > 0:\n        yield desc[node], pathtoroot(rev, node)\n        for child in graph[node]:\n            yield from internal_edges(graph, child[\"n\"])\n\n\ndef as_graph(suff):\n    def build_graph(suff, T, n1):\n        n2 = n1\n        for edge in sorted(suff):\n            n2 += 1\n            T[n1].append({\"n\": n2, \"l\": edge})\n            n2 = build_graph(suff[edge], T, n2)\n        return n2\n\n    T = defaultdict(list)\n    build_graph(suff, T, 0)\n    return T\n\n\ndef main(file):\n    seq = Parser(file).line()\n    tree = as_graph(suff(seq + \"$\"))\n    edges = list(internal_edges(tree, 0))\n    d = defaultdict(list)\n    for n, edge in edges:\n        if len(edge) >= 20:\n            d[n].append(edge)\n    for repeats in d.values():\n        maximal = [x for x in repeats if sum(x in y for y in repeats) == 1]\n        print(*maximal, sep=\"\\n\")\n","167":"# Find the Length of a Longest Path in a Manhattan-like Grid\n\nimport numpy as np\n\n\ndef manhattan_tourist(n, m, down, right):\n    s = np.zeros((n + 1, m + 1))\n    for i in range(1, n + 1):\n        s[i][0] = s[i - 1][0] + down[i - 1][0]\n    for j in range(1, m + 1):\n        s[0][j] = s[0][j - 1] + right[0][j - 1]\n    for i in range(1, n + 1):\n        for j in range(1, m + 1):\n            s[i][j] = max([s[i - 1][j] + down[i - 1][j], s[i][j - 1] + right[i][j - 1]])\n    return int(s[n][m])\n\n\ndef main(file):\n    dim, *lines = open(file).read().splitlines()\n    n, m = map(int, dim.split())\n    down = np.array([list(map(int, x.split())) for x in lines[0:n]])\n    right = np.array([list(map(int, x.split())) for x in lines[(n + 1) :]])\n    print(manhattan_tourist(n, m, down, right))\n","168":"# Shortest Paths in DAG\n\nfrom .helpers import parse_graph\nfrom .ts import topological_sort\nfrom math import inf\n\n\ndef simplify_graph(graph):\n    return {k: [x[\"n\"] for x in v] for k, v in graph.items()}\n\n\ndef sdag(graph):\n    n = len(graph)\n    dist = [inf for _ in range(n + 1)]\n    dist[1] = 0\n    order = topological_sort(simplify_graph(graph))\n    for u in order:\n        # -----------------------------------------------------------------------\n        # Hack to pass!\n        # For this question, if there are multiple edges connecting the same\n        # two nodes, we must only consider the last one (not the shortest)!\n        # So we'll process edges in reverse order and skip any we've seen...\n        # -----------------------------------------------------------------------\n        seen = set()\n        for edge in graph[u][::-1]:\n            if edge[\"n\"] not in seen:\n                seen.add(edge[\"n\"])\n                if dist[u] + edge[\"w\"] < dist[edge[\"n\"]]:\n                    dist[edge[\"n\"]] = dist[u] + edge[\"w\"]\n    return [\"x\" if x == inf else x for x in dist[1:]]\n\n\ndef main(file):\n    print(*sdag(parse_graph(open(file), directed=True, weighted=True)))\n","169":"# Find a Highest-Scoring Multiple Sequence Alignment\n\nfrom itertools import product\n\n\n# Does this pointer\/score refer to a non-negative cell in our matrix?\ndef valid_coord(x, pos):\n    return all([i >= 0 for i in prev(pos, x)])\n\n\n# get previous position given a pointer\ndef prev(pos, ptr):\n    return tuple([p + d for p, d in zip(pos, ptr)])\n\n\n# given sequences, a position and a pointer, calculate score.\n# This represents a scoring function in which the score of an alignment column\n# is 1 if all three symbols are identical and 0 otherwise.\ndef score(seqs, pos, ptr):\n    if ptr == (-1, -1, -1):\n        bases = [seqs[i][j] for i, j in enumerate(prev(pos, ptr))]\n        return all(x == bases[0] for x in bases)\n    else:\n        return 0\n\n\n# generate possible previous cells relative to current cell (pointers)\ndef moves(n):\n    return list(product([0, -1], repeat=n))[1:]\n\n\ndef multiple_alignment(seqs):\n    m, p = {}, {}\n    m[0, 0, 0] = 0\n    ranges = [range(0, len(s) + 1) for s in seqs]\n    for pos in product(*ranges):\n        ptrs = list(filter(lambda x: valid_coord(x, pos), moves(3)))\n        if not len(ptrs):\n            continue\n        sc = [m[prev(pos, x)] + score(seqs, pos, x) for x in ptrs]\n        m[pos] = max(sc)\n        p[pos] = ptrs[sc.index(max(sc))]\n\n    # traceback to recover alignment\n    tot = m[pos]\n    aln = [\"\", \"\", \"\"]\n    while any([x > 0 for x in pos]):\n        ptr = p[pos]\n        for i, seq in enumerate(seqs):\n            aln[i] += seq[pos[i] - 1] if ptr[i] == -1 else \"-\"\n        pos = prev(pos, ptr)\n\n    return tot, aln[0][::-1], aln[1][::-1], aln[2][::-1]\n\n\ndef main(file):\n    seqs = open(file).read().splitlines()\n    print(*multiple_alignment(seqs), sep=\"\\n\")\n","170":"# Testing Acyclicity\n\nfrom .helpers import parse_graphs\n\n# Recursively remove leaf nodes (with no onward connections)\n# If we remove all nodes, graph must be acyclic.\n\n\ndef graph_nodes(graph):\n    nodes = set()\n    for node, val in graph.items():\n        nodes.add(node)\n        nodes = nodes.union(val)\n    return nodes\n\n\ndef remove_leaves(graph):\n    nodes = graph_nodes(graph)\n    leaves = nodes - graph.keys()\n    return {n: set(v) - leaves for n, v in graph.items() if len(set(v) - leaves)}\n\n\ndef dag(graph):\n    while graph:\n        ngraph = remove_leaves(graph)\n        if len(graph) == len(ngraph):\n            return -1\n        graph = ngraph\n    return 1\n\n\ndef main(file):\n    gen = parse_graphs(open(file), directed=True)\n    print(*[dag(graph) for graph in gen])\n","171":"# Independent Segregation of Chromosomes\n\nfrom math import log10\nfrom .helpers import Parser, pbinom\n\n\ndef indc(n):\n    \"\"\"Independent Segregation of Chromosomes\"\"\"\n    res = [pbinom(2 * n - x, 2 * n, 0.5) for x in range(1, 2 * n + 1)]\n    return [round(log10(x), 3) for x in res]\n\n\ndef main(file):\n    print(*indc(*Parser(file).ints()))\n","172":"# Implement MotifEnumeration\n\nfrom .ba1n import neighbors\nfrom .ba1g import hamming\nfrom .ba1a import substrings\n\n\ndef all_kmers(dna, k):\n    return set(x for text in dna for x in substrings(text, k))\n\n\ndef contains_approx_match(pattern, text, d):\n    for x in substrings(text, len(pattern)):\n        if hamming(x, pattern) <= d:\n            return True\n    return False\n\n\ndef motif_enumeration(dna, k, d):\n    patterns = set()\n    for kmer in all_kmers(dna, k):\n        for hkmer in neighbors(kmer, d):\n            if all(contains_approx_match(hkmer, x, d) for x in dna):\n                patterns.add(hkmer)\n    return patterns\n\n\ndef main(file):\n    ints, *dna = open(file).read().splitlines()\n    k, d = map(int, ints.split())\n    print(*sorted(motif_enumeration(dna, k, d)))\n","173":"# Partial Sort\n\nfrom .helpers import ints\nfrom .hea import heap, heapify\nfrom .hs import sift_down, hs\n\n# To start, we insert the first k elements of the input into a max-heap. Then\n# for each remaining elements, we add it and heapify to put it into correct\n# location. Now we want to remove the biggest element (the first), which we do\n# by swapping the root with the last element and sifting down the new root.\n#\n# Though not specified in the question, I believe the k smallest elements must\n# also be sorted...\n\n\ndef ps(arr, k):\n    hea = heap(arr[:k])\n    for x in arr[k:]:\n        hea.append(x)\n        heapify(hea, k)\n        hea[0] = hea[k]\n        hea = hea[:-1]\n        sift_down(hea, 0, k - 1)\n    return hea\n\n\ndef main(file):\n    _, arr, k = open(file).read().splitlines()\n    res = ps(ints(arr), int(k))\n    print(*hs(res))\n","174":"# Find the Longest Path in a DAG\n\nfrom math import inf\nfrom copy import deepcopy\nfrom collections import defaultdict\n\n\n# A \"reverse\" graph, from each node to all incoming nodes\ndef incoming(graph):\n    x = defaultdict(list)\n    k = list(graph.keys())\n    k += [x[\"n\"] for v in graph.values() for x in v]\n    for g in list(set(k)):\n        for node in graph[g]:\n            x[node[\"n\"]] += [{\"n\": g, \"w\": node[\"w\"]}]\n    return x\n\n\n# Nodes with no incoming edges.\ndef sources(graph):\n    inc = incoming(graph)\n    return [g for g in graph if len(inc[g]) == 0]\n\n\ndef topological_order(graph):\n    graph = deepcopy(graph)\n    order = []\n    candidates = sources(graph)\n    while candidates:\n        n = candidates[0]\n        order.append(n)\n        candidates.remove(n)\n        graph[n] = []\n        candidates = list(set(sources(graph)) - set(order))\n    return order\n\n\ndef longest_path(graph, src, sink):\n    score = {}\n    path = defaultdict(list)\n    for node in graph:\n        score[node] = -inf\n    score[src] = 0\n    path[src] = [src]\n    inc = incoming(graph)\n    order = topological_order(graph)\n    for node in order[order.index(src) + 1 :]:\n        if len(inc[node]):\n            scores = [score[pre[\"n\"]] + pre[\"w\"] for pre in inc[node]]\n            score[node] = max(scores)\n            i = scores.index(max(scores))\n            path[node] = path[inc[node][i][\"n\"]] + [node]\n    return score[sink], path[sink]\n\n\ndef parse_graph(graph):\n    g = defaultdict(list)\n    for x in graph:\n        n, o = x.split(\"->\")\n        node, weight = o.split(\":\")\n        g[n] += [{\"n\": node, \"w\": int(weight)}]\n    return g\n\n\ndef main(file):\n    source, sink, *graph = open(file).read().splitlines()\n    score, path = longest_path(parse_graph(graph), source, sink)\n    print(score)\n    print(*path, sep=\"->\")\n","175":"# Construct a String Spelled by a Gapped Genome Path\n\nfrom .ba3j import string_from_paired_composition\n\n\ndef main(file):\n    ints, *pairs = open(file).read().splitlines()\n    k, d = map(int, ints.split())\n    pairs = [x.split(\"|\") for x in pairs]\n    print(string_from_paired_composition(pairs, k, d))\n","176":"# Implement 2-BreakOnGenomeGraph\n\nfrom .ba6i import parse_edge_string\nfrom copy import copy\n\n\ndef ints(x):\n    return list(map(int, x.split(\", \")))\n\n\ndef rm_edge(graph, edge):\n    if edge[0] in graph:\n        graph.pop(edge[0])\n    else:\n        graph.pop(edge[1])\n\n\ndef two_break_on_genome_graph(graph, i, ip, j, jp):\n    new = copy(graph)\n    for edge in [(i, ip), (j, jp)]:\n        rm_edge(new, edge)\n    new[i] = j\n    new[ip] = jp\n    return new\n\n\ndef main(file):\n    s, edges = open(file).read().splitlines()\n    graph = parse_edge_string(s)\n    edges = ints(edges)\n    new = two_break_on_genome_graph(graph, *edges)\n    edges = [(k, v) for k, v in new.items()]\n    print(*edges, sep=\", \")\n","177":"# Constructing a De Bruijn Graph\n\nfrom .helpers import Parser, Dna\n\n\ndef dbru(seqs, rev=True):\n    \"\"\"Constructing a De Bruijn Graph\"\"\"\n    seqs = list(seqs)\n    if rev:\n        # add reverse complement sequences\n        seqs = set(seqs).union([Dna(seq).revc().seq for seq in seqs])\n    return [(x[:-1], x[1:]) for x in seqs]\n\n\ndef main(file):\n    for pair in sorted(dbru(Parser(file).lines())):\n        print(\"(\", pair[0], \", \", pair[1], \")\", sep=\"\")\n","178":"# Perfect Matchings and RNA Secondary Structures\n\nfrom math import factorial, prod\nfrom .helpers import Parser\n\n\ndef pmch(seq):\n    return prod([factorial(seq.count(x)) for x in \"AG\"])\n\n\ndef main(file):\n    print(pmch(Parser(file).seqs()[0]))\n","179":"# Multiple Alignment\n\nfrom .helpers import Parser\nfrom itertools import product, combinations\n\n\ndef valid_coord(x, pos):\n    return all([i >= 0 for i in prev(pos, x)])\n\n\ndef prev(pos, ptr):\n    return tuple([p + d for p, d in zip(pos, ptr)])\n\n\ndef insertions(seqs, pos, ptr):\n    return [seq[pos[i] - 1] if ptr[i] == -1 else \"-\" for i, seq in enumerate(seqs)]\n\n\n# score is obtained as sum over all possible pairs\ndef score(seqs, pos, ptr):\n    a = insertions(seqs, pos, ptr)\n    return sum(0 if a == b else -1 for a, b in combinations(a, 2))\n\n\ndef moves(n):\n    return list(product([0, -1], repeat=n))[1:]\n\n\ndef mult(seqs):\n    m, p = {}, {}\n    m[0, 0, 0, 0] = 0\n    ranges = [range(0, len(s) + 1) for s in seqs]\n    for pos in product(*ranges):\n        ptrs = list(filter(lambda x: valid_coord(x, pos), moves(4)))\n        if not len(ptrs):\n            continue\n        sc = [m[prev(pos, x)] + score(seqs, pos, x) for x in ptrs]\n        m[pos] = max(sc)\n        p[pos] = ptrs[sc.index(max(sc))]\n\n    # traceback to recover alignment\n    tot = m[pos]\n    aln = [\"\", \"\", \"\", \"\"]\n    while any([x > 0 for x in pos]):\n        ptr = p[pos]\n        for i, v in enumerate(insertions(seqs, pos, ptr)):\n            aln[i] += v\n        pos = prev(pos, ptr)\n\n    return tot, *[a[::-1] for a in aln]\n\n\ndef main(file):\n    seqs = Parser(file).seqs()\n    print(*mult(seqs), sep=\"\\n\")\n","180":"# Implement UPGMA\n\nfrom .ba7b import parse_mat\nimport numpy as np\nfrom collections import defaultdict\n\n\ndef as_edges(graph):\n    edges = []\n    for k in sorted(graph):\n        for v in graph[k]:\n            edges += [f\"{k}->{v['n']}:{v['w']:.3f}\"]\n            edges += [f\"{v['n']}->{k}:{v['w']:.3f}\"]\n    return sorted(edges)\n\n\n# find (first) minimum off diagonal index in an array\ndef closest(D):\n    D = np.copy(D)\n    np.fill_diagonal(D, D.max() + 1)\n    return divmod(D.argmin(), D.shape[1])\n\n\n# replace the ith row\/col with the average of the ith and jth and remove\n# the jth\ndef average_ind(D, i, j, di, dj):\n    D = np.copy(D)\n    av = (D[i, :] * di + D[j, :] * dj) \/ (di + dj)\n    D[i, :] = av\n    D[:, i] = av\n    D = np.delete(D, j, 0)\n    D = np.delete(D, j, 1)\n    np.fill_diagonal(D, 0)\n    return D\n\n\ndef upgma(D, n):\n    clusters = list(range(0, n))\n    ages = defaultdict(lambda: 0)  # the \"age\" of a node\n    size = defaultdict(lambda: 1)  # the number of descendants of a node\n    T = {}  # the graph \/ tree we're building\n    node = n  # a label for internal nodes as we add them\n    while len(clusters) > 1:\n        i, j = closest(D)\n        a, b = clusters[i], clusters[j]\n\n        T[node] = [\n            {\"n\": a, \"w\": D[i, j] \/ 2 - ages[a]},\n            {\"n\": b, \"w\": D[i, j] \/ 2 - ages[b]},\n        ]\n        size[node] = size[a] + size[b]\n        ages[node] = D[i, j] \/ 2\n        clusters[i] = node\n        del clusters[j]\n        D = average_ind(D, *closest(D), size[a], size[b])\n        node += 1\n\n    return T\n\n\ndef main(file):\n    n, *D = open(file).read().splitlines()\n    D = np.array(parse_mat(D), float)\n    graph = upgma(D, int(n))\n    for edge in as_edges(graph):\n        print(edge)\n","181":"# Computing GC Content\n\nfrom .helpers import Parser, Dna\n\n\ndef max_gc(seqs):\n    gc = [Dna(rec.seq).gc_content() for rec in seqs]\n    m = gc.index(max(gc))\n    return {\"name\": seqs[m].id, \"value\": gc[m] * 100}\n\n\ndef main(file):\n    res = max_gc(Parser(file).fastas())\n    print(res[\"name\"], round(res[\"value\"], 5), sep=\"\\n\")\n","182":"# Implement FarthestFirstTraversal\n\nfrom math import sqrt\n\n\ndef read_types(handle, type):\n    for line in handle:\n        yield list(map(type, line.split()))\n\n\ndef euclidean_distance(a, b):\n    \"\"\"Euclidean distance between a pair of n-dimensional points\"\"\"\n    return sqrt(sum((x - y) ** 2 for x, y in zip(a, b)))\n\n\ndef ncd(x, centers):\n    \"\"\"Euclidean distance from DataPoint to its closest center\"\"\"\n    return min(euclidean_distance(x, c) for c in centers)\n\n\ndef farthest_first_traversal(points, k):\n    centers = [points[0]]\n    while len(centers) < k:\n        dists = [(i, ncd(point, centers)) for i, point in enumerate(points)]\n        centers += [points[max(dists, key=lambda x: x[1])[0]]]\n    return centers\n\n\ndef main(file):\n    handle = open(file)\n    k, m = next(read_types(handle, int))\n    points = [point for point in read_types(handle, float)]\n    centers = farthest_first_traversal(points, k)\n    for c in centers:\n        print(*c)\n","183":"# Matching a Spectrum to a Protein\n\nfrom .helpers import Parser\nfrom .conv import conv\nfrom .spec import spectrum\n\n\ndef prsm(s, r):\n    \"\"\"Matching a Spectrum to a Protein\"\"\"\n    mult = [conv(spectrum(x), r)[1] for x in s]\n    return [max(mult), s[mult.index(max(mult))]]\n\n\ndef main(file):\n    dat = Parser(file).lines()\n    n = int(dat[0])\n    res = prsm(dat[1 : 1 + n], [float(x) for x in dat[1 + n :]])\n    print(*res, sep=\"\\n\")\n","184":"# Insertion Sort\n\n# As mentioned in the problem, we will implement insertion sort and then\n# count swaps.\n\nfrom .helpers import ints\n\n\ndef insertion_sort(x):\n    swaps = 0\n    for i in range(1, len(x)):\n        k = i\n        while k > 0 and x[k] < x[k - 1]:\n            x[k - 1], x[k] = x[k], x[k - 1]\n            swaps += 1\n            k -= 1\n    return swaps\n\n\ndef main(file):\n    _, x = open(file).read().splitlines()\n    print(insertion_sort(ints(x)))\n","185":"# Merge Two Sorted Arrays\n\nfrom .helpers import ints\n\n\ndef mer(a1, a2):\n    a = []\n    while a1 and a2:\n        a.append(a1.pop(0) if a1[0] < a2[0] else a2.pop(0))\n    return a + a1 + a2\n\n\ndef main(file):\n    _, a1, _, a2 = open(file).read().splitlines()\n    print(*mer(ints(a1), ints(a2)))\n","186":"# Isolating Symbols in Alignments\n\nimport numpy as np\nfrom .helpers import Parser\n\n\ndef score(a, b):\n    return 1 if a == b else -1\n\n\ndef glob(s1, s2):\n    m = np.zeros((len(s1) + 1, len(s2) + 1), int)\n    for i in range(len(s1) + 1):\n        m[i][0] = -1 * i\n    for j in range(len(s2) + 1):\n        m[0][j] = -1 * j\n\n    for i in range(len(s1)):\n        for j in range(len(s2)):\n            m[i + 1][j + 1] = max(\n                m[i][j + 1] - 1,\n                m[i][j] + score(s1[i], s2[j]),\n                m[i + 1][j] - 1,\n            )\n\n    return m\n\n\ndef osym(s1, s2):\n    pm = glob(s1, s2)\n    sm = glob(s1[::-1], s2[::-1])\n    scores = [\n        pm[i][j] + score(s1[i], s2[j]) + sm[len(s1) - 1 - i][len(s2) - 1 - j]\n        for i in range(len(s1))\n        for j in range(len(s2))\n    ]\n    return max(scores), sum(scores)\n\n\ndef main(file):\n    print(*osym(*Parser(file).seqs()), sep=\"\\n\")\n","187":"# Open Reading Frames\n\nimport re\nfrom .helpers import Parser, Dna\n\n\ndef orf(seq):\n    for x in [seq, seq.revc()]:\n        for i in range(3):\n            subseq = x[i : len(x) - (len(x) - i) % 3]\n            for m in re.finditer(\"(?=(M[^\\\\*]*)\\\\*)\", subseq.translate().seq):\n                yield m.group(1)\n\n\ndef main(file):\n    seq = Dna(Parser(file).fastas()[0].seq)\n    orfs = list(orf(seq))\n    orfs = sorted(list(dict.fromkeys(orfs)))\n    print(\"\\n\".join(orfs))\n","188":"# Error Correction in Reads\n\nfrom .helpers import Parser, Dna\nfrom .hamm import hamm\n\n\ndef find_errors(seqs):\n    rseqs = [Dna(x).revc().seq for x in seqs]\n    unique = [x for x in seqs if seqs.count(x) + rseqs.count(x) == 1]\n    correct = set(seqs + rseqs).difference(set(unique))\n\n    for x in unique:\n        for y in correct:\n            if hamm(x, y) == 1:\n                yield x + \"->\" + y\n\n\ndef main(file):\n    seqs = Parser(file).fastas()\n    seqs = [x.seq for x in seqs]\n    print(*find_errors(seqs), sep=\"\\n\")\n","189":"# Consensus and Profile\n\nimport numpy as np\nfrom .helpers import Parser\n\n\ndef profile_matrix(seqs):\n    def count_bases(v):\n        return [sum(v == b) for b in \"ACGT\"]\n\n    x = np.array([list(x) for x in seqs])\n    return np.array([count_bases(v) for v in x.T]).T\n\n\ndef consensus_sequence(mat):\n    return \"\".join([\"ACGT\"[np.argmax(v)] for v in mat.T])\n\n\ndef main(file):\n    x = Parser(file).seqs()\n    mat = profile_matrix(x)\n    print(consensus_sequence(mat))\n    for i in range(0, 4):\n        print(\"ACGT\"[i] + \":\", *mat[i])\n","190":"# Binary Search\n\nfrom .helpers import ints\n\n\ndef bins(x, arr):\n    low = 0\n    high = len(arr) - 1\n    while low <= high:\n        mid = (low + high) \/\/ 2\n        if x == arr[mid]:\n            return mid + 1\n        elif x < arr[mid]:\n            high = mid - 1\n        else:\n            low = mid + 1\n    return -1\n\n\ndef main(file):\n    _, _, arr, m = open(file).read().splitlines()\n    arr = ints(arr)\n    m = ints(m)\n    print(*[bins(x, arr) for x in m])\n","191":"# Finding Disjoint Motifs in a Gene\n\nfrom .helpers import Parser\n\n\n# Generate possible forward moves relative to current position\n# if we've started including a pattern, we have to increment sequence \"s\"\ndef moves(i, j, k):\n    if j > 0 or k > 0:\n        return [(1, 0, 1), (1, 1, 0)]\n    else:\n        return [(1, 0, 0), (1, 0, 1), (1, 1, 0)]\n\n\n# Calculate the next position given current position and the increment\ndef npos(pos, move):\n    return tuple(a + b for a, b in zip(pos, move))\n\n\n# Is the proposed position valid, given the increment made and the sequences\n# Valid moves are within the sequence lengths and with matching bases\ndef valid(pos, move, seqs):\n    for p, x in zip(pos, seqs):\n        if p > len(x):\n            return False\n    bases = [x[p - 1] for x, p, m in zip(seqs, pos, move) if m == 1 and p > 0]\n    return len(set(bases)) == 1\n\n\n# Queue based iterative approach. Pop a position from the queue and add all\n# new valid positions.\n# If we reach the end of both our patterns, they can be interleaved.\ndef itwv(s, t, u):\n    pos = (0, 0, 0)\n    q = [pos]\n    seqs = [s, t, u]\n\n    while q:\n        pos = q.pop(0)\n        for move in moves(*pos):\n            np = npos(pos, move)\n            if valid(np, move, seqs):\n                if np[1] == len(t) and np[2] == len(u):\n                    return 1\n                q.append(np)\n    return 0\n\n\ndef main(file):\n    seq, *patterns = Parser(file).lines()\n    res = [[itwv(seq, p1, p2) for p1 in patterns] for p2 in patterns]\n    for x in res:\n        print(*x)\n","192":"# Base Quality Distribution\n\nfrom Bio import SeqIO\nfrom functools import reduce\n\n\ndef main(file):\n    handle = open(file)\n    q = int(next(handle))\n    qarr = [x.letter_annotations[\"phred_quality\"] for x in SeqIO.parse(handle, \"fastq\")]\n    tots = reduce(lambda a, b: [x + y for x, y in zip(a, b)], qarr)\n    print(sum(x \/ len(qarr) < q for x in tots))\n","193":"# Edit Distance Alignment\n\nfrom .helpers import Parser\n\n\ndef edta(s1, s2):\n    \"\"\"Edit Distance Alignment\"\"\"\n    # initialise\n    m, p = {}, {}\n    for j in range(len(s2) + 1):\n        m[j, 0] = j\n        p[j, 0] = [j - 1, 0]\n\n    for i in range(len(s1) + 1):\n        m[0, i] = i\n        p[0, i] = [0, i - 1]\n\n    # fill matrices\n    for j in range(len(s2)):\n        for i in range(len(s1)):\n            if s1[i] == s2[j]:\n                m[j + 1, i + 1] = m[j, i]\n                p[j + 1, i + 1] = [j, i]\n            else:\n                opt = [m[j + 1, i], m[j, i], m[j, i + 1]]\n                m[j + 1, i + 1] = min(opt) + 1\n                p[j + 1, i + 1] = [[j + 1, i], [j, i], [j, i + 1]][opt.index(min(opt))]\n\n    # traceback\n    a1, a2 = \"\", \"\"\n    i, j = len(s1), len(s2)\n    while i > 0 and j > 0:\n        if p[j, i] == [j - 1, i - 1]:\n            a1 += s1[i - 1]\n            a2 += s2[j - 1]\n        elif p[j, i] == [j, i - 1]:\n            a1 += s1[i - 1]\n            a2 += \"-\"\n        elif p[j, i] == [j - 1, i]:\n            a1 += \"-\"\n            a2 += s2[j - 1]\n        j, i = p[j, i]\n\n    return {\"dist\": m[len(s2), len(s1)], \"a1\": a1[::-1], \"a2\": a2[::-1]}\n\n\ndef main(file):\n    seqs = Parser(file).seqs()\n    out = edta(seqs[0], seqs[1])\n    print(out[\"dist\"], out[\"a1\"], out[\"a2\"], sep=\"\\n\")\n","194":"# Implement DecodingIdealSpectrum\n\nfrom .ba11a import spectrum_graph\n\n\ndef full(ions):\n    def infer_peptide(w, seq, seen):\n        if len(seq) == len(ions) \/\/ 2:\n            yield seq\n        for k in graph[w]:\n            if k[\"n\"] not in seen:\n                yield from infer_peptide(k[\"n\"], seq + k[\"l\"], seen + [w])\n\n    graph = spectrum_graph(ions)\n    yield from infer_peptide(min(ions), \"\", [])\n\n\ndef main(file):\n    x = [0] + list(map(int, open(file).read().split()))\n    specs = list(full(x))\n    print(specs[0])\n","195":"# Pattern Matching with the Suffix Array\n\nfrom .ba9g import suffix_array\n\n\ndef pattern_matching_with_suffix_array(text, pattern, sa):\n    minIndex = 0\n    maxIndex = len(text)\n    while minIndex < maxIndex:\n        midIndex = (minIndex + maxIndex) \/\/ 2\n        if pattern > text[sa[midIndex] :][: len(pattern)]:\n            minIndex = midIndex + 1\n        else:\n            maxIndex = midIndex\n    first = minIndex\n    maxIndex = len(text)\n    while minIndex < maxIndex:\n        midIndex = (minIndex + maxIndex) \/\/ 2\n        if pattern < text[sa[midIndex] :][: len(pattern)]:\n            maxIndex = midIndex\n        else:\n            minIndex = midIndex + 1\n    last = maxIndex\n    if first > last:\n        return []\n    else:\n        return list(range(first, last))\n\n\ndef main(file):\n    seq, *patterns = open(file).read().splitlines()\n    sa = suffix_array(seq)\n    inds = []\n    for p in patterns:\n        for ind in pattern_matching_with_suffix_array(seq, p, sa):\n            inds += [sa[ind]]\n    print(*sorted(set(inds)))\n","196":"# Heap Sort\n\nfrom .helpers import ints\nfrom .hea import heap\n\n\n# sift-down the element at heap[start] to its proper place\ndef sift_down(heap, start, end):\n    root = start\n    while root * 2 + 1 <= end:\n        left = root * 2 + 1\n        right = left + 1\n        swap = root\n        if heap[swap] < heap[left]:\n            swap = left\n        if right <= end and heap[swap] < heap[right]:\n            swap = right\n        if swap != root:\n            heap[root], heap[swap] = heap[swap], heap[root]\n            root = swap\n        else:\n            return\n\n\ndef hs(x):\n    x = heap(x)\n    i = len(x) - 1\n    while i > 0:\n        x[0], x[i] = x[i], x[0]\n        i -= 1\n        sift_down(x, 0, i)\n    return x\n\n\ndef main(file):\n    _, arr = open(file).read().splitlines()\n    print(*hs(ints(arr)))\n","197":"# Global Alignment with Scoring Matrix\n\nfrom .helpers import Parser\nfrom rosalind.helpers import blosum62\n\n\ndef glob(s1, s2, score, penalty):\n    \"\"\"Global Alignment with Scoring Matrix\"\"\"\n    m = {}\n    for j in range(len(s2) + 1):\n        m[j, 0] = penalty * j\n    for i in range(len(s1) + 1):\n        m[0, i] = penalty * i\n\n    for j in range(len(s2)):\n        for i in range(len(s1)):\n            pos = [(j + 1, i), (j, i), (j, i + 1)]\n            cost = [penalty, score[s1[i]][s2[j]], penalty]\n            scores = [m[pos[x]] + cost[x] for x in range(3)]\n            m[j + 1, i + 1] = max(scores)\n\n    return m[len(s2), len(s1)]\n\n\ndef main(file):\n    seqs = Parser(file).seqs()\n    print(glob(seqs[0], seqs[1], blosum62(), -5))\n","198":"# Counting Unrooted Binary Trees\n\nfrom functools import reduce\n\n\n# The answer here is just (2n -5)!!\n# We can help the calculation by taking the modulo at each step\ndef cunr(n, mod=10**6):\n    return reduce(lambda a, b: a * b % mod, range(2 * n - 5, 1, -2))\n\n\ndef main(file):\n    print(cunr(int(open(file).read())))\n","199":"# Double-Degree Array\n\nfrom .helpers import parse_graph\n\n\ndef main(file):\n    graph = parse_graph(open(file))\n    print(*[sum(len(graph[k]) for k in graph[n]) for n in graph.keys()])\n","200":"# Construct a Trie from a Collection of Patterns\n\nfrom collections import defaultdict\n\n\ndef edge_match(graph, node, label):\n    m = [x[\"n\"] for x in graph[node] if x[\"l\"] == label]\n    return m[0] if m else None\n\n\ndef trie(seqs):\n    graph = defaultdict(list)\n    count = 1\n    for seq in seqs:\n        curr_node = 0\n        for s in seq:\n            match = edge_match(graph, curr_node, s)\n            if match:\n                curr_node = match\n            else:\n                graph[curr_node].append({\"n\": count, \"l\": s})\n                curr_node = count\n                count += 1\n    return graph\n\n\ndef main(file):\n    seqs = open(file).read().splitlines()\n    g = trie(seqs)\n    for n1, v in g.items():\n        for n2 in v:\n            print(f\"{n1}->{n2['n']}:{n2['l']}\")\n","201":"# Construct a Profile HMM with Pseudocounts\n\nimport numpy as np\nfrom .ba10e import normalise, print_tprob, print_eprob, profile_hmm\n\n\ndef parse_input(handle):\n    \u03b8, \u03c3 = map(float, next(handle).rstrip().split())\n    next(handle)\n    alphabet = next(handle).split()\n    next(handle)\n    alignment = np.array([list(x) for x in handle.read().splitlines()])\n    return \u03b8, \u03c3, alphabet, alignment\n\n\n# Add pseudocounts to a transition probability matrix\n# S  can go to I0, M1, D1\n# In\/Mn\/Dn can go to In  M(n+1)  D(n+1)\n# The indexing here is horrible...\ndef add_transition_pseudocounts(x, \u03c3):\n    n = (x.shape[0] - 3) \/\/ 3\n    x[0, 1:4] += \u03c3\n    x[1, 1:4] += \u03c3\n    for i in range(0, n):\n        x[i * 3 + 2 : i * 3 + 5, (i + 1) * 3 + 1 : (i + 1) * 3 + 4] += \u03c3\n    return normalise(x)\n\n\n# Add pseudocounts to an emission probability matrix\ndef add_emission_pseudocounts(x, \u03c3):\n    n = (x.shape[0] - 3) \/\/ 3\n    x[1, :] += \u03c3\n    for i in range(0, n):\n        x[i * 3 + 2, :] += \u03c3\n        x[i * 3 + 4, :] += \u03c3\n    return normalise(x)\n\n\ndef pseudocount_profile_hmm(\u03b8, \u03c3, alphabet, alignment):\n    tprob, eprob = profile_hmm(\u03b8, alphabet, alignment)\n    tprob = add_transition_pseudocounts(tprob, \u03c3)\n    eprob = add_emission_pseudocounts(eprob, \u03c3)\n    return tprob, eprob\n\n\ndef main(file):\n    \u03b8, \u03c3, alphabet, alignment = parse_input(open(file))\n    tprob, eprob = pseudocount_profile_hmm(\u03b8, \u03c3, alphabet, alignment)\n    print_tprob(tprob)\n    print(\"--------\")\n    print_eprob(eprob, alphabet)\n","202":"# Enumerating k-mers Lexicographically\n\nfrom .helpers import Parser\nfrom itertools import product\n\n\ndef main(file):\n    l1, l2 = Parser(file).lines()\n    set = l1.split(\" \")\n    n = int(l2)\n    perm = [\"\".join(x) for x in product(set, repeat=n)]\n    print(*sorted(perm), sep=\"\\n\")\n","203":"# Longest Increasing Subsequence\n\nfrom .helpers import Parser\n\n\ndef lgis(x):\n    \"\"\"DP approach to longest increasing subsequence\"\"\"\n    n = len(x)\n    d = [1] * n  # length of the longest increasing subsequence ending at i\n    p = [-1] * n  # pointer to previous part of subsequence\n\n    for i in range(n):\n        for j in range(i):\n            if x[j] < x[i] and d[i] < d[j] + 1:\n                d[i] = d[j] + 1\n                p[i] = j\n\n    ans = max(d)\n    pos = d.index(ans)\n\n    # traceback\n    subseq = []\n    while pos != -1:\n        subseq.append(x[pos])\n        pos = p[pos]\n\n    subseq.reverse()\n    return subseq\n\n\ndef main(file):\n    data = Parser(file).lines()[1]\n    data = [int(x) for x in data.split(\" \")]\n    s1 = lgis(data)\n    print(*s1)\n\n    s2 = lgis([-x for x in data])\n    s2 = [-x for x in s2]\n    print(*s2)\n","204":"# Translate an RNA String into an Amino Acid String\n\nimport yaml\nimport re\nimport math\nfrom importlib import resources\n\n\ndef genetic_code():\n    path = resources.files(\"rosalind.resources\").joinpath(\"genetic_code.yaml\")\n    with open(path) as stream:\n        return yaml.safe_load(stream)\n\n\ndef translate(rna):\n    code = genetic_code()\n    end = math.floor(len(rna) \/ 3) * 3\n    prot = [code[rna[i : i + 3]] for i in range(0, end, 3)]\n    return re.sub(\"\\\\*$\", \"\", \"\".join(prot))\n\n\ndef main(file):\n    rna = open(file).read().splitlines()[0]\n    print(translate(rna))\n","205":"# Implement Hierarchical Clustering\n\nimport numpy as np\nfrom collections import defaultdict\nfrom .ba7d import closest, average_ind\n\n\ndef descendants(T, node):\n    q = [node]\n    x = []\n    while len(q):\n        n = q.pop(0)\n        if n in T:\n            q += T[n]\n        else:\n            x += [n]\n    return x\n\n\ndef hierarchical_clustering(D, n):\n    clusters = list(range(1, n + 1))\n    T = {}\n    size = defaultdict(lambda: 1)  # the number of descendants of a node\n    node = n\n    while len(clusters) > 1:\n        node += 1\n        i, j = closest(D)\n        a, b = clusters[i], clusters[j]\n        T[node] = [a, b]\n        size[node] = size[a] + size[b]\n        D = average_ind(D, *closest(D), size[a], size[b])\n        clusters[i] = node\n        del clusters[j]\n        yield descendants(T, a) + descendants(T, b)\n\n\ndef main(file):\n    n, *m = open(file).read().splitlines()\n    m = np.array([list(map(float, x.split())) for x in m])\n    for step in hierarchical_clustering(m, int(n)):\n        print(*step)\n","206":"# Compute Distances Between Leaves\n\nfrom re import split\nfrom collections import defaultdict\nfrom math import inf\nfrom heapq import heappush, heappop\n\n# For this solution, since the first n nodes are the leaves\n# I run Djiakstra's algorithm starting at the first n nodes,\n# and keep distances for the first n termini for each.\n# This gives the required matrix, but it's not terribly efficient.\n\n# A smarter solution might be to drop internal nodes and then\n# read off the distances between leaves directly\n\n\ndef nodes(graph):\n    s = list(graph.keys())\n    e = [y[\"n\"] for v in graph.values() for y in v]\n    return set(s) | set(e)\n\n\n# Dijkstra's algorithm to find distance from start to all other nodes\n# Assumes nodes are integers starting a 0!\ndef dij(start, graph):\n    d = [inf for i in range(len(nodes(graph)))]\n    d[start] = 0\n    q = []\n    heappush(q, (0, start))\n    processed = set()\n\n    while q:\n        u = heappop(q)[1]\n        processed.add(u)\n        for v in graph[u]:\n            if v[\"n\"] not in processed:\n                d[v[\"n\"]] = min(d[u] + v[\"w\"], d[v[\"n\"]])\n                heappush(q, (d[v[\"n\"]], v[\"n\"]))\n\n    return d\n\n\ndef parse_weighted_graph(edges):\n    graph = defaultdict(list)\n\n    for edge in edges:\n        f, t, w = map(int, split(r\"\\D+\", edge))\n        graph[f].append({\"n\": t, \"w\": w})\n\n    return graph\n\n\ndef main(file):\n    n_leaves, *edges = open(file).read().splitlines()\n    n_leaves = int(n_leaves)\n    graph = parse_weighted_graph(edges)\n    for i in range(n_leaves):\n        print(*dij(i, graph)[:n_leaves])\n","207":"# General Sink\n\nfrom .helpers import parse_graphs\nfrom .bfs import bfs\n\n\ndef gs(graph):\n    for node in graph:\n        dist = bfs(graph, node)\n        valid = [x >= 0 for x in dist]\n        if all(valid):\n            return node\n    return -1\n\n\ndef main(file):\n    graphs = parse_graphs(open(file), directed=True)\n    print(*[gs(g) for g in graphs])\n","208":"# 2-Satisfiability\n\n# https:\/\/en.wikipedia.org\/wiki\/2-satisfiability#Strongly_connected_components\n\nfrom .helpers import ints, recursionlimit\nfrom .scc import scc\nfrom .sc import condense\nfrom .ts import topological_sort\n\n\ndef parse_twosat(handle):\n    info = next(handle)\n    if info == \"\\n\":\n        info = next(handle)\n    nodes, n_edges = ints(info)\n    edges = [next(handle) for _ in range(n_edges)]\n    graph = {}\n\n    for n in range(1, nodes + 1):\n        graph[n] = list()\n        graph[-n] = list()\n\n    for edge in edges:\n        f, t = ints(edge)\n        graph[-f].append(t)\n        graph[-t].append(f)\n\n    return graph\n\n\ndef parse_twosats(handle):\n    n = int(next(handle))\n    for _ in range(n):\n        yield parse_twosat(handle)\n\n\ndef find_comp(n, components):\n    for j, comp2 in enumerate(components):\n        if n in comp2:\n            return j\n\n\ndef twosat(graph):\n    components = list(scc(graph))\n    ngraph = condense(graph, components)\n\n    # If any component has a literal and its negation,the instance is not\n    # satisfiable, so return 0\n    for i in topological_sort(ngraph):\n        for a in components[i]:\n            if -a in components[i]:\n                return 0, []\n\n    # If here, then the instance is satisfiable, let's recover assignment\n    assignment = []\n    for i in topological_sort(ngraph)[::-1]:\n        for v in components[i]:\n            if v not in assignment and -v not in assignment:\n                assignment.append(v)\n\n    return 1, sorted(assignment, key=lambda x: abs(x))\n\n\ndef main(file):\n    with recursionlimit(5000):\n        graphs = parse_twosats(open(file))\n        for graph in graphs:\n            x, a = twosat(graph)\n            print(x, *a)\n","209":"# Compute the Score of a Linear Peptide\n\nfrom .ba4g import linear_score\nfrom .ba4c import mass\n\n\ndef main(file):\n    peptide, spectrum = open(file).read().splitlines()\n    spectrum = list(map(int, spectrum.split()))\n    masses = mass()\n    print(linear_score([masses[x] for x in peptide], spectrum))\n","210":"# Find an Eulerian Cycle in a Graph\n\nfrom copy import deepcopy\n\n\ndef find_cycle(graph, key):\n    cycle = []\n    cycle += [key]\n    while len(graph[key]):\n        key = graph[key].pop(0)\n        cycle += [key]\n    for k in list(graph):\n        if len(graph[k]) == 0:\n            del graph[k]\n    return cycle\n\n\ndef eulerian_cycle(graph):\n    \"\"\"\n    Find an Eulerian cycle (if it exists).\n    \"\"\"\n    g = deepcopy(graph)\n    cycle = find_cycle(g, list(g.keys())[0])\n    while len(g):\n        key = [x for x in cycle if x in g][0]\n        i = cycle.index(key)\n        cycle = cycle[:i] + find_cycle(g, key) + cycle[(i + 1) :]\n    return cycle\n\n\ndef parse_edges(edges):\n    \"\"\"\n    Parse adjacency list to graph\n    \"\"\"\n    g = {}\n    for edge in edges:\n        k, v = edge.split(\" -> \")\n        g[k] = v.split(\",\")\n    return g\n\n\ndef main(file):\n    edges = open(file).read().splitlines()\n    cycle = eulerian_cycle(parse_edges(edges))\n    print(\"->\".join(cycle))\n","211":"from .ba5d import topological_order\nfrom collections import defaultdict\n\n\ndef parse_graph(graph):\n    g = defaultdict(list)\n    for edge in graph:\n        x, nodes = edge.split(\" -> \")\n        for y in nodes.split(\",\"):\n            g[x] += [{\"n\": y, \"w\": 0}]\n    return g\n\n\ndef main(file):\n    graph = open(file).read().splitlines()\n    print(*sorted(topological_order(parse_graph(graph))), sep=\", \")\n","212":"# Find Frequent Words with Mismatches and Reverse Complements\n\nfrom .ba1c import revcomp\nfrom .ba1g import hamming\nfrom .ba1b import count_kmers, most_frequent\nfrom .ba1i import generate_kmers\n\n\ndef count_hamming_revcomp_kmers(kmers, d, k):\n    for x in generate_kmers(k):\n        count = sum(kmers[y] for y in kmers if hamming(x, y) <= d)\n        count += sum(kmers[y] for y in kmers if hamming(revcomp(x), y) <= d)\n        if count > 0:\n            yield [x, count]\n\n\ndef main(file):\n    seq, ints = open(file).read().splitlines()\n    k, d = list(map(int, ints.split()))\n    kmers = count_kmers(seq, k)\n    hkmers = dict(count_hamming_revcomp_kmers(kmers, d, k))\n    print(*most_frequent(hkmers))\n","213":"# Find the Minimum Number of Coins Needed to Make Change\n\n\ndef dp_change(money, coins):\n    mc = [0] * (money + 1)\n    for m in range(1, money + 1):\n        mc[m] = min(mc[m - coin] + 1 for coin in coins if m >= coin)\n    return mc[money]\n\n\ndef main(file):\n    money, coins = open(file).read().splitlines()\n    coins = list(map(int, coins.split(\",\")))\n    print(dp_change(int(money), coins))\n","214":"# Maximum Matchings and RNA Secondary Structures\n\nfrom math import factorial\nfrom .helpers import Parser\n\n\ndef nPr(n, k):\n    \"\"\"Returns the number of k-permutations of n.\"\"\"\n    return factorial(n) \/\/ factorial(n - k)\n\n\ndef mmch(seq):\n    \"\"\"Maximum Matchings and RNA Secondary Structures\"\"\"\n    au = [seq.count(x) for x in \"AU\"]\n    gc = [seq.count(x) for x in \"GC\"]\n    return nPr(max(au), min(au)) * nPr(max(gc), min(gc))\n\n\ndef main(file):\n    print(mmch(Parser(file).seqs()[0]))\n","215":"# Implement SmallParsimony\n\nfrom collections import defaultdict\nfrom math import inf\n\n\n# return all nodes of a simple graph\ndef nodes(graph):\n    s = list(graph.keys())\n    e = [y for v in graph.values() for y in v]\n    return set(s) | set(e)\n\n\n# return all leaves of a simple graph\ndef leaves(graph):\n    return set(y for v in list(graph.values()) for y in v if not graph[y])\n\n\n# return all root node of a simple graph\ndef root(graph):\n    rev = reverse_graph(graph)\n    node = list(nodes(graph))[0]\n    while node in rev:\n        node = rev[node]\n    return node\n\n\n# reverse a simple graph (child points to parent)\ndef reverse_graph(graph):\n    rev = {}\n    for node in graph:\n        for child in graph[node]:\n            rev[child] = node\n    return rev\n\n\ndef parse_input(handle):\n    n = next(handle)\n    n = int(n)\n    seqs = {}\n    graph = defaultdict(list)\n    for i, edge in enumerate(range(n)):\n        f, t = next(handle).rstrip().split(\"->\")\n        graph[int(f)].append(i)\n        seqs[i] = t\n    for edge in handle.readlines():\n        f, t = edge.rstrip().split(\"->\")\n        graph[int(f)].append(int(t))\n    return seqs, graph\n\n\n# print (bidirectional) edges\ndef print_edges(graph, seqs, node):\n    for child in graph[node]:\n        dist = sum(a != b for a, b in zip(seqs[node], seqs[child]))\n        print(f\"{seqs[node]}->{seqs[child]}:{dist}\")\n        print(f\"{seqs[child]}->{seqs[node]}:{dist}\")\n        print_edges(graph, seqs, child)\n\n\ndef extract_position(graph, seqs, pos):\n    chars = {}\n    for n in nodes(graph) - leaves(graph):\n        chars[n] = \"\"\n    for leaf in leaves(graph):\n        chars[leaf] = seqs[leaf][pos]\n    return chars\n\n\ndef traceback(skp, node, ind):\n    bases = [\"A\", \"C\", \"T\", \"G\"]\n    chars = {}\n    chars[node] = bases[ind]\n    for k, v in skp[node][ind].items():\n        if k in skp:\n            chars = chars | traceback(skp, k, v)\n    return chars\n\n\ndef small_parsimony(graph, chars):\n    bases = [\"A\", \"C\", \"T\", \"G\"]\n    sk = {}  # minimum parsimony score of the subtree over possible labels\n    skp = {}  # pointer to selected base for each child over possible labels\n    to_process = nodes(graph)\n\n    # # initialise leaves\n    for leaf in leaves(graph):\n        sk[leaf] = [0 if chars[leaf] == c else inf for c in bases]\n        to_process.remove(leaf)\n\n    # iterate over available nodes till all are processed\n    while to_process:\n        for n in list(to_process):\n            if all(v in sk for v in graph[n]):\n                sk[n], skp[n] = [], []\n                for k in bases:\n                    tot = 0\n                    ptr = {}\n                    for d, sk_child in [(d, sk[d]) for d in graph[n]]:\n                        score = []\n                        for i, c in enumerate(bases):\n                            score += [sk_child[i] + (0 if c == k else 1)]\n                        tot += min(score)\n                        ptr[d] = score.index(min(score))\n                    skp[n] += [ptr]\n                    sk[n] += [tot]\n                to_process.remove(n)\n\n    # Recover sequence\n    node = root(graph)\n    score = min(sk[node])\n    return score, traceback(skp, node, sk[node].index(score))\n\n\ndef ba6f(graph, seqs):\n    # initialise sequences\n    for n in nodes(graph) - leaves(graph):\n        seqs[n] = \"\"\n\n    total_score = 0\n    for pos in range(len(seqs[0])):\n        chars = extract_position(graph, seqs, pos)\n        score, tbchars = small_parsimony(graph, chars)\n        total_score += score\n        for k, v in tbchars.items():\n            seqs[k] += v\n\n    return total_score, seqs\n\n\ndef main(file):\n    seqs, graph = parse_input(open(file))\n    total_score, seqs = ba6f(graph, seqs)\n    print(total_score)\n    print_edges(graph, seqs, root(graph))\n","216":"# Construct the Partial Suffix Array of a String\n\nfrom .ba9g import suffix_array\n\n\ndef partial_suffix_array(seq, k):\n    return [(i, x) for i, x in enumerate(suffix_array(seq)) if x % k == 0]\n\n\ndef main(file):\n    seq, k = open(file).read().splitlines()\n    for x in partial_suffix_array(seq, int(k)):\n        print(*x, sep=\",\")\n","217":"# Reconstruct a String from its k-mer Composition\n\nfrom .ba3b import genome_path\nfrom .ba3d import dbru\nfrom .ba3g import eulerian_path\n\n\ndef main(file):\n    k, *dna = open(file).read().splitlines()\n    print(genome_path(eulerian_path(dbru(dna))))\n","218":"# Compute the Score of a Cyclic Peptide Against a Spectrum\n\nfrom .ba4c import mass, cyclo_spectrum\n\n\ndef score(theoretical, expected):\n    spec, score = theoretical[:], 0\n    if spec:\n        for m in expected:\n            if m in spec:\n                score += 1\n                spec.remove(m)\n    return score\n\n\ndef main(file):\n    peptide, spectrum = open(file).read().splitlines()\n    spectrum = list(map(int, spectrum.split()))\n    m = mass()\n    spec = cyclo_spectrum([m[x] for x in peptide])\n    print(score(spec, spectrum))\n","219":"# Partial Permutations\n\nfrom functools import reduce\nfrom .helpers import Parser\n\n\ndef main(file):\n    n, k = Parser(file).ints()\n    print(reduce(lambda p, i: (p * i) % 1_000_000, range(n, n - k, -1)))\n","220":"# Creating a Character Table from Genetic Strings\n\n\ndef main(file):\n    seqs = open(file).read().splitlines()\n    for pos in zip(*seqs):\n        pos = [int(x == pos[0]) for x in pos]\n        if 1 < sum(pos) < (len(seqs) - 1):\n            print(*pos, sep=\"\")\n","221":"# Reconstruct a String from its Paired Composition\n\nfrom collections import defaultdict\nfrom .ba3b import genome_path\nfrom .ba3g import eulerian_path\n\n\ndef dbru_paired(pairs):\n    g = defaultdict(list)\n    for x in pairs:\n        p = tuple([x[0][:-1], x[1][:-1]])\n        s = tuple([x[0][1:], x[1][1:]])\n        g[p].append(s)\n    return g\n\n\ndef string_from_paired_composition(pairs, k, d):\n    path = eulerian_path(dbru_paired(pairs))\n    a = genome_path([x[0] for x in path])\n    b = genome_path([x[1] for x in path])\n    return a + b[-(k + d) :]\n\n\ndef main(file):\n    ints, *pairs = open(file).read().splitlines()\n    k, d = map(int, ints.split())\n    pairs = [x.split(\"|\") for x in pairs]\n    print(string_from_paired_composition(pairs, k, d))\n","222":"# Strongly Connected Components\n\nfrom collections import defaultdict\nfrom .ts import topological_sort\nfrom .helpers import parse_graph\nfrom .cc import find_component\n\n\ndef reverse_graph(graph):\n    rev = defaultdict(list)\n    for node in graph:\n        for child in graph[node]:\n            rev[child].append(node)\n    return rev\n\n\ndef scc(graph):\n    order = topological_sort(graph)\n    rev = reverse_graph(graph)\n    while order:\n        n = order.pop(0)\n        res = find_component(n, rev)\n        order = [x for x in order if x not in res]\n        for k in rev.keys():\n            rev[k] = [n for n in rev[k] if n not in res]\n        yield res\n\n\ndef main(file):\n    graph = parse_graph(open(file), directed=True)\n    print(len(list(scc(graph))))\n","223":"# Implement BWMatching\n\nfrom .ba9j import index_seq\n\n\ndef rindex(x, value):\n    x.reverse()\n    i = x.index(value)\n    x.reverse()\n    return len(x) - i - 1\n\n\ndef bwmatching(FirstColumn, LastColumn, Pattern):\n    first = list(index_seq(FirstColumn))\n    last = list(index_seq(LastColumn))\n\n    top = 0\n    bottom = len(LastColumn) - 1\n    while top <= bottom:\n        if Pattern:\n            Pattern, symbol = Pattern[:-1], Pattern[-1]\n            x = [LastColumn[i] for i in range(top, bottom + 1)]\n            if symbol in x:\n                topIndex = x.index(symbol) + top\n                bottomIndex = rindex(x, symbol) + top\n                top = first.index(last[topIndex])\n                bottom = first.index(last[bottomIndex])\n            else:\n                return 0\n        else:\n            return bottom - top + 1\n\n\ndef main(file):\n    text, patterns = open(file).read().splitlines()\n    patterns = patterns.split()\n    print(*[bwmatching(sorted(text), text, pattern) for pattern in patterns])\n","224":"# Identifying Reversing Substitutions\n\n# Some things to note:\n# - might not be immediate, e.g. A -> T -> T -> A\n# - they can overlap, e.g. A -> T -> A -> T\n# - if they occur on internal edges, they will be shared\n\nimport re\nfrom .helpers import Parser\nfrom .nwck import parse_newick\nfrom .alph import simplify_tree, nodes\n\n\ndef reverse_graph(graph):\n    rev = {}\n    for node in graph.keys():\n        for child in graph[node]:\n            rev[child] = node\n    return rev\n\n\n# The approach here will be to do a DFS. At each position, we find all\n# descendants and construct a line of our character table.\ndef ancestors(tree):\n    rev = reverse_graph(tree)\n    res = {}\n    for x in rev.keys():\n        res[x] = []\n        node = x\n        while node != \"0\":\n            node = rev[node]\n            res[x] += [node]\n    return res\n\n\ndef extract_position(graph, seqs, pos):\n    return {leaf: seqs[leaf][pos] for leaf in nodes(graph) - set(\"0\")}\n\n\n# Matches any character then not that character (with a negative lookahead) Then\n# we match the next character (.) and allow it to repeat (\\2*) before matching\n# the first again (\\1) at the end of the string ($).\ndef recurrent(node, anc, seq):\n    m = re.search(r\"(.)(?!\\1)((.)\\3*)\\1$\", seq)\n    if m:\n        g = m.groups()\n        return anc[node][len(g[1]) - 1], node, f\"{g[0]}->{g[2]}->{g[0]}\"\n\n\ndef paths(tree, pos):\n    def recurse(node, seq):\n        yield recurrent(node, anc, seq + pos[node])\n        if node in tree:\n            for child in tree[node]:\n                yield from recurse(child, seq + pos[node])\n\n    anc = ancestors(tree)\n    return recurse(\"0\", \"\")\n\n\ndef rsub(tree, seqs, i):\n    pos = extract_position(tree, seqs, i)\n    pos[\"0\"] = \"\"\n    for path in paths(tree, pos):\n        if path:\n            print(path[0], path[1], i + 1, path[2])\n    return None\n\n\ndef main(file):\n    handle = open(file)\n    tree = parse_newick(next(handle))\n    tree = simplify_tree(tree)\n    seqs = Parser(handle).fastas()\n    seqs = {x.id: x.seq for x in seqs}\n    n = len(list(seqs.values())[0])\n    for i in range(n):\n        rsub(tree, seqs, i)\n","225":"# Compute the Squared Error Distortion\n\nfrom .ba8a import ncd, read_types\n\n\ndef distortion(points, centers):\n    return (1 \/ len(points)) * sum(ncd(point, centers) ** 2 for point in points)\n\n\ndef main(file):\n    handle = open(file)\n    k, _ = next(read_types(handle, int))\n    gen = read_types(handle, float)\n    centers = [next(gen) for i in range(k)]\n    _ = next(handle)\n    points = [point for point in gen]\n    print(round(distortion(points, centers), 3))\n","226":"# Generate All Maximal Non-Branching Paths in a Graph\n\nfrom .ba3f import parse_edges\nfrom .ba3k import maximal_nonbranching_paths\n\n\ndef main(file):\n    edges = open(file).read().splitlines()\n    for path in sorted(maximal_nonbranching_paths(parse_edges(edges))):\n        print(\" -> \".join(path))\n","227":"# Counting Phylogenetic Ancestors\n\nfrom .helpers import Parser\n\n\ndef main(file):\n    print(Parser(file).ints()[0] - 2)\n","228":"# Global Alignment with Constant Gap Penalty\n\nfrom .helpers import Parser\nfrom rosalind.helpers import blosum62\n\n\ndef gcon(s1, s2, score, penalty):\n    \"\"\"Global Alignment with Constant Gap Penalty\"\"\"\n    # See Biological Sequence Analysis page 29\n    # Its a simplification of the more general affine gap penalty\n    # with an extension penalty of 0.\n    # We now have to keep track of three matrices m, x and y\n\n    m, x, y = {}, {}, {}\n    for j in range(len(s2) + 1):\n        m[j, 0] = penalty\n        y[j, 0] = -99\n    for i in range(len(s1) + 1):\n        m[0, i] = penalty\n        x[0, i] = -99\n\n    m[0, 0] = 0\n    for j in range(len(s2)):\n        for i in range(len(s1)):\n            new = (j + 1, i + 1)\n            x[new] = max([m[j, i + 1] + penalty, x[j, i + 1]])\n            y[new] = max([m[j + 1, i] + penalty, y[j + 1, i]])\n            m[new] = max([m[j, i] + score[s1[i]][s2[j]], x[new], y[new]])\n\n    return m[len(s2), len(s1)]\n\n\ndef main(file):\n    seqs = Parser(file).seqs()\n    print(gcon(seqs[0], seqs[1], blosum62(), -5))\n","229":"# Inferring Protein from Spectrum\n\nfrom math import isclose\nfrom rosalind.helpers import aa_mass\nfrom .helpers import Parser\nfrom .prtm import protein_mass\n\n\ndef match_mass(weight, rel_tol=1e-6):\n    aam = aa_mass()\n    matches = [k for k in aam if isclose(weight, aam[k], rel_tol=rel_tol)]\n    return None if len(matches) == 0 else matches[0]\n\n\ndef spectrum(seq):\n    \"\"\"Complete spectrum\"\"\"\n    r = range(1, len(seq))\n    spec = [seq] + [seq[:k] for k in r] + [seq[k:] for k in r]\n    return [protein_mass(x) for x in spec]\n\n\ndef main(file):\n    weights = [float(x) for x in Parser(file).lines()]\n    diff = [j - i for i, j in zip(weights[:-1], weights[1:])]\n    print(\"\".join([match_mass(x) for x in diff]))\n","230":"# Implement Viterbi Learning\n\nimport numpy as np\nfrom .ba10c import viterbi\nfrom .ba10e import print_mat\nfrom .ba10h import estimate_tmat, estimate_emat\n\n\ndef print_dict(d, rl, cl):\n    mat = np.zeros((len(rl), len(cl)), dtype=float)\n    for i, r in enumerate(rl):\n        for j, c in enumerate(cl):\n            mat[i, j] = d[r, c]\n    print_mat(mat, rl, cl)\n\n\ndef parse_input(handle):\n    niter = int(next(handle).rstrip())\n    next(handle)\n    seq = next(handle).rstrip()\n    next(handle)\n    alphabet = next(handle).split()\n    next(handle)\n    states = next(handle).split()\n    next(handle)\n    lines = [next(handle) for _ in range(len(states) + 1)]\n    tmat = {\n        (states[i], states[j]): float(v)\n        for i, x in enumerate(lines[1:])\n        for j, v in enumerate(x.split()[1:])\n    }\n    next(handle)\n    lines = [next(handle) for i in range(len(states) + 1)]\n    emat = {\n        (states[i], alphabet[j]): float(v)\n        for i, x in enumerate(lines[1:])\n        for j, v in enumerate(x.split()[1:])\n    }\n    return niter, seq, states, alphabet, tmat, emat\n\n\ndef main(file):\n    niter, seq, st, al, tmat, emat = parse_input(open(file))\n    for _ in range(niter):\n        path = viterbi(seq, st, tmat, emat)\n        tmat = estimate_tmat(path, st, to_dict=True)\n        emat = estimate_emat(seq, al, path, st, to_dict=True)\n    print_dict(tmat, st, st)\n    print(\"--------\")\n    print_dict(emat, st, al)\n","231":"# Encoding Suffix Trees\n\nfrom functools import cache\nfrom os.path import commonprefix\nfrom .helpers import Parser\n\n\ndef get_edges(graph):\n    for k in graph.keys():\n        yield k\n        yield from get_edges(graph[k])\n\n\n@cache\ndef suffix_tree(seq, starts):\n    graph = {}\n    bases = sorted(set([seq[start] for start in starts]))\n    for base in bases:\n        matching = [start for start in starts if seq[start] == base]\n        seqs = [seq[s:] for s in matching]\n        prefix = commonprefix(seqs)\n        size = len(prefix)\n        new_starts = [start + size for start in matching if start + size < len(seq)]\n        graph[prefix] = suffix_tree(seq, tuple(new_starts))\n    return graph\n\n\ndef suff(seq):\n    return suffix_tree(seq, tuple(range(len(seq))))\n\n\ndef main(file):\n    seq = Parser(file).line()\n    tree = suff(seq)\n    print(*list(get_edges(tree)), sep=\"\\n\")\n","232":"# Find a Highest-Scoring Local Alignment of Two Strings\n\nfrom importlib import resources\n\n\ndef pam250():\n    path = resources.files(\"rosalind.resources\").joinpath(\"pam250.txt\")\n    lines = open(path).read().splitlines()\n    header = lines[0].split()\n    return dict([x[0], dict(zip(header, map(int, x.split()[1:])))] for x in lines[1:])\n\n\ndef local_alignment(s1, s2, penalty=-5):\n    score = pam250()\n    m, p = {}, {}\n    for j in range(len(s2) + 1):\n        m[j, 0] = 0\n        p[j, 0] = \"\u2191\"\n    for i in range(len(s1) + 1):\n        m[0, i] = 0\n        p[0, i] = \"\u2190\"\n\n    m[0, 0] = 0\n    for j in range(len(s2)):\n        for i in range(len(s1)):\n            new = (j + 1, i + 1)\n            opt = [\n                m[j, i] + score[s1[i]][s2[j]],\n                m[j, i + 1] + penalty,\n                m[j + 1, i] + penalty,\n                0,\n            ]\n            m[new] = max(opt)\n            p[new] = [\"\u2196\", \"\u2191\", \"\u2190\", \"\u2196\"][opt.index(max(opt))]\n\n    max_score = max(x for x in m.values())\n    j, i = [k for k, v in m.items() if v == max_score][0]\n    a1, a2 = \"\", \"\"\n    while i > 0 or j > 0:\n        if m[j, i] == 0:\n            break\n        if p[j, i] == \"\u2196\":\n            a1 += s1[i - 1]\n            a2 += s2[j - 1]\n            j, i = j - 1, i - 1\n        elif p[j, i] == \"\u2190\":\n            a1 += s1[i - 1]\n            a2 += \"-\"\n            i = i - 1\n        elif p[j, i] == \"\u2191\":\n            a1 += \"-\"\n            a2 += s2[j - 1]\n            j = j - 1\n\n    return max_score, a1[::-1], a2[::-1]\n\n\ndef main(file):\n    s1, s2 = open(file).read().splitlines()\n    print(*local_alignment(s1, s2), sep=\"\\n\")\n","233":"# Creating a Character Table\n\nimport re\nfrom .helpers import nodes\nfrom .nwck import parse_newick\n\n\n# The approach here will be to do a DFS. At each position, we find all\n# descendants and construct a line of our character table.\ndef descendants(T):\n    def recurse(node):\n        if node not in T:\n            d[node] = []\n            return []\n        if node not in d:\n            children = [x[\"n\"] for x in T[node]]\n            d[node] = children + [y for x in T[node] for y in recurse(x[\"n\"])]\n            return d[node]\n\n    d = {}\n    recurse(\"0\")\n    return d\n\n\n# This is hacky and relies on knowing that the taxa are not just numbers (which\n# are used for other internal nodes not of interest)\ndef ctbl(tree):\n    desc = descendants(tree)\n    allnodes = sorted(x for x in nodes(tree) if not re.match(r\"^\\d+$\", x))\n    for subnodes in desc.values():\n        if not len(subnodes) or all([x in subnodes for x in allnodes]):\n            continue\n        yield \"\".join([\"1\" if n in subnodes else \"0\" for n in allnodes])\n\n\ndef main(file):\n    tree = parse_newick(open(file).read().rstrip())\n    for code in ctbl(tree):\n        print(code)\n","234":"# Counting DNA Nucleotides\n\nfrom .helpers import Parser\n\n\ndef main(file):\n    print(*Parser(file).dna().table().values())\n","235":"# Topological Sorting\n\nfrom .helpers import parse_graph\nfrom collections import defaultdict\nfrom .dag import graph_nodes\n\n\ndef topological_sort(g):\n    def find_sort(v, visited, stack):\n        visited[v] = True\n        for i in g[v]:\n            if not visited[i]:\n                find_sort(i, visited, stack)\n        stack.append(v)\n\n    visited = defaultdict(bool)\n    stack = []\n    for i in graph_nodes(g):\n        if not visited[i]:\n            find_sort(i, visited, stack)\n\n    return stack[::-1]\n\n\ndef main(file):\n    g = parse_graph(open(file), directed=True)\n    print(*topological_sort(g))\n","236":"# Construct the Burrows-Wheeler Transform of a String\n\nfrom .ba9g import suffix_array\n\n\ndef bwt(seq):\n    return \"\".join(seq[i - 1] for i in suffix_array(seq))\n\n\ndef main(file):\n    print(bwt(open(file).read().rstrip()))\n","237":"# Generate the Last-to-First Mapping of a String\n\nfrom .ba9j import index_seq\n\n\ndef last2first(seq, i):\n    first = list(index_seq(sorted(seq)))\n    last = list(index_seq(seq))\n    return first.index(last[i])\n\n\ndef main(file):\n    seq, i = open(file).read().splitlines()\n    print(last2first(seq, int(i)))\n","238":"# Find a k-Universal Circular String\n\nfrom itertools import product\nfrom .ba3b import genome_path\nfrom .ba3d import dbru\nfrom .ba3f import eulerian_cycle\n\n\ndef k_universal_binary_string(k):\n    kmers = [\"\".join(x) for x in product([\"0\", \"1\"], repeat=k)]\n    cycle = eulerian_cycle(dbru(kmers))\n    return genome_path(cycle[: -(k - 1)])\n\n\ndef main(file):\n    k = int(open(file).read().splitlines()[0])\n    print(k_universal_binary_string(k))\n","239":"# Finding the Longest Multiple Repeat\n\nfrom collections import defaultdict\nfrom .helpers import Parser\n\n\ndef build_seq(node, rev, edges):\n    seq = \"\"\n    while node in rev:\n        seq = edges[node] + seq\n        node = rev[node]\n    return seq\n\n\n# traverse graph and find longest seq up to a node with a least k leaves?\ndef lrep(seq, k, graph):\n    # parse data and build structures\n    edges = defaultdict(list)\n    rev = {}\n    heads, tails = set(), set()\n    for edge in graph:\n        n1, n2, i, n = edge.split()\n        tails.add(n2)\n        heads.add(n1)\n        rev[n2] = n1\n        edges[n2] = seq[(int(i) - 1) : (int(i) + int(n) - 1)]\n\n    # count the number of descendents (leaves) from each node\n    descendents = defaultdict(int)\n    for leaf in tails - heads:\n        while leaf in rev:\n            leaf = rev[leaf]\n            descendents[leaf] += 1\n\n    candidates = [x for x in descendents if descendents[x] >= k]\n    seqs = [build_seq(cand, rev, edges) for cand in candidates]\n    return max(seqs, key=len)\n\n\ndef main(file):\n    seq, k, *g = Parser(file).lines()\n    print(lrep(seq, int(k), g))\n","240":"# Speeding Up Motif Finding\n\nfrom .helpers import Parser\n\n\ndef kmp_preprocess(seq):\n    \"\"\"KMP preprocessing algorithm\"\"\"\n    j = -1\n    b = [j]\n    for i in range(len(seq)):\n        while j >= 0 and seq[i] != seq[j]:\n            j = b[j]\n        j += 1\n        b.append(j)\n    return b[1:]\n\n\ndef main(file):\n    seq = Parser(file).fastas()[0].seq\n    print(*kmp_preprocess(seq))\n","241":"# Compute the Probability of a Spectral Dictionary\n\n\nfrom functools import cache\nfrom .ba11c import masses\n\n\ndef dict_pr(sv, T, max_score):\n    @cache\n    def pr(i, t):\n        if i == 0 and t == 0:\n            return 1\n        if t < 0 or i <= 0:\n            return 0\n        return sum(pr(i - x, t - sv[i]) \/ len(mass) for x in mass)\n\n    mass = masses().values()\n    n = len(sv)\n    sv = [0] + sv\n    return sum(pr(n, x) for x in range(T, max_score + 1))\n\n\ndef main(file):\n    sv, T, max_score = open(file).read().splitlines()\n    sv = list(map(int, sv.split()))\n    print(dict_pr(sv, int(T), int(max_score)))\n","242":"# Trim a Peptide Leaderboard\n\nfrom .ba4c import mass\nfrom .ba4g import linear_score\n\n\ndef cut(peptides, spectrum, n):\n    masses = mass()\n    specs = [[masses[x] for x in p] for p in peptides]\n    sc = [linear_score(p, spectrum) for p in specs]\n    lim = sorted(sc, reverse=True)[n - 1]\n    return [p for p, sc in zip(peptides, sc) if sc >= lim]\n\n\ndef main(file):\n    lb, spectrum, n = open(file).read().splitlines()\n    lb = lb.split()\n    spectrum = list(map(int, spectrum.split()))\n    print(*cut(lb, spectrum, int(n)))\n","243":"# Implement GreedySorting\n\n\ndef reversal(perm, a, b):\n    perm[a : b + 1] = [-x for x in perm[a : b + 1][::-1]]\n\n\ndef locate(x, perm):\n    return [abs(y) for y in perm].index(x)\n\n\ndef format_perm(perm):\n    return \"(\" + \" \".join([f\"{x:+}\" for x in perm]) + \")\"\n\n\ndef read_perm(s):\n    return list(map(int, s[1:-1].split()))\n\n\ndef greedy_sorting(perm):\n    x = 1\n    while x <= len(perm):\n        if perm[x - 1] == x:\n            x += 1\n        else:\n            reversal(perm, x - 1, locate(x, perm))\n            yield perm\n\n\ndef main(file):\n    s = open(file).read().rstrip()\n    for step in greedy_sorting(read_perm(s)):\n        print(format_perm(step))\n","244":"# Introduction to Alternative Splicing\n\nfrom math import comb\nfrom .helpers import Parser\n\n\ndef main(file):\n    n, k = Parser(file).ints()\n    print(sum([comb(n, x) for x in range(k, n + 1)]) % 1000000)\n","245":"# Interleaving Two Motifs\n\nfrom .helpers import Parser\n\n\n# We can calculate a shortest common supersequence by constructing a\n# 2D matrix as before, but disallowing mismatches and with a special\n# traceback.\ndef scsp(s1, s2):\n    \"\"\"Interleaving Two Motifs\"\"\"\n\n    # Initialise\n    m, p = {}, {}\n    for j in range(len(s2) + 1):\n        m[j, 0] = j\n        p[j, 0] = [j - 1, 0]\n\n    for i in range(len(s1) + 1):\n        m[0, i] = i\n        p[0, i] = [0, i - 1]\n\n    # fill matrices\n    for j in range(len(s2)):\n        for i in range(len(s1)):\n            if s1[i] == s2[j]:\n                m[j + 1, i + 1] = m[j, i]\n                p[j + 1, i + 1] = [j, i]\n            else:\n                opt = [m[j + 1, i], m[j, i + 1]]\n                m[j + 1, i + 1] = min(opt) + 1\n                p[j + 1, i + 1] = [[j + 1, i], [j, i + 1]][opt.index(min(opt))]\n\n    # traceback\n    ss = \"\"\n    i, j = len(s1), len(s2)\n    while i > 0 or j > 0:\n        if p[j, i] == [j - 1, i - 1]:\n            ss += s1[i - 1]\n        elif p[j, i] == [j, i - 1]:\n            ss += s1[i - 1]\n        elif p[j, i] == [j - 1, i]:\n            ss += s2[j - 1]\n        j, i = p[j, i]\n\n    return {\"dist\": m[len(s2), len(s1)], \"ss\": ss[::-1]}\n\n\ndef main(file):\n    seqs = Parser(file).lines()\n    out = scsp(seqs[0], seqs[1])\n    print(out[\"ss\"], sep=\"\\n\")\n","246":"# Generate the d-Neighborhood of a String\n\nfrom .ba1g import hamming\n\n\ndef immediate_neighbors(seq):\n    for i in range(len(seq)):\n        for s in [\"A\", \"T\", \"G\", \"C\"]:\n            if s != seq[i]:\n                yield seq[:i] + s + seq[i + 1 :]\n\n\ndef neighbors(seq, d):\n    bases = [\"A\", \"T\", \"G\", \"C\"]\n    if d == 0:\n        return set([seq])\n    if len(seq) == 1:\n        return set(bases)\n    n = set()\n    for x in neighbors(seq[1:], d):\n        if hamming(seq[1:], x) < d:\n            for b in bases:\n                n.add(b + x)\n        else:\n            n.add(seq[0] + x)\n    return n\n\n\ndef main(file):\n    seq, d = open(file).read().splitlines()\n    print(*sorted(neighbors(seq, int(d))), sep=\"\\n\")\n","247":"# Compute the Number of Times a Pattern Appears in a Text\n\nimport re\nfrom .ba4a import translate\nfrom .ba1c import revcomp\n\n\ndef transcribe(dna):\n    return re.sub(\"T\", \"U\", dna)\n\n\ndef rev_transcribe(rna):\n    return re.sub(\"U\", \"T\", rna)\n\n\ndef find_matches(rna, pattern):\n    for i in range(3):\n        f = translate(rna[i:])\n        for m in re.finditer(rf\"(?=({pattern}))\", f):\n            start = m.span()[0] * 3 + i\n            end = start + len(pattern) * 3\n            yield rna[start:end]\n\n\ndef find_genome_substrings(dna, aa):\n    for match in find_matches(transcribe(dna), aa):\n        yield rev_transcribe(match)\n    for match in find_matches(transcribe(revcomp(dna)), aa):\n        yield revcomp(rev_transcribe(match))\n\n\ndef main(file):\n    dna, aa = open(file).read().splitlines()\n    for seq in find_genome_substrings(dna, aa):\n        print(seq)\n","248":"# 3-Way Partition\n\nfrom .helpers import ints\n\n\ndef par3(A, low=None, high=None):\n    if not low:\n        low = 0\n    if not high:\n        high = len(A) - 1\n    val = A[low]\n    i = low\n    while i <= high:\n        if A[i] < val:\n            A[i], A[low] = A[low], A[i]\n            i += 1\n            low += 1\n        elif A[i] > val:\n            A[i], A[high] = A[high], A[i]\n            high -= 1\n        else:\n            i += 1\n    return low, high\n\n\ndef main(file):\n    handle = open(file)\n    next(handle)\n    A = ints(next(handle))\n    par3(A)\n    print(*A)\n","249":"# Character-Based Phylogeny\n\n\ndef pick_informative(characters):\n    for i, ch in enumerate(characters):\n        if sum(ch) == 2:\n            return i, 1\n        if sum(ch) == len(ch) - 2:\n            return i, 0\n\n\ndef drop_uninformative(characters):\n    return [x for x in characters if 1 < sum(x) < len(x) - 1]\n\n\ndef flatten(x):\n    if isinstance(x, (list, tuple)):\n        return \"(\" + \",\".join(flatten(e) for e in x) + \")\"\n    else:\n        return str(x)\n\n\ndef chbp(names, characters):\n    while characters:\n        i, v = pick_informative(characters)\n        ind = [i for i, x in enumerate(characters[i]) if x == v]\n        names[ind[0]] = tuple(names[i] for i in ind)\n        del names[ind[1]]\n        for ch in characters:\n            del ch[ind[1]]\n        characters = drop_uninformative(characters)\n    return tuple(names)\n\n\ndef main(file):\n    names, *characters = open(file).read().splitlines()\n    names = names.split()\n    characters = [[int(x) for x in list(ch)] for ch in characters]\n    res = chbp(names, characters)\n    print(flatten(res), \";\", sep=\"\")\n","250":"# Enumerating Oriented Gene Orderings\n\nfrom .helpers import Parser\nfrom itertools import permutations, product\n\n\ndef sign(n):\n    \"\"\"Enumerating Oriented Gene Orderings\"\"\"\n    perm = list(permutations(range(1, n + 1)))\n    sign = list(product([-1, 1], repeat=n))\n    res = []\n    for p in perm:\n        for s in sign:\n            res.append([x * y for x, y in zip(s, p)])\n    return res\n\n\ndef main(file):\n    n = Parser(file).ints()[0]\n    res = sign(n)\n    print(len(res))\n    for i in res:\n        print(*i)\n","251":"# Square in a Graph\n\nfrom .helpers import parse_graphs\n\n\ndef dfs(graph, start, depth):\n    def recurse(node, dp, visited):\n        if dp == depth:\n            yield node\n        if dp < depth:\n            for new in graph[node]:\n                if new not in visited:\n                    yield from recurse(new, dp + 1, visited | set([node]))\n\n    return recurse(start, 0, set())\n\n\ndef sq(graph, n):\n    for node in graph:\n        for tail in dfs(graph, node, n - 1):\n            if tail in graph[node]:\n                return 1\n    return -1\n\n\ndef main(file):\n    graphs = parse_graphs(open(file), directed=False)\n    print(*[sq(g, 4) for g in graphs])\n","252":"# Genome Assembly Using Reads\n\nfrom .helpers import Parser\nfrom .dbru import dbru\nfrom .pcov import find_cycle, join_cycle\n\n\ndef kmers(seq, k):\n    \"\"\"Return all kmers of length k from sequence seq\"\"\"\n    return [seq[i : (i + k)] for i in range(len(seq) - k + 1)]\n\n\ndef extract_chain(graph):\n    ch1 = find_cycle(graph)\n    graph = dict(filter(lambda x: x[0] not in ch1, graph.items()))\n    return ch1, graph\n\n\ndef gasm(seqs):\n    \"\"\"Genome Assembly Using Reads\"\"\"\n    s0 = seqs[0][:-1]\n    for k in range(len(s0) + 1, 0, -1):\n        subseqs = [i for x in seqs for i in kmers(x, k)]\n        graph = dict(dbru(subseqs))\n        try:\n            ch1, graph = extract_chain(graph)\n            ch2, graph = extract_chain(graph)\n            if len(graph) == 0:\n                return join_cycle(ch1)\n        except KeyError:\n            continue\n\n\ndef main(file):\n    print(gasm(Parser(file).lines()))\n"}}